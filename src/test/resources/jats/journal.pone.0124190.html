



<div class="abstract toc-section"><a id="abstract0" name="abstract0" data-toc="abstract0" class="link-target" title="Abstract"></a><h2>Abstract</h2><a id="article1.front1.article-meta1.abstract1.p1" name="article1.front1.article-meta1.abstract1.p1" class="link-target"></a><p>Salient distractors draw our attention spontaneously, even when we intentionally want to ignore them. When this occurs, the real targets close to or overlapping with the distractors benefit from attention capture and thus are detected and discriminated more quickly. However, a puzzling opposite effect was observed in a search display with a column of vertical collinear bars presented as a task-irrelevant distractor [6]. In this case, it was harder to discriminate the targets overlapping with the salient distractor. Here we examined whether this effect originated from factors known to modulate attentional capture: (a) low probability—the probability occurrence of target location at the collinear column was much less (14%) than the rest of the display (86%), and observers might strategically direct their attention away from the collinear distractor; (b) attentional control setting—the distractor and target task interfered with each other because they shared the same continuity set in attentional task; and/or (c) lack of time to establish the optional strategy. We tested these hypotheses by (a) increasing to 60% the trials in which targets overlapped with the same collinear distractor columns, (b) replacing the target task to be connectivity-irrelevant (i.e., luminance discrimination), and (c) having our observers practice the same search task for 10 days. Our results speak against all these hypotheses and lead us to conclude that a collinear distractor impairs search at a level that is unaffected by probabilistic information, attentional setting, and learning.</p>
</div>


<div class="articleinfo"><p><strong>Citation: </strong>Tseng C-h, Jingling L (2015) A Salient and Task-Irrelevant Collinear Structure Hurts Visual Search. PLoS ONE 10(4):
           e0124190.
        
        https://doi.org/10.1371/journal.pone.0124190</p><p><strong>Academic Editor: </strong>Suliann Ben Hamed, Centre de Neuroscience Cognitive, FRANCE</p><p><strong>Received: </strong>September 10, 2014; <strong>Accepted: </strong>February 26, 2015; <strong>Published: </strong> April 24, 2015</p><p><strong>Copyright: </strong> © 2015 Tseng, Jingling. This is an open access article distributed under the terms of the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</p><p><strong>Data Availability: </strong>All relevant data are within the paper.</p><p><strong>Funding: </strong>Dr. Li Jingling was supported by Taiwan National Science Council (NSC-100-2627-B-039-004 and NSC101-2410-H-039-001-MY2) (<a href="http://www.most.gov.tw/">http://www.most.gov.tw/</a>) and a travel grant by HKU China Affair Office (<a href="http://www.als.hku.hk/hkucao/">http://www.als.hku.hk/hkucao/</a>). Dr. Chia-huei Tseng was supported by the Hong Kong Grant Research Council and HKU Seed Funding Programme for Basic Research. (<a href="http://www.ugc.edu.hk">http://www.ugc.edu.hk</a> and <a href="http://www.rss.hku.hk/">http://www.rss.hku.hk/</a>). NTT Research Lab in Japan supported Dr. Tseng during completion of this manuscript. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests: </strong> The authors have declared that no competing interests exist.</p></div>





<div id="section1" class="section toc-section"><a id="sec001" name="sec001" data-toc="sec001" class="link-target" title="Introduction"></a><h2>Introduction</h2><a id="article1.body1.sec1.p1" name="article1.body1.sec1.p1" class="link-target"></a><p>Visual search is an integral part of our daily lives, and selective attention is the central mechanism that determines what relevant information is to be processed in our search behavior. In some cases, selective attention seems effortless and largely driven by stimuli properties [<a href="#pone.0124190.ref001" class="ref-tip">1</a>], for example, when one area or object possesses a color or shape distinct from the rest, it catches attention immediately, even when these distinct features are of no help in finding our target. This effect is revealed when the searched-for target (e.g., a letter or oriented bar) is detected faster when it occurs on the area marked by task-irrelevant but salient features, such as color, shape, or abrupt onset [<a href="#pone.0124190.ref002" class="ref-tip">2</a>–<a href="#pone.0124190.ref005" class="ref-tip">5</a>]. This speed facilitation from a non-target (yet salient) object—termed “attentional capture”—suggests that attention is oriented by the stimuli properties.</p>
<a id="article1.body1.sec1.p2" name="article1.body1.sec1.p2" class="link-target"></a><p>Interestingly, Jingling and Tseng [<a href="#pone.0124190.ref006" class="ref-tip">6</a>] reported an unexpected effect in a search display that contained a column of collinear bars that distinctly stood out from the search display (<a href="#pone-0124190-g001">Fig 1</a>). This column was a salient but task-irrelevant distractor, as it did not provide any location information to help target orientation discrimination (main task). Unexpectedly, when the target spatially overlapped with the distractor column (Fig <a href="#pone-0124190-g001">1A</a> and <a href="#pone-0124190-g001">1C</a>), observers’ search time increased—opposite to a reduction, as would be predicted from previous attention capture literature [<a href="#pone.0124190.ref002" class="ref-tip">2</a>, <a href="#pone.0124190.ref004" class="ref-tip">4</a>]. Further, this search impairment is limited only when the distractor column is formed of snake-like collinear vertical bars (Fig <a href="#pone-0124190-g001">1A</a> and <a href="#pone-0124190-g001">1D</a>). When non-collinear (horizontal) bars were grouped to form the ladder-like distractor column (<a href="#pone-0124190-g001">Fig 1C</a>), the target overlapping with it received neither impairment (as in the collinear distractor condition) nor search facilitation (as predicted by attention capture theories) (<a href="#pone-0124190-g002">Fig 2A</a>). In both cases, overlapping targets contained identical local orientation contrast (both neighbored by two orthogonal and two parallel bars), so this ruled out a difference constituted by the salience computation based on local feature contrast. An additional experiment with collinear structure in the horizontal direction (<a href="#pone-0124190-g001">Fig 1D</a>) further suggested that collinear grouping of the global structure, instead of orientation of individual bar, was the critical factor driving the key difference (<a href="#pone-0124190-g002">Fig 2A</a>). We also found this search impairment was eliminated when the distractor column size was reduced (Figs <a href="#pone-0124190-g001">1B</a> and <a href="#pone-0124190-g002">2B</a>). As the local featural contrast (i.e., bottom-up defined salience) of the target was identical regardless of the distractor column length, our result strongly suggested that the collinear grouping determined the search interference. To summarize, our report highlights that the role of perceptual grouping on visual search, especially the principle of continuity or collinearity, is more important than previously thought. Nonetheless, how the perceptually grouped regions interact with the salience map and affect attentional capture are still open questions.</p>
<a class="link-target" id="pone-0124190-g001" name="pone-0124190-g001"></a><div class="figure" data-doi="10.1371/journal.pone.0124190.g001"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0124190.g001" data-doi="info:doi/10.1371/journal.pone.0124190" data-uri="info:doi/10.1371/journal.pone.0124190.g001"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0124190.g001" alt="thumbnail" class="thumbnail"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><div class="definition-label"><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g001">
                  PPT
                </a></div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g001">
                PowerPoint slide
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g001">
                  PNG
                </a></div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g001">
                larger image
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g001">
                  TIFF
                </a></div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g001">
                original image
              </a></li></ul></div><div class="figcaption"><span>Fig 1. </span> Examples of the search display in Jingling and Tseng (2013).</div><p class="caption_target"><a id="article1.body1.sec1.fig1.caption1.p1" name="article1.body1.sec1.fig1.caption1.p1" class="link-target"></a><p>The target is either overlapping (A or C) or non-overlapping (B or D) with the distractor. The distractor can be of long (21 bars) (A, C or D) or short collinear (3 bars) (B) in the distractor column. The configuration can be collinear vertical (A, B) or non-collinear (C), or collinear horizontal (D). The target is highlighted in (A) but not shown in experiment.</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0124190.g001">
              https://doi.org/10.1371/journal.pone.0124190.g001</a></p></div><a class="link-target" id="pone-0124190-g002" name="pone-0124190-g002"></a><div class="figure" data-doi="10.1371/journal.pone.0124190.g002"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0124190.g002" data-doi="info:doi/10.1371/journal.pone.0124190" data-uri="info:doi/10.1371/journal.pone.0124190.g002"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0124190.g002" alt="thumbnail" class="thumbnail"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><div class="definition-label"><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g002">
                  PPT
                </a></div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g002">
                PowerPoint slide
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g002">
                  PNG
                </a></div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g002">
                larger image
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g002">
                  TIFF
                </a></div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g002">
                original image
              </a></li></ul></div><div class="figcaption"><span>Fig 2. </span> Illustration of main findings in Jingling and Tseng (2013).</div><p class="caption_target"><a id="article1.body1.sec1.fig2.caption1.p1" name="article1.body1.sec1.fig2.caption1.p1" class="link-target"></a><p>(A) The effect of distractor configuration. (B) The effect of distractor size.</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0124190.g002">
              https://doi.org/10.1371/journal.pone.0124190.g002</a></p></div><a id="article1.body1.sec1.p3" name="article1.body1.sec1.p3" class="link-target"></a><p>The view that attentional capture is highly automatic was challenged recently by several studies. For example, reduction of the probability of a color or onset singleton distractor in a search display lowered observers’ manual and eye movement response to target search [<a href="#pone.0124190.ref007" class="ref-tip">7</a>–<a href="#pone.0124190.ref009" class="ref-tip">9</a>], which implies statistical information might penetrate the salience computation derived from featural contrast and interfere with subsequent search. Besides, task-related demand such as attentional control setting is also reported to alter attentional capture. Folk and his colleagues [<a href="#pone.0124190.ref010" class="ref-tip">10</a>, <a href="#pone.0124190.ref011" class="ref-tip">11</a>] found that salient task-irrelevant items could draw attention only when their features were contingent on top-down control settings, not when there was a mismatch between stimulus properties and what the observer was looking for. The requirement of match between task requirement and stimuli suggests the involvement of voluntary attention. Learning in visual search task was also reported to effectively inhibit attentional capture. Kelley and Yantis [<a href="#pone.0124190.ref012" class="ref-tip">12</a>] reported that observers’ ability to suppress a task-irrelevant distracting onset improved after practice, and the learning effect was transferrable to other conditions. The learning generalization implies that the practiced suppression to be distracted was not a local adaptation effect, but more likely, a reflection of enhanced higher-level selection efficiency. The regulations from probability information, attentional control setting, and learning on attentional capture all argue against the view that attentional capture is driven automatically only by stimulus salience [<a href="#pone.0124190.ref013" class="ref-tip">13</a>–<a href="#pone.0124190.ref017" class="ref-tip">17</a>].</p>
<a id="article1.body1.sec1.p4" name="article1.body1.sec1.p4" class="link-target"></a><p>Before further efforts are invested into examination of the role of collinear grouping in selective attention, it is of great importance to examine whether the above-mentioned factors—namely, statistical information, attentional control setting, and learning—can be alternative accounts that lead to collinear search impairment. In Jingling and Tseng [<a href="#pone.0124190.ref006" class="ref-tip">6</a>], we intended to make the collinear structure uninformative about the target location by equating the chance that a target overlaps on this collinear column and the other six possible columns (i.e., each at one out of seven, ~14%). It is possible that observers learned from the statistical regularity, intentionally suppressing the collinear distractor area due to its relatively low probability to coincide with target location compared with the rest of the areas in the search display (low-probability hypothesis). Several studies have demonstrated that observers can detect statistical regularities in the environment both with and without awareness [<a href="#pone.0124190.ref018" class="ref-tip">18</a>–<a href="#pone.0124190.ref023" class="ref-tip">23</a>, <a href="#pone.0124190.ref036" class="ref-tip">36</a>]. Eye movement studies also showed that people directed their eyes and attention faster to highly probable locations than to low-probability locations, without employing an explicit strategy to do so [<a href="#pone.0124190.ref024" class="ref-tip">24</a>, <a href="#pone.0124190.ref025" class="ref-tip">25</a>]. If statistical regularity modulates search impairment, collinear or continuous grouping may not be the primary consideration.</p>
<a id="article1.body1.sec1.p5" name="article1.body1.sec1.p5" class="link-target"></a><p>Another possible source is attentional control setting, the mental set to look for helpful features to complete a task. Folk et al. [<a href="#pone.0124190.ref010" class="ref-tip">10</a>] asked the participants to discriminate between a red T and a red L surrounded by white distractors. A non-informative cue of target location was shown before each target display. Usually this non-informative cue would not benefit target discrimination, but if the cue were red, which was congruent with the defining feature of the target, the observers’ response time was shortened. Subsequent studies found that such attentional control setting could be triggered by a distractor when the distractor shared a specific feature level with the target [<a href="#pone.0124190.ref010" class="ref-tip">10</a>, <a href="#pone.0124190.ref026" class="ref-tip">26</a>] or in a different feature level but in the same feature dimension [<a href="#pone.0124190.ref010" class="ref-tip">10</a>–<a href="#pone.0124190.ref011" class="ref-tip">11</a>], when the distractor and the target were both the oddest ones [<a href="#pone.0124190.ref027" class="ref-tip">27</a>], or when the distractor matched the task-wide expectation of how the target would appear [<a href="#pone.0124190.ref028" class="ref-tip">28</a>–<a href="#pone.0124190.ref029" class="ref-tip">29</a>]. In our case, the observers’ task was to discriminate between the orientation of a small gap, which indicated discontinuity between items, and the formation of a collinear distractor, which indicated continuity between items; thus, these two were in conflict in the attentional control setting, which might result in interference for the target task. If this is true, we would expect to reduce or remove the search impairment with another target task that did not require breaking the continuous distractor.</p>
<a id="article1.body1.sec1.p6" name="article1.body1.sec1.p6" class="link-target"></a><p>Finally, the collinear-selective impairment may be overcome by observers’ practice or learning to develop an optimal search strategy. In Jingling and Tseng [<a href="#pone.0124190.ref006" class="ref-tip">6</a>], observers performed 10–12 of practice trials before completing a block of 400 trials, among which only 1/7 trials (~70 trials) contained targets overlapped with the collinear structure. Studies indicated that practice improved participants’ search ability, and in some cases, an initially inefficient (serial) task can become to an efficient (parallel) search task after hundred trials of learning [<a href="#pone.0124190.ref030" class="ref-tip">30</a>–<a href="#pone.0124190.ref032" class="ref-tip">32</a>]. This change was attributed to high-level strategic development because this effect can be easily generalized to a different location, eye, or task [<a href="#pone.0124190.ref030" class="ref-tip">30</a>–<a href="#pone.0124190.ref032" class="ref-tip">32</a>]. It is possible that our naïve observers were hampered in the overlapping target condition due to insufficient practice to optimize their search strategy. It is therefore desirable to further examine whether we can remove this search disadvantage in an overlapping condition after substantial practice.</p>
<a id="article1.body1.sec1.p7" name="article1.body1.sec1.p7" class="link-target"></a><p>In this study, we designed three experiments to further examine these alternative accounts that are not related to stimuli property: statistical learning, attentional control setting, and visual search learning.</p>
</div>

<div id="section2" class="section toc-section"><a id="sec002" name="sec002" data-toc="sec002" class="link-target" title="Experiment Design"></a><h2>Experiment Design</h2><a id="article1.body1.sec2.p1" name="article1.body1.sec2.p1" class="link-target"></a><p>Our study was approved by IRB at the University of Hong Kong and China Medical University. All participants provide their written informed consent approved by IRB prior to the study.</p>
<a id="article1.body1.sec2.p2" name="article1.body1.sec2.p2" class="link-target"></a><p>We first replicated our previous findings in Experiment 1. Then we increased the likelihood so that targets were six times (60%) more likely to appear on the task-irrelevant column than other possible column locations (four possible locations at 10% each) in Experiment 2. If the probability manipulation reduced or reversed the search impairment from the collinear distractor, it would support the view that statistic information may be the source of the damage (low-probability hypothesis). We found the search impairment persisted, which did not support this hypothesis.</p>
<a id="article1.body1.sec2.p3" name="article1.body1.sec2.p3" class="link-target"></a><p>In Experiment 3, we let the target be a brighter or darker bar relative to other elements, and the observers’ task was to discriminate the target’s luminance. Luminance alternation of grouped elements does not affect grouping strength of the global structure [<a href="#pone.0124190.ref033" class="ref-tip">33</a>]; thus, a brighter or a darker target should not interfere with continuity of the collinear distractor. We still observed the same search impairment.</p>
<a id="article1.body1.sec2.p4" name="article1.body1.sec2.p4" class="link-target"></a><p>In Experiment 4, the observers underwent 10 days of search training. If the observers’ search strategy for a particular display was the bottleneck, we should expect to see the difference between overlapping and non-overlapping conditions gradually disappear through practice. We did not observe this trend, and the persistent collinearity-dependent impairment led us to reject the search strategy hypothesis.</p>
</div>

<div id="section3" class="section toc-section"><a id="sec003" name="sec003" data-toc="sec003" class="link-target" title="Experiment 1: Chance Level Overlapping Target"></a><h2>Experiment 1: Chance Level Overlapping Target</h2><a id="article1.body1.sec3.p1" name="article1.body1.sec3.p1" class="link-target"></a><p>In this experiment, we intended to replicate our previous findings regarding the impairment of target search by comparing responses when the target was on one of the bars in the salient column (overlapping target) and on the other texture columns (non-overlapping target). More importantly, the overlapping probability was set on a chance level, as in Turatto and Galfano [<a href="#pone.0124190.ref002" class="ref-tip">2</a>, <a href="#pone.0124190.ref004" class="ref-tip">4</a>]. To test the robustness of the effect, we allowed targets to appear in five possible locations in pre-specified 9 rows x 5 columns, which made it more difficult than having seven possible locations (a central row of only 7 columns), as in Jingling and Tseng [<a href="#pone.0124190.ref006" class="ref-tip">6</a>]. Targets occurred at salient collinear distractor columns at the chance level (i.e., 20%).</p>
<a id="article1.body1.sec3.p2" name="article1.body1.sec3.p2" class="link-target"></a><p>The critical component of the design was that the locations of salient line and target were independently determined among the same five locations; salient line location was not informative about target location. According to Turatto and Galfano [<a href="#pone.0124190.ref002" class="ref-tip">2</a>, <a href="#pone.0124190.ref004" class="ref-tip">4</a>], although the salient line was not relevant or informative to the target task, when our attention was captured by it, a target overlapping on it enjoyed the benefits of attention capture and, as such, required less time to be detected. However, according to our previous findings [<a href="#pone.0124190.ref006" class="ref-tip">6</a>], an overlapping target turned out to be harder to be discriminate than was a non-overlapping target.</p>

<div id="section1" class="section toc-section"><a id="sec004" name="sec004" class="link-target" title="Participants"></a>
<h3>Participants</h3>
<a id="article1.body1.sec3.sec1.p1" name="article1.body1.sec3.sec1.p1" class="link-target"></a><p>Twenty-two undergraduate students at China Medical University were recruited and kept naïve about the goal of the study. Their time was compensated with 100 NTD or additional course credits. They had normal or corrected-to-normal vision, according to self-reports.</p>
</div>

<div id="section2" class="section toc-section"><a id="sec005" name="sec005" class="link-target" title="Stimuli and Procedures"></a>
<h3>Stimuli and Procedures</h3>
<a id="article1.body1.sec3.sec2.p1" name="article1.body1.sec3.sec2.p1" class="link-target"></a><p>Observers saw 576 white element bars arranged in 21 rows x 27 columns against a dark background (<a href="#pone-0124190-g001">Fig 1A</a>) displayed on a 21” ViewSonic monitor. The luminance level was 120.05 cd/m<sup>2</sup> for white and 0 cd/m<sup>2</sup> for black. Each bar was .81° by .18° in the visual angle, with a viewing distance of 60 cm. One of the bars contained a tilt gap in the middle, and observers performed an orientation discrimination task to report which side this gap tilted toward (left or right). The target location varied from trial to trial, chosen among five columns (10th, 12th, 14th, 16th, and 18th) and nine rows (7th ~ 15th) by equal opportunity. One of the five columns was chosen as the distractor column; all element bars in this column were rotated by 90° and formed a salient, collinear structure. There was 1.04° of space between any two element bars. The distractor column was independently and randomly chosen, so only in one-fifth of the trial did the target overlap with the collinear column (namely, overlapping target). The search display stayed on the screen until the participants’ responses were received, and we encouraged our participants to respond as quickly as possible without sacrificing accuracy. Each participant completed 200 trials, which took approximately 15 minutes. The experiment was controlled with Psychtoolbox 2.54 in Matlab [<a href="#pone.0124190.ref034" class="ref-tip">34</a>, <a href="#pone.0124190.ref035" class="ref-tip">35</a>].</p>
</div>
</div>

<div id="section4" class="section toc-section"><a id="sec006" name="sec006" data-toc="sec006" class="link-target" title="Results of Experiment 1"></a><h2>Results of Experiment 1</h2><a id="article1.body1.sec4.p1" name="article1.body1.sec4.p1" class="link-target"></a><p>Only correct responses within two standard deviations above the grand mean of response times (RT) were kept for analysis. Thus, 4.41% of trials were discarded under this criterion. The selected data are shown in <a href="#pone-0124190-g003">Fig 3A</a>. We replicated the RT search impairment of overlapping targets [<em>t</em> (21) = 6.56, <em>p</em> &lt; .001, RT<sub>overlapping</sub> = 1461.74 ms, RT<sub>non-overlapping</sub> = 1180.97 ms] as well as accuracy impairment (<em>t</em> (21) = 3.41, <em>p</em> &lt; .001, Accuracy<sub>overlapping</sub> = 92.05%, Accuracy<sub>non-overlapping</sub> = 95.88%] in Jingling and Tseng’s study [<a href="#pone.0124190.ref006" class="ref-tip">6</a>]. This is the opposite of what Turatto and Galfano [<a href="#pone.0124190.ref002" class="ref-tip">2</a>, <a href="#pone.0124190.ref004" class="ref-tip">4</a>] discovered. Because accuracy data and RTs are known to diverge from a normal distribution, we did log-transformation of the data uphold the normality assumption in ANOVA testing. The analysis results after transformation were the same as that with raw data in all 4 experiments. In the text we follow the convention to report the analysis results with raw data only.</p>
<a class="link-target" id="pone-0124190-g003" name="pone-0124190-g003"></a><div class="figure" data-doi="10.1371/journal.pone.0124190.g003"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0124190.g003" data-doi="info:doi/10.1371/journal.pone.0124190" data-uri="info:doi/10.1371/journal.pone.0124190.g003"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0124190.g003" alt="thumbnail" class="thumbnail"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><div class="definition-label"><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g003">
                  PPT
                </a></div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g003">
                PowerPoint slide
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g003">
                  PNG
                </a></div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g003">
                larger image
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g003">
                  TIFF
                </a></div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g003">
                original image
              </a></li></ul></div><div class="figcaption"><span>Fig 3. </span> Results of chance occurrence in Experiment 1 (A), and high occurrence in Experiment 2 (B).</div><p class="caption_target"><a id="article1.body1.sec4.fig1.caption1.p1" name="article1.body1.sec4.fig1.caption1.p1" class="link-target"></a><p>Red columns are data for non-overlapping targets, while blue columns are data for overlapping targets. The asterisk indicates significant differences (<em>p</em> &lt; .05) between conditions.</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0124190.g003">
              https://doi.org/10.1371/journal.pone.0124190.g003</a></p></div>
<div id="section1" class="section toc-section"><a id="sec007" name="sec007" class="link-target" title="Discussion of Experiment 1"></a>
<h3>Discussion of Experiment 1</h3>
<a id="article1.body1.sec4.sec1.p1" name="article1.body1.sec4.sec1.p1" class="link-target"></a><p>We replicated the robust main finding in our previous observations [<a href="#pone.0124190.ref006" class="ref-tip">6</a>] that a target overlapped with a salient distractor took observers longer to discriminate its orientation than when it overlapped with the background. In our search display, a collinear structure that was task irrelevant (i.e., its presence and location were uninformative to target orientation report) slowed down observers’ responses.</p>
<a id="article1.body1.sec4.sec1.p2" name="article1.body1.sec4.sec1.p2" class="link-target"></a><p>Our next experiment evaluated the modulation from probability distribution and top-down suppression. In Experiment 1, the target occurred on the collinear column only by chance (i.e., 20%), and the accumulation probability for all non-collinear columns was four times greater than it was for the collinear column. It is possible that observers, based on this probability information, strategically suppressed the space occupied by the collinear column. In Experiment 2, we increased the probability of target occurrence on the collinear column to 60%. The search disadvantage from the overlapped target should have decreased or reversed if observers were able to optimize their search behavior based on the statistical information.</p>
</div>
</div>

<div id="section5" class="section toc-section"><a id="sec008" name="sec008" data-toc="sec008" class="link-target" title="Experiment 2: High Occurrence of Overlapping Target"></a><h2>Experiment 2: High Occurrence of Overlapping Target</h2>
<div id="section1" class="section toc-section"><a id="sec009" name="sec009" class="link-target" title="Participants"></a>
<h3>Participants</h3>
<a id="article1.body1.sec5.sec1.p1" name="article1.body1.sec5.sec1.p1" class="link-target"></a><p>Sixteen undergraduate students in China Medical University were recruited for Experiment 2. They had normal or corrected-to-normal vision and did not participate in the previous experiment.</p>
</div>

<div id="section2" class="section toc-section"><a id="sec010" name="sec010" class="link-target" title="Stimuli and Procedures"></a>
<h3>Stimuli and Procedures</h3>
<a id="article1.body1.sec5.sec2.p1" name="article1.body1.sec5.sec2.p1" class="link-target"></a><p>All details were identical to those in Experiment 1 except that: (a) the target overlapped with the collinear column in 57.14% of the trials and appeared evenly at six other non-collinear columns in 42.86% of the trials (~6% each). This made the collinear column a predictor to target location; and (b) the collinear column length varied among 3, 9, and 21 bars (<a href="#pone-0124190-g002">Fig 2B</a>). According to Jingling and Tseng [<a href="#pone.0124190.ref006" class="ref-tip">6</a>], the collinear column masked the target only when the length was long enough (e.g., 9 or 21 bars). Further, the target was always fixed at a central (11th) row instead of varying among seven possible rows, as in Experiment 1; thus, the task was relatively easier. Each participant completed 294 trials.</p>
</div>

<div id="section3" class="section toc-section"><a id="sec011" name="sec011" class="link-target" title="Results of Experiment 2"></a>
<h3>Results of Experiment 2</h3>
<a id="article1.body1.sec5.sec3.p1" name="article1.body1.sec5.sec3.p1" class="link-target"></a><p>We discarded the inaccurate trials and those with RT two standard deviations above the group mean; 2.38% of the trials were excluded. Results, shown in <a href="#pone-0124190-g003">Fig 3B</a>, were submitted to a two (collinear length 3, 9, or 21 bars) by two (target overlapping or non-overlapping with the collinear distractor) repeated ANOVA. We found that RT was longer when the distractor column length was long: <em>F</em> (2, 15) = 4.50, <em>MSE</em> = 1218.44, <em>p</em> &lt; .05. Meanwhile, we found a significant interaction between distractor length and target type: <em>F</em> (2, 30) = 28.85, <em>MSE</em> = 1298.56, <em>p</em> &lt; .0001. Simple main effect analysis on this interaction showed that overlapping targets (758.98 ms) were discriminated <em>faster</em> than non-overlapping targets (807.19 ms) when the distractor column was at size 3, <em>F</em> (1, 45) = 7.29, <em>MSE</em> = 2552.10, <em>p</em> &lt; .01. Yet, the effect reversed when the distractor column was at size 21 (overlapping targets 846.33 ms vs. non-overlapping targets 758.15 ms, <em>F</em> (1, 45) = 24.37, <em>MSE</em> = 2552.10, <em>p</em> &lt; .0001). Therefore, even if the collinear column predicted the target location, it still impaired target discrimination when the column was long enough. The ANOVA analysis on accuracy data did not find any significant difference, arguing against possible speed—accuracy trade-off.</p>
</div>

<div id="section4" class="section toc-section"><a id="sec012" name="sec012" class="link-target" title="Discussion of Experiment 2"></a>
<h3>Discussion of Experiment 2</h3>
<a id="article1.body1.sec5.sec4.p1" name="article1.body1.sec5.sec4.p1" class="link-target"></a><p>In Experiment 2, the distractor column was predicative to the target location, and observers could use this informative cue to enhance their search performance by directing their attention to this salient structure in the search display. However, observers continued to suffer at trials where the target overlapped with the distractor column when the column was long. The results speak against the low-probability hypothesis, which suggests that the visual search impairment in Experiment 1 was due to statistical information.</p>
<a id="article1.body1.sec5.sec4.p2" name="article1.body1.sec5.sec4.p2" class="link-target"></a><p>Muller et al. [<a href="#pone.0124190.ref008" class="ref-tip">8</a>] systematically varied the portion of a task-irrelevant singleton distractor to appear in a search display where observers looked for a shape-defined target. They found that the interference from the infrequent singleton distractor was significantly bigger than frequent ones, implying that observers did use the probabilistic information of distractor presence to customize their search strategy. In our design, the distractor was always present, but its predictability about target location varied. At higher occurrence condition (Exp 2), observers actually benefited from directing their attention to the distractor column, as the target fell on this column for 60% of the trails. However, this was only visible from observers’ response time when the distractor was short (3-bar condition), which is not applicable to long-distractor conditions (9-, 21-bar). The characteristic difference between the short and long collinear column suggested that regardless of the probability of the target occurrence overlapping with the distractor column, a penalty seemed to be imposed exclusively on the long collinear structure. Can this be owing to observers’ limited access to this information due to factors such as unawareness of the statistical distribution of the target location? Statistical learning has been reported in tasks embedding more implicit probabilistic information, even when observers were unaware of it [<a href="#pone.0124190.ref018" class="ref-tip">18</a>–<a href="#pone.0124190.ref023" class="ref-tip">23</a>]. More plausibly, we speculate that collinear impairment may occur at a perceptual level that is not heavily modulated by event probable occurrence.</p>
</div>
</div>

<div id="section6" class="section toc-section"><a id="sec013" name="sec013" data-toc="sec013" class="link-target" title="Experiment 3: Luminance Discrimination Target Task"></a><h2>Experiment 3: Luminance Discrimination Target Task</h2><a id="article1.body1.sec6.p1" name="article1.body1.sec6.p1" class="link-target"></a><p>In this experiment, we tested whether the search impairment effect for a local target overlapping with a salient collinear column occurred only when a distractor property (e.g., continuous collinear structure) mismatched target task nature (e.g., a breakdown of continuity). In this experiment, we retained the same search display but re-designed the target task to be a luminance discrimination task so that they did not conflict with the continuous distractor. If task set is important, than search impairment effect is not expected in this experiment.</p>

<div id="section1" class="section toc-section"><a id="sec014" name="sec014" class="link-target" title="Participants"></a>
<h3>Participants</h3>
<a id="article1.body1.sec6.sec1.p1" name="article1.body1.sec6.sec1.p1" class="link-target"></a><p>Twelve undergraduates in China Medical University were recruited and compensated with additional course credits or 100 NTD. They claimed to have normal or corrected-to-normal vision. They were not told about the goal of this study in advance and did not participate in the previous experiment.</p>
</div>

<div id="section2" class="section toc-section"><a id="sec015" name="sec015" class="link-target" title="Stimuli and Procedures"></a>
<h3>Stimuli and Procedures</h3>
<a id="article1.body1.sec6.sec2.p1" name="article1.body1.sec6.sec2.p1" class="link-target"></a><p>The experiment was similar to that in Jingling and Tseng [<a href="#pone.0124190.ref006" class="ref-tip">6</a>]. <a href="#pone-0124190-g004">Fig 4</a> shows part of the search display. The whole search display was filled with 21 rows and 27 columns of gray bars (128 of 256 luminance level, 41.9 cd/m<sup>2</sup>) on black background (0 cd/m<sup>2</sup>). These bars were all horizontal, except for one odd-man-out distractor column of vertical bars. The distractor column could have three or 21 bars in vertical, which is called <em>distractor size</em> hereafter. Participants discriminated whether the target was a brighter (192 over 256 luminance level, 82.4 cd/m<sup>2</sup>) or a darker (64 over 256 luminance level, 17.5 cd/m<sup>2</sup>) bar in each trial by pressing corresponding keys. There were seven possible columns (8th, 10th, 12th, 14th, 16th, 18th, and 20th) of the total 27 columns that could present the target or the column of the distractor. The targets’ vertical position was fixed at the central (11th) row. Positions of the target and the distractor were manipulated orthogonally, making the distractor and the target spatially irrelevant. Each participant completed 392 trials after 10 practice trials.</p>
<a class="link-target" id="pone-0124190-g004" name="pone-0124190-g004"></a><div class="figure" data-doi="10.1371/journal.pone.0124190.g004"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0124190.g004" data-doi="info:doi/10.1371/journal.pone.0124190" data-uri="info:doi/10.1371/journal.pone.0124190.g004"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0124190.g004" alt="thumbnail" class="thumbnail"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><div class="definition-label"><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g004">
                  PPT
                </a></div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g004">
                PowerPoint slide
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g004">
                  PNG
                </a></div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g004">
                larger image
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g004">
                  TIFF
                </a></div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g004">
                original image
              </a></li></ul></div><div class="figcaption"><span>Fig 4. </span> Part of the search display used in this experiment.</div><p class="caption_target"><a id="article1.body1.sec6.sec2.fig1.caption1.p1" name="article1.body1.sec6.sec2.fig1.caption1.p1" class="link-target"></a><p>The target is a brighter bar and overlaps with the salient collinear column in this example.</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0124190.g004">
              https://doi.org/10.1371/journal.pone.0124190.g004</a></p></div></div>

<div id="section3" class="section toc-section"><a id="sec016" name="sec016" class="link-target" title="Results of Experiment 3"></a>
<h3>Results of Experiment 3</h3>
<a id="article1.body1.sec6.sec3.p1" name="article1.body1.sec6.sec3.p1" class="link-target"></a><p>We excluded incorrect trials (4.31%) and those trials (2.87%) which took longer than two standard deviations of the grand mean. The selected RTs were then submitted to a two-way repeated ANOVA, with target type (overlapped or non-overlapped) and distractor size (3 or 21 bars) as factors. <a href="#pone-0124190-g005">Fig 5</a> shows the results of the selected RT. The main effect of target type was found, <em>F</em> (1, 11) = 26.43, <em>MSE</em> = 873.11, <em>p</em> &lt; .001. That is, responses were longer for overlapping targets (626.89 ms) than they were for non-overlapping targets (583.06 ms). Also, the two-way interaction was significant, <em>F</em> (1, 11) = 18.60, <em>MSE</em> = 580.96, <em>p</em> &lt; .001. As shown in <a href="#pone-0124190-g005">Fig 5</a>, responses for overlapping targets were slower than those for non-overlapping targets when the distractor column was long, <em>F</em> (1, 22) = 45.02, <em>MSE</em> = 727.03, <em>p</em> &lt; .0001, but not when the distractor column was short, <em>p</em> = .22. This result replicated that in Jingling and Tseng [<a href="#pone.0124190.ref006" class="ref-tip">6</a>], suggesting that a similar search impairment effect can be observed with a different task set.</p>
<a class="link-target" id="pone-0124190-g005" name="pone-0124190-g005"></a><div class="figure" data-doi="10.1371/journal.pone.0124190.g005"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0124190.g005" data-doi="info:doi/10.1371/journal.pone.0124190" data-uri="info:doi/10.1371/journal.pone.0124190.g005"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0124190.g005" alt="thumbnail" class="thumbnail"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><div class="definition-label"><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g005">
                  PPT
                </a></div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g005">
                PowerPoint slide
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g005">
                  PNG
                </a></div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g005">
                larger image
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g005">
                  TIFF
                </a></div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g005">
                original image
              </a></li></ul></div><div class="figcaption"><span>Fig 5. </span> Results of Experiment 3.</div><p class="caption_target"><a id="article1.body1.sec6.sec3.fig1.caption1.p1" name="article1.body1.sec6.sec3.fig1.caption1.p1" class="link-target"></a><p>The error bars are the standard error of the mean.</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0124190.g005">
              https://doi.org/10.1371/journal.pone.0124190.g005</a></p></div><a id="article1.body1.sec6.sec3.p2" name="article1.body1.sec6.sec3.p2" class="link-target"></a><p>The overall accuracy was 92.84%. Accuracy data were also submitted to a two-way repeated ANOVA, and no effect was significant. There was no evidence of a speed—accuracy trade-off.</p>
</div>

<div id="section4" class="section toc-section"><a id="sec017" name="sec017" class="link-target" title="Discussion of Experiment 3"></a>
<h3>Discussion of Experiment 3</h3>
<a id="article1.body1.sec6.sec4.p1" name="article1.body1.sec6.sec4.p1" class="link-target"></a><p>The search disadvantage for overlapping targets persisted when the target task was replaced with a luminance discrimination task. In the contingent capture hypothesis, visual features that are irrelevant to the behavioral goals are filtered out and do not cause involuntary attentional shifts [<a href="#pone.0124190.ref010" class="ref-tip">10</a>]. Our result shows that attentional control setting is not a major factor that drives this effect. Rather, it is the composition of the display—particularly, the continuous collinearity—that slows down target processing when it is spatially overlapped with the collinear structure. The target processing, regardless of orientation judgment or luminance discrimination, was interfered with. The mental set to prepare attention ready for a particular dimension is not the cause of this phenomenon.</p>
</div>
</div>

<div id="section7" class="section toc-section"><a id="sec018" name="sec018" data-toc="sec018" class="link-target" title="Experiment 4: Visual Search Practice"></a><h2>Experiment 4: Visual Search Practice</h2><a id="article1.body1.sec7.p1" name="article1.body1.sec7.p1" class="link-target"></a><p>To understand whether the search impairment effect was diminished after intense practice, we repeated the search task for 10 days. If the search impairment were under top-down control or a result of search strategy, a reduction of the impairment along experience would be expected. If, however, this effect were mainly induced by stimulus properties (e.g., salience or collinear grouping), practice should not alter the amount of such effect.</p>

<div id="section1" class="section toc-section"><a id="sec019" name="sec019" class="link-target" title="Participants"></a>
<h3>Participants</h3>
<a id="article1.body1.sec7.sec1.p1" name="article1.body1.sec7.sec1.p1" class="link-target"></a><p>Eight graduate students in China Medical University joined to complete one session a day for 10 consecutive days. They were naïve about the goal of this study and had not had experience on similar search tasks before. They had normal or corrected-to-normal vision. They received 1000 NTD for a reward.</p>
</div>

<div id="section2" class="section toc-section"><a id="sec020" name="sec020" class="link-target" title="Stimuli and Procedures"></a>
<h3>Stimuli and Procedures</h3>
<a id="article1.body1.sec7.sec2.p1" name="article1.body1.sec7.sec2.p1" class="link-target"></a><p>Two factors were designed in this experiment: distractor size and target overlapping or not. The experiment was identical to Experiment 4 in Jingling and Tseng [<a href="#pone.0124190.ref006" class="ref-tip">6</a>]: each session contained 294 trials, with seven possible target (and distractor) locations X 3 distractor size. Ten practice trials were completed before each session. Participants discriminated the target tilt as soon as possible. These four participants completed 10 sessions in 10 days.</p>
</div>

<div id="section3" class="section toc-section"><a id="sec021" name="sec021" class="link-target" title="Results of Experiment 4"></a>
<h3>Results of Experiment 4</h3>
<a id="article1.body1.sec7.sec3.p1" name="article1.body1.sec7.sec3.p1" class="link-target"></a><p>Inaccurate trials (3.62%) or the trials with RT exceeding two standard deviations of the individual mean (3.70%) were excluded. As shown in <a href="#pone-0124190-g006">Fig 6</a> (upper panel), we found that overall RT decreased with practice, from 769.16 ms for the first section reduced to 635.51 ms for the last section. However, significant improvement was found only for the first four sections, <em>F</em> (9, 63) = 6.36, <em>MSE</em> = 8413.12<em>p</em> &lt; .01, while the RTs hardly improved in the remaining sections. The overall accuracy did not significantly vary across sections, <em>p</em>s &gt; .05.</p>
<a class="link-target" id="pone-0124190-g006" name="pone-0124190-g006"></a><div class="figure" data-doi="10.1371/journal.pone.0124190.g006"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0124190.g006" data-doi="info:doi/10.1371/journal.pone.0124190" data-uri="info:doi/10.1371/journal.pone.0124190.g006"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0124190.g006" alt="thumbnail" class="thumbnail"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><div class="definition-label"><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g006">
                  PPT
                </a></div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g006">
                PowerPoint slide
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g006">
                  PNG
                </a></div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g006">
                larger image
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g006">
                  TIFF
                </a></div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g006">
                original image
              </a></li></ul></div><div class="figcaption"><span>Fig 6. </span> Overall responses across 10 sections.</div><p class="caption_target"><a id="article1.body1.sec7.sec3.fig1.caption1.p1" name="article1.body1.sec7.sec3.fig1.caption1.p1" class="link-target"></a><p>The upper panel is data of response times, while the lower panel is accuracy. The error bars are the standard error of the mean.</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0124190.g006">
              https://doi.org/10.1371/journal.pone.0124190.g006</a></p></div><a id="article1.body1.sec7.sec3.p2" name="article1.body1.sec7.sec3.p2" class="link-target"></a><p>The general responses across all observers and sections (<a href="#pone-0124190-g007">Fig 7</a>) replicated the findings in [<a href="#pone.0124190.ref006" class="ref-tip">6</a>]. Results collapsed across observers were submitted to a 10 (sections) by 3 (distractor size, 3, 9, or 21 bars) by 2 (target type, overlapping or non-overlapping) repeated-measure ANOVA. We observed the signature patterns here also: significantly higher RTs and lower accuracy for overlapping targets (694.58 ms, 87.92%) than for non-overlapping targets (612.53 ms, 94.14%), <em>F</em> (1, 7) = 160.97 and 30.25, <em>MSE</em> = 5506.14 and .015, <em>p</em>s &lt; .001, respectively. Significant interaction between distractor size and target type, <em>F</em> (2, 14) = 41.18 and 6.36, <em>MSE</em> = 1530.83 and .014, <em>p</em>s &lt; .001, respectively. Overlapping target detection was hurt only when the distractor size was 9-bar or 21-bar; <em>F</em> (1, 21) = 135.99 and 194.40, respectively; <em>MSE</em> = 2855.93, <em>p</em>s &lt; .001 for RT; <em>F</em> (1, 21) = 14.19 and 29.50, respectively; and <em>MSE</em> = .015, with <em>p</em>s &lt; .001 for accuracy.</p>
<a class="link-target" id="pone-0124190-g007" name="pone-0124190-g007"></a><div class="figure" data-doi="10.1371/journal.pone.0124190.g007"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0124190.g007" data-doi="info:doi/10.1371/journal.pone.0124190" data-uri="info:doi/10.1371/journal.pone.0124190.g007"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0124190.g007" alt="thumbnail" class="thumbnail"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><div class="definition-label"><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g007">
                  PPT
                </a></div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g007">
                PowerPoint slide
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g007">
                  PNG
                </a></div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g007">
                larger image
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g007">
                  TIFF
                </a></div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g007">
                original image
              </a></li></ul></div><div class="figcaption"><span>Fig 7. </span> RTs of each condition for the four participants.</div><p class="caption_target"><a id="article1.body1.sec7.sec3.fig2.caption1.p1" name="article1.body1.sec7.sec3.fig2.caption1.p1" class="link-target"></a><p>The error bars are the standard error of the mean.</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0124190.g007">
              https://doi.org/10.1371/journal.pone.0124190.g007</a></p></div><a id="article1.body1.sec7.sec3.p3" name="article1.body1.sec7.sec3.p3" class="link-target"></a><p>To understand how participant’s search impairment varied with sections, we normalized observers’ search impairment by introducing a search impairment index (SI), defined by the difference between RT of trials with overlapping targets and non-overlapping targets, divided by the mean RT [<a href="#pone.0124190.ref037" class="ref-tip">37</a>]. A positive value refers to search impairment, and negative value refers to search advantage of overlapping targets. <a href="#pone-0124190-g008">Fig 8</a> shows SI of three conditions in each day. As revealed by the 3-waya ANOVA, these effects did not vary with sessions, suggesting that practice did not reduce the size of search impairment in overlapping targets.</p>
<a class="link-target" id="pone-0124190-g008" name="pone-0124190-g008"></a><div class="figure" data-doi="10.1371/journal.pone.0124190.g008"><div class="img-box"><a title="Click for larger image" href="article/figure/image?size=medium&amp;id=info:doi/10.1371/journal.pone.0124190.g008" data-doi="info:doi/10.1371/journal.pone.0124190" data-uri="info:doi/10.1371/journal.pone.0124190.g008"><img src="article/figure/image?size=inline&amp;id=info:doi/10.1371/journal.pone.0124190.g008" alt="thumbnail" class="thumbnail"></a><div class="expand"></div></div><div class="figure-inline-download">
          Download:
          <ul><li><div class="definition-label"><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g008">
                  PPT
                </a></div><a href="article/figure/powerpoint?id=info:doi/10.1371/journal.pone.0124190.g008">
                PowerPoint slide
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g008">
                  PNG
                </a></div><a href="article/figure/image?download&amp;size=large&amp;id=info:doi/10.1371/journal.pone.0124190.g008">
                larger image
              </a></li><li><div class="definition-label"><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g008">
                  TIFF
                </a></div><a href="article/figure/image?download&amp;size=original&amp;id=info:doi/10.1371/journal.pone.0124190.g008">
                original image
              </a></li></ul></div><div class="figcaption"><span>Fig 8. </span> Search impairment index on RT in each session.</div><p class="caption_target"><a id="article1.body1.sec7.sec3.fig3.caption1.p1" name="article1.body1.sec7.sec3.fig3.caption1.p1" class="link-target"></a><p>The error bar is the standard error of the mean.</p>
</p><p class="caption_object"><a href="https://doi.org/10.1371/journal.pone.0124190.g008">
              https://doi.org/10.1371/journal.pone.0124190.g008</a></p></div></div>

<div id="section4" class="section toc-section"><a id="sec022" name="sec022" class="link-target" title="Discussion of Experiment 4"></a>
<h3>Discussion of Experiment 4</h3>
<a id="article1.body1.sec7.sec4.p1" name="article1.body1.sec7.sec4.p1" class="link-target"></a><p>With sufficient practice, observers continued to suffer from having slower reaction times and higher error rates when the searched-for targets were on the collinear distractor column if the collinear length were long enough. The extensive practice in 10 days was not enough to let observers develop strategies to facilitate local target orientation processing hampered by the background contextual organization.</p>
<a id="article1.body1.sec7.sec4.p2" name="article1.body1.sec7.sec4.p2" class="link-target"></a><p>Kelly and Yantis [<a href="#pone.0124190.ref012" class="ref-tip">12</a>] reported observers’ capacity to learn to ignore task-irrelevant distractors. In their design, an abrupt-onset distractor showed up in the screen corner 100ms before a central 5x5 array appeared. The target task required observers’ judgment whether more red or green dots were presented in the central array. At first, observers’ accuracy was reduced due to the distractor’s presence, but it recovered to the same level as in the no-distractor condition at the end of 1,000 practice trials. Also, observers’ improved ability to filter out task-irrelevant information was limited to the trained spatial locations and distractor types. However, if the distractor-to-learn was heterogeneous enough (i.e., a set of 520 distinct images), then the learning effect transferred across location and distractor types. This shows observers are flexible and can enable learning at multiple facets from very locally-limited information to a more global information efficiency enhancement. Our observers did not benefit practice, and there were several notable differences between the two studies worth mentioning.</p>
<a id="article1.body1.sec7.sec4.p3" name="article1.body1.sec7.sec4.p3" class="link-target"></a><p>First, the distractor and target in Kelly and Yantis [<a href="#pone.0124190.ref012" class="ref-tip">12</a>] were separated in space and time (100 ms), while in our case, the target was always on one of the element distractors. The spatial-temporal characteristics of the target-distractor may be a limiting factor for learning effect, which has not been studied much. The gradual reduced response time from the first few sessions was likely from faster target detection, key press, or decision making after familiarity of the task.</p>
<a id="article1.body1.sec7.sec4.p4" name="article1.body1.sec7.sec4.p4" class="link-target"></a><p>Secondly, feedback was provided with a reward matrix in Kelly and Yantis [<a href="#pone.0124190.ref012" class="ref-tip">12</a>], which could serve as a strong guidance for search strategy optimalization. In our case, observers were instructed to respond as fast as possible without sacrificing accuracy. No feedback was provided in our study, and our observers’ accuracy was at a level (92.69%) comparable to other similar tasks requiring observers’ fast response. Despite these differences, the clear signature difference between overlapping and non-overlapping conditions throughout all 10 sessions disagrees with the hypothesis that this impairment is from sub-optimal search strategy due to lack of exposure. More plausibly, the display property and stimuli spatial arrangement are the fundamental limits for observers to discriminate the local orientation.</p>
</div>
</div>

<div id="section8" class="section toc-section"><a id="sec023" name="sec023" data-toc="sec023" class="link-target" title="General Discussion"></a><h2>General Discussion</h2><a id="article1.body1.sec8.p1" name="article1.body1.sec8.p1" class="link-target"></a><p>In this study, we tested three top-down hypotheses about the origin of the visual search impairment generated by the presence of collinear task-irrelevant distractors in a search display. Our results did not support the view that collinear distractors were suppressed due to their low probabilistic occurrence of overlapping with the primary target, their attentional control setting in specific tasks, or lack of learning regarding search strategy development.</p>
<a id="article1.body1.sec8.p2" name="article1.body1.sec8.p2" class="link-target"></a><p>Ecologically, both selective attention and perceptual grouping are both processes that reduce our information processing load. The former directs our mental resources to the spatial areas or features that deserve more processing. The latter enhances our efficiency by quickly sorting properties likely to be possessed by the same surface or object into one grouped unit. It is not surprising that these two processes serving the same purpose interact at some point in our visual processing, but the potential neural platform and the cause of this interaction (whether it is stimulus-driven or modulated by cognition) is still largely unknown.</p>
<a id="article1.body1.sec8.p3" name="article1.body1.sec8.p3" class="link-target"></a><p>Contextual background information, even task-irrelevant, was influential in local target tasks. Driver et al. [<a href="#pone.0124190.ref038" class="ref-tip">38</a>] reported that target luminance change detection was modulated by the background context arrangement and that the detection was easier when the target was seen as the foreground. Kimchi and colleagues [<a href="#pone.0124190.ref039" class="ref-tip">39</a>] found that task-irrelevant element configuration facilitated target color identification when the target located within an object was formed by Gestalt factors such as collinearity, closure, and symmetry. An opposite cost effect was observed when the target was located outside of such object. Similar effect was observed when the target task was replaced by a vernier judgment, suggesting an attentional facilitation from a perceptual object induced by collinear/closure perceptual grouping principles [<a href="#pone.0124190.ref040" class="ref-tip">40</a>]. In our case, the collinear grouping of the distractor is most intuitively taken as a popped-out distinctive structure (<a href="#pone-0124190-g001">Fig 1</a>). However, it could just as well be taken as a divider that separates the search display into two large areas, thereby globally seen as a negligible part to which attention is rarely directed. The “distinctive structure” or “divider” view is a figure—ground perspective that can be task-defined, and the interpretation can be different or opposite in different contexts. If the collinear column in a search display is defined as a divider (i.e., ground) rather than an object, it may lead to low salience and, thus, slower response time. Indeed our eye movement study suggested that overlapping targets were perceptually less salient than non-overlapping targets [<a href="#pone.0124190.ref006" class="ref-tip">6</a>]. Because the task-dependent and context-dependent nature in figure/ground interpretation, it is beyond the bottom-up simple feature contrast computation. It requires some thought to incorporate it into a visual search model.</p>
<a id="article1.body1.sec8.p4" name="article1.body1.sec8.p4" class="link-target"></a><p>Although the interpretation of figure/ground suggests the modulation from top-down influence (e.g., task—demand), it was not supported by available models and empirical results in the case of collinear search impairment. The V1 saliency model proposed by Li [<a href="#pone.0124190.ref041" class="ref-tip">41</a>] and Zhaoping [<a href="#pone.0124190.ref042" class="ref-tip">42</a>] is the most relevant architecture that concerns collinear facilitation in addition to other basic visual features to predict how attention is deployed and attenuated. In our search display, all the component bars in the collinear column were orthogonal to their neighboring bars, thus yielding the maximum local contrast salience. According to the V1 saliency model, the integrated result from local-contrast salience and collinear grouping will determine the V1 neuronal activation and subsequent search performance. In other words, whether oriented bars aligned to each other directly contribute to perceptual salience. As demonstrated in Jingling and Zhaoping [<a href="#pone.0124190.ref043" class="ref-tip">43</a>], collinearly grouped texture boundary created higher salience than un-grouped boundary. Beyond the local feature contrast computation, any structure grouped by continuity will require a second-order computation that determines the strength of this structure in relation to other regions. The size effect observed here can be explained by the competition between the local contrast salience computation and the second-order salience computation based on perceptual grouping laws. At one end, when the collinear grouping is weak (i.e., three bars), the first-order salience computation dominates; therefore, conventional attention capture facilitation is observed when the target overlaps with the salient distractor. At the other end, when the perceptual grouping strength is increased, the second-order computation dominates attentional selection in a visual search. The above-mentioned computation is described to automatically (i.e., bottom-up) take place at V1.</p>
<a id="article1.body1.sec8.p5" name="article1.body1.sec8.p5" class="link-target"></a><p>The V1 saliency model received support from an empirical study that utilized the collinear impairment size effect and binocular fusion to explore interaction neural sites for selective attention and perceptual collinearity. Chow, Jingling, and Tseng [<a href="#pone.0124190.ref037" class="ref-tip">37</a>] separated the task-irrelevant collinear distractor column into parts and presented them to individual eyes with a stereoscope. Each eye only saw a small part too short to elicit search impairment, but when binocularly fused, the combined structure exceeds the length to slow down the target search. The result indicated that monocular information dictated the search performance, implying the origin of eye information was available even when observers were not unaware of it. This puts V1, the cortical area containing the most monocular cells [<a href="#pone.0124190.ref044" class="ref-tip">44</a>–<a href="#pone.0124190.ref045" class="ref-tip">45</a>], a highly probable candidate for the collinear grouping to interact with selective attention.</p>
<a id="article1.body1.sec8.p6" name="article1.body1.sec8.p6" class="link-target"></a><p>Another promising direction is the insight derived from the perceptual grouping model proposed by Roelfsema and Houtkamp [<a href="#pone.0124190.ref046" class="ref-tip">46</a>]. In this model, perceptual grouping involves two level of processes (i.e. base-grouping and incremental grouping). During base-grouping, individual neurons are activated based on their intrinsic preferences toward selective visual features such as orientation. Later a feedforward and recurrent processing bring in environmental information and our attentive selection. This multi-layered computation of grouping offers an architecture of grouping principles that “collinearity” becomes a property not exclusively processed at a single stage. In other words, collinearity can evoke strong activation in base-grouping due to neurons’ orientation consistency, as well as a strong activation from recurrent lateral connections that might involve attention. Our experiments have suggested that learning, probability, and attentional control setting on task set all have little effect to overcome the collinear impairment. It may imply that it was the base group principles that matter most.</p>
<a id="article1.body1.sec8.p7" name="article1.body1.sec8.p7" class="link-target"></a><p>Taken together, the converging evidence suggests that collinear search impairment is a stimulus-driven interference from perceptual grouping. Without observers’ awareness, the origin of the eye is reserved in determining the attentional search behavior. This effect is not easily maneuvered by cognitive modulations including statistical regularity information of target presence and location, attentional set control, or extensive learning. It is still an open question whether all grouping principles facilitate visual search and constrain selective attention in the same way. This will be a direction for future studies.</p>
</div>

<div id="section9" class="section toc-section"><a id="sec024" name="sec024" data-toc="sec024" class="link-target" title="Conclusions"></a><h2>Conclusions</h2><a id="article1.body1.sec9.p1" name="article1.body1.sec9.p1" class="link-target"></a><p>In summary, most models of visual search have extensively considered the dynamics of how salience information is formulated by various types of bottom-up attributes with stimuli and distractors that are similar in size. Similar efforts have also been invested to explore the role of top-down control. Our study signifies the need for additional consideration of mid-level computation, such as perceptual grouping and its role in guiding visual search. Our observations also lead us to suggest that (1) perceptual grouping interacts with visual attention in a distinct way from bottom-up salience defined by feature contrast, and therefore should be considered separately; and (2) collinear grouping is less modulated by probability information or the top-down strategy developed upon it. These two features should be incorporated into a future extension of visual selection models.</p>
</div>



<div class="contributions toc-section"><a id="authcontrib" name="authcontrib" data-toc="authcontrib" title="Author Contributions"></a><h2>Author Contributions</h2><p>Conceived and designed the experiments: CT LJ. Performed the experiments: CT LJ. Analyzed the data: CT LJ. Contributed reagents/materials/analysis tools: CT LJ. Wrote the paper: CT LJ.</p></div><div class="toc-section"><a id="references" name="references" class="link-target" data-toc="references" title="References"></a><h2>References</h2><ol class="references"><li id="ref1"><span class="order">1.
            </span><a name="pone.0124190.ref001" id="pone.0124190.ref001" class="link-target"></a>Theeuwes J. Top—down and bottom—up control of visual selection. Acta psychol. 2010; 135(2): 77–99.  pmid:20507828 <ul class="reflinks"><li><a href="#" data-author="Theeuwes" data-cit="TheeuwesJ.%20Top%E2%80%94down%20and%20bottom%E2%80%94up%20control%20of%20visual%20selection.%20Acta%20psychol.%202010%3B%20135%282%29%3A%2077%E2%80%9399.%20doi%3A%2010.1016%2Fj.actpsy.2010.02.006%2020507828" data-title="Top%E2%80%94down%20and%20bottom%E2%80%94up%20control%20of%20visual%20selection" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/20507828" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Top%E2%80%94down+and+bottom%E2%80%94up+control+of+visual+selection+Theeuwes+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref2"><span class="order">2.
            </span><a name="pone.0124190.ref002" id="pone.0124190.ref002" class="link-target"></a>Turatto M, Galfano G. Color, form and luminance capture attention in visual search. Vision Res. 2000; 40, 1639–1643. pmid:10814751 <ul class="reflinks"><li><a href="#" data-author="Turatto" data-cit="TurattoM%2C%20GalfanoG.%20Color%2C%20form%20and%20luminance%20capture%20attention%20in%20visual%20search.%20Vision%20Res.%202000%3B%2040%2C%201639%E2%80%931643.%2010814751" data-title="Color%2C%20form%20and%20luminance%20capture%20attention%20in%20visual%20search" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/10814751" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Color%2C+form+and+luminance+capture+attention+in+visual+search+Turatto+2000" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref3"><span class="order">3.
            </span><a name="pone.0124190.ref003" id="pone.0124190.ref003" class="link-target"></a>Jonides J, Yantis S. Uniqueness of abrupt visual onset in capturing attention. Percept Psychophys. 1988; 43, 346–354. pmid:3362663 <ul class="reflinks"><li><a href="#" data-author="Jonides" data-cit="JonidesJ%2C%20YantisS.%20Uniqueness%20of%20abrupt%20visual%20onset%20in%20capturing%20attention.%20Percept%20Psychophys.%201988%3B%2043%2C%20346%E2%80%93354.%203362663" data-title="Uniqueness%20of%20abrupt%20visual%20onset%20in%20capturing%20attention" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/3362663" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Uniqueness+of+abrupt+visual+onset+in+capturing+attention+Jonides+1988" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref4"><span class="order">4.
            </span><a name="pone.0124190.ref004" id="pone.0124190.ref004" class="link-target"></a>Turatto M, Galfano G. Attentional capture by color without any relevant attentional set. Percept Psychophys. 2001; 63, 286–297. pmid:11281103 <ul class="reflinks"><li><a href="#" data-author="Turatto" data-cit="TurattoM%2C%20GalfanoG.%20Attentional%20capture%20by%20color%20without%20any%20relevant%20attentional%20set.%20Percept%20Psychophys.%202001%3B%2063%2C%20286%E2%80%93297.%2011281103" data-title="Attentional%20capture%20by%20color%20without%20any%20relevant%20attentional%20set" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/11281103" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Attentional+capture+by+color+without+any+relevant+attentional+set+Turatto+2001" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref5"><span class="order">5.
            </span><a name="pone.0124190.ref005" id="pone.0124190.ref005" class="link-target"></a>Yantis S, Jonides J. Abrupt visual onsets and selective attention: evidence from visual search. J Exp Psychol Human. 1984; 10, 601–621. pmid:6238122 <ul class="reflinks"><li><a href="#" data-author="Yantis" data-cit="YantisS%2C%20JonidesJ.%20Abrupt%20visual%20onsets%20and%20selective%20attention%3A%20evidence%20from%20visual%20search.%20J%20Exp%20Psychol%20Human.%201984%3B%2010%2C%20601%E2%80%93621.%206238122" data-title="Abrupt%20visual%20onsets%20and%20selective%20attention%3A%20evidence%20from%20visual%20search" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/6238122" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Abrupt+visual+onsets+and+selective+attention%3A+evidence+from+visual+search+Yantis+1984" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref6"><span class="order">6.
            </span><a name="pone.0124190.ref006" id="pone.0124190.ref006" class="link-target"></a>Jingling L, Tseng CH. Collinearity impairs local element visual search. J Exp Psychol Human. 2013; 39, 156–167.  pmid:22329767 <ul class="reflinks"><li><a href="#" data-author="Jingling" data-cit="JinglingL%2C%20TsengCH.%20Collinearity%20impairs%20local%20element%20visual%20search.%20J%20Exp%20Psychol%20Human.%202013%3B%2039%2C%20156%E2%80%93167.%20doi%3A%2010.1037%2Fa0027325%2022329767" data-title="Collinearity%20impairs%20local%20element%20visual%20search" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/22329767" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Collinearity+impairs+local+element+visual+search+Jingling+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref7"><span class="order">7.
            </span><a name="pone.0124190.ref007" id="pone.0124190.ref007" class="link-target"></a>Geyer T, Müller HJ, Krummenacher J. Expectancies modulate attentional capture by salient color singletons. Vision Res. 2008; 48, 1315–1326.  pmid:18407311 <ul class="reflinks"><li><a href="#" data-author="Geyer" data-cit="GeyerT%2C%20M%C3%BCllerHJ%2C%20KrummenacherJ.%20Expectancies%20modulate%20attentional%20capture%20by%20salient%20color%20singletons.%20Vision%20Res.%202008%3B%2048%2C%201315%E2%80%931326.%20doi%3A%2010.1016%2Fj.visres.2008.02.006%2018407311" data-title="Expectancies%20modulate%20attentional%20capture%20by%20salient%20color%20singletons" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/18407311" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Expectancies+modulate+attentional+capture+by+salient+color+singletons+Geyer+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref8"><span class="order">8.
            </span><a name="pone.0124190.ref008" id="pone.0124190.ref008" class="link-target"></a>Müller HJ, Geyer T, Zehetleitner M, Krummenacher J. Attentional capture by salient color singleton distractors is modulated by top-down dimensional set. J Exp Psychol Human. 2009; 35, 1–16. <ul class="reflinks"><li><a href="#" data-author="M%C3%BCller" data-cit="M%C3%BCllerHJ%2C%20GeyerT%2C%20ZehetleitnerM%2C%20KrummenacherJ.%20Attentional%20capture%20by%20salient%20color%20singleton%20distractors%20is%20modulated%20by%20top-down%20dimensional%20set.%20J%20Exp%20Psychol%20Human.%202009%3B%2035%2C%201%E2%80%9316." data-title="Attentional%20capture%20by%20salient%20color%20singleton%20distractors%20is%20modulated%20by%20top-down%20dimensional%20set" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Attentional+capture+by+salient+color+singleton+distractors+is+modulated+by+top-down+dimensional+set+M%C3%BCller+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref9"><span class="order">9.
            </span><a name="pone.0124190.ref009" id="pone.0124190.ref009" class="link-target"></a>Sayim B, Grubert A, Herzog MH, Krummenacher J. Display probability modulates attentional capture by onset distractors. J Vision. 2010; 103, 1–8. <ul class="reflinks"><li><a href="#" data-author="Sayim" data-cit="SayimB%2C%20GrubertA%2C%20HerzogMH%2C%20KrummenacherJ.%20Display%20probability%20modulates%20attentional%20capture%20by%20onset%20distractors.%20J%20Vision.%202010%3B%20103%2C%201%E2%80%938." data-title="Display%20probability%20modulates%20attentional%20capture%20by%20onset%20distractors" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Display+probability+modulates+attentional+capture+by+onset+distractors+Sayim+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref10"><span class="order">10.
            </span><a name="pone.0124190.ref010" id="pone.0124190.ref010" class="link-target"></a>Folk CL, Remington RW, Johnston JC. Involuntary covert orienting is contingent on attentional control settings. J Exp Psychol Human. 1992; 18, 1030–1044. pmid:1431742 <ul class="reflinks"><li><a href="#" data-author="Folk" data-cit="FolkCL%2C%20RemingtonRW%2C%20JohnstonJC.%20Involuntary%20covert%20orienting%20is%20contingent%20on%20attentional%20control%20settings.%20J%20Exp%20Psychol%20Human.%201992%3B%2018%2C%201030%E2%80%931044.%201431742" data-title="Involuntary%20covert%20orienting%20is%20contingent%20on%20attentional%20control%20settings" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/1431742" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Involuntary+covert+orienting+is+contingent+on+attentional+control+settings+Folk+1992" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref11"><span class="order">11.
            </span><a name="pone.0124190.ref011" id="pone.0124190.ref011" class="link-target"></a>Folk CL, Remington RW. Selectivity in distraction by irrelevant featural singletons: Evidence for two forms of attentional capture. J Exp Psychol Human.1998; 24, 847–858. pmid:9627420 <ul class="reflinks"><li><a href="#" data-author="Folk" data-cit="FolkCL%2C%20RemingtonRW.%20Selectivity%20in%20distraction%20by%20irrelevant%20featural%20singletons%3A%20Evidence%20for%20two%20forms%20of%20attentional%20capture.%20J%20Exp%20Psychol%20Human.1998%3B%2024%2C%20847%E2%80%93858.%209627420" data-title="Selectivity%20in%20distraction%20by%20irrelevant%20featural%20singletons%3A%20Evidence%20for%20two%20forms%20of%20attentional%20capture" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/9627420" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Selectivity+in+distraction+by+irrelevant+featural+singletons%3A+Evidence+for+two+forms+of+attentional+capture+Folk+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref12"><span class="order">12.
            </span><a name="pone.0124190.ref012" id="pone.0124190.ref012" class="link-target"></a>Kelley T, Yantis S. Learning to attend: Effects of practice on information selection. J Vision. 2009; 9, 1–18. <ul class="reflinks"><li><a href="#" data-author="Kelley" data-cit="KelleyT%2C%20YantisS.%20Learning%20to%20attend%3A%20Effects%20of%20practice%20on%20information%20selection.%20J%20Vision.%202009%3B%209%2C%201%E2%80%9318." data-title="Learning%20to%20attend%3A%20Effects%20of%20practice%20on%20information%20selection" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Learning+to+attend%3A+Effects+of+practice+on+information+selection+Kelley+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref13"><span class="order">13.
            </span><a name="pone.0124190.ref013" id="pone.0124190.ref013" class="link-target"></a>Cohen A, Magen H. Intra- and cross-dimensional visual search for single-feature targets. Percept Psychophys. 1999; 61, 291–307. pmid:10089762 <ul class="reflinks"><li><a href="#" data-author="Cohen" data-cit="CohenA%2C%20MagenH.%20Intra-%20and%20cross-dimensional%20visual%20search%20for%20single-feature%20targets.%20Percept%20Psychophys.%201999%3B%2061%2C%20291%E2%80%93307.%2010089762" data-title="Intra-%20and%20cross-dimensional%20visual%20search%20for%20single-feature%20targets" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/10089762" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Intra-+and+cross-dimensional+visual+search+for+single-feature+targets+Cohen+1999" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref14"><span class="order">14.
            </span><a name="pone.0124190.ref014" id="pone.0124190.ref014" class="link-target"></a>Mortier K, Theeuwes J, Starreveld P. Response Selection Modulates Visual Search Within and Across Dimensions. J Exp Psychol Human. 2005; 31, 542–557. pmid:15982130 <ul class="reflinks"><li><a href="#" data-author="Mortier" data-cit="MortierK%2C%20TheeuwesJ%2C%20StarreveldP.%20Response%20Selection%20Modulates%20Visual%20Search%20Within%20and%20Across%20Dimensions.%20J%20Exp%20Psychol%20Human.%202005%3B%2031%2C%20542%E2%80%93557.%2015982130" data-title="Response%20Selection%20Modulates%20Visual%20Search%20Within%20and%20Across%20Dimensions" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/15982130" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Response+Selection+Modulates+Visual+Search+Within+and+Across+Dimensions+Mortier+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref15"><span class="order">15.
            </span><a name="pone.0124190.ref015" id="pone.0124190.ref015" class="link-target"></a>Theeuwes J. Cross-dimensional perceptual selectivity. Percept. Psychophys. 1991; 50, 184–193. pmid:1945740 <ul class="reflinks"><li><a href="#" data-author="Theeuwes" data-cit="TheeuwesJ.%20Cross-dimensional%20perceptual%20selectivity.%20Percept.%20Psychophys.%201991%3B%2050%2C%20184%E2%80%93193.%201945740" data-title="Cross-dimensional%20perceptual%20selectivity.%20Percept" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/1945740" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Cross-dimensional+perceptual+selectivity.+Percept+Theeuwes+1991" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref16"><span class="order">16.
            </span><a name="pone.0124190.ref016" id="pone.0124190.ref016" class="link-target"></a>Theeuwes J. Perceptual selectivity for color and form. Percept. Psychophys. 1992; 51, 599–606. pmid:1620571 <ul class="reflinks"><li><a href="#" data-author="Theeuwes" data-cit="TheeuwesJ.%20Perceptual%20selectivity%20for%20color%20and%20form.%20Percept.%20Psychophys.%201992%3B%2051%2C%20599%E2%80%93606.%201620571" data-title="Perceptual%20selectivity%20for%20color%20and%20form.%20Percept" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/1620571" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Perceptual+selectivity+for+color+and+form.+Percept+Theeuwes+1992" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref17"><span class="order">17.
            </span><a name="pone.0124190.ref017" id="pone.0124190.ref017" class="link-target"></a>Theeuwes J, Reimann B, Mortier K. Visual search for featural singletons: No top-down modulation, only bottom-up priming. Vis Cogn.2006; 14, 466–489. <ul class="reflinks"><li><a href="#" data-author="Theeuwes" data-cit="TheeuwesJ%2C%20ReimannB%2C%20MortierK.%20Visual%20search%20for%20featural%20singletons%3A%20No%20top-down%20modulation%2C%20only%20bottom-up%20priming.%20Vis%20Cogn.2006%3B%2014%2C%20466%E2%80%93489." data-title="Visual%20search%20for%20featural%20singletons%3A%20No%20top-down%20modulation%2C%20only%20bottom-up%20priming" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Visual+search+for+featural+singletons%3A+No+top-down+modulation%2C+only+bottom-up+priming+Theeuwes+2006" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref18"><span class="order">18.
            </span><a name="pone.0124190.ref018" id="pone.0124190.ref018" class="link-target"></a>Chun MM, Jiang Y. Contextual cueing: implicit learning and memory of visual context guides spatial attention. Cognitive Psychol. 1998; 36, 28–71. pmid:9679076 <ul class="reflinks"><li><a href="#" data-author="Chun" data-cit="ChunMM%2C%20JiangY.%20Contextual%20cueing%3A%20implicit%20learning%20and%20memory%20of%20visual%20context%20guides%20spatial%20attention.%20Cognitive%20Psychol.%201998%3B%2036%2C%2028%E2%80%9371.%209679076" data-title="Contextual%20cueing%3A%20implicit%20learning%20and%20memory%20of%20visual%20context%20guides%20spatial%20attention" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/9679076" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Contextual+cueing%3A+implicit+learning+and+memory+of+visual+context+guides+spatial+attention+Chun+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref19"><span class="order">19.
            </span><a name="pone.0124190.ref019" id="pone.0124190.ref019" class="link-target"></a>Fiser J, Aslin RN. Unsupervised statistical learning of higher-order spatial structures from visual scenes. Psychol Sci. 2001; 12, 499–504. pmid:11760138 <ul class="reflinks"><li><a href="#" data-author="Fiser" data-cit="FiserJ%2C%20AslinRN.%20Unsupervised%20statistical%20learning%20of%20higher-order%20spatial%20structures%20from%20visual%20scenes.%20Psychol%20Sci.%202001%3B%2012%2C%20499%E2%80%93504.%2011760138" data-title="Unsupervised%20statistical%20learning%20of%20higher-order%20spatial%20structures%20from%20visual%20scenes" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/11760138" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Unsupervised+statistical+learning+of+higher-order+spatial+structures+from+visual+scenes+Fiser+2001" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref20"><span class="order">20.
            </span><a name="pone.0124190.ref020" id="pone.0124190.ref020" class="link-target"></a>Fiser J, Aslin RN. Statistical learning of higher order temporal structure from visual shape sequences. J Exp Psychol Learn. 2002; 28, 458–467. pmid:12018498 <ul class="reflinks"><li><a href="#" data-author="Fiser" data-cit="FiserJ%2C%20AslinRN.%20Statistical%20learning%20of%20higher%20order%20temporal%20structure%20from%20visual%20shape%20sequences.%20J%20Exp%20Psychol%20Learn.%202002%3B%2028%2C%20458%E2%80%93467.%2012018498" data-title="Statistical%20learning%20of%20higher%20order%20temporal%20structure%20from%20visual%20shape%20sequences" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/12018498" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Statistical+learning+of+higher+order+temporal+structure+from+visual+shape+sequences+Fiser+2002" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref21"><span class="order">21.
            </span><a name="pone.0124190.ref021" id="pone.0124190.ref021" class="link-target"></a>Geng JJ, Behrmann M. Probability cuing of target location facilitates visual search implicitly in normal participants and patients with hemispatial neglect. Psychol Sci. 2002; 13, 520–525. pmid:12430835 <ul class="reflinks"><li><a href="#" data-author="Geng" data-cit="GengJJ%2C%20BehrmannM.%20Probability%20cuing%20of%20target%20location%20facilitates%20visual%20search%20implicitly%20in%20normal%20participants%20and%20patients%20with%20hemispatial%20neglect.%20Psychol%20Sci.%202002%3B%2013%2C%20520%E2%80%93525.%2012430835" data-title="Probability%20cuing%20of%20target%20location%20facilitates%20visual%20search%20implicitly%20in%20normal%20participants%20and%20patients%20with%20hemispatial%20neglect" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/12430835" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Probability+cuing+of+target+location+facilitates+visual+search+implicitly+in+normal+participants+and+patients+with+hemispatial+neglect+Geng+2002" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref22"><span class="order">22.
            </span><a name="pone.0124190.ref022" id="pone.0124190.ref022" class="link-target"></a>Geng JJ, Behrmann M. Spatial probability as an attentional cue in visual search. Percept Psychophys. 2005; 67, 1252–1268. pmid:16502846 <ul class="reflinks"><li><a href="#" data-author="Geng" data-cit="GengJJ%2C%20BehrmannM.%20Spatial%20probability%20as%20an%20attentional%20cue%20in%20visual%20search.%20Percept%20Psychophys.%202005%3B%2067%2C%201252%E2%80%931268.%2016502846" data-title="Spatial%20probability%20as%20an%20attentional%20cue%20in%20visual%20search" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/16502846" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Spatial+probability+as+an+attentional+cue+in+visual+search+Geng+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref23"><span class="order">23.
            </span><a name="pone.0124190.ref023" id="pone.0124190.ref023" class="link-target"></a>Turk-Browne NB, Jungé JA, Scholl BJ. The automaticity of visual statistical learning. J Exp Psychol Gen. 2005; 234, 552–564. <ul class="reflinks"><li><a href="#" data-author="Turk-Browne" data-cit="Turk-BrowneNB%2C%20Jung%C3%A9JA%2C%20SchollBJ.%20The%20automaticity%20of%20visual%20statistical%20learning.%20J%20Exp%20Psychol%20Gen.%202005%3B%20234%2C%20552%E2%80%93564." data-title="The%20automaticity%20of%20visual%20statistical%20learning" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=The+automaticity+of+visual+statistical+learning+Turk-Browne+2005" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref24"><span class="order">24.
            </span><a name="pone.0124190.ref024" id="pone.0124190.ref024" class="link-target"></a>Farrell S, Ludwig CJ, Ellis LA, Gilchrist ID. Influence of environmental statistics on inhibition of saccadic return. P Natl Acad Sci USA. 2009; 107, 929–934. <ul class="reflinks"><li><a href="#" data-author="Farrell" data-cit="FarrellS%2C%20LudwigCJ%2C%20EllisLA%2C%20GilchristID.%20Influence%20of%20environmental%20statistics%20on%20inhibition%20of%20saccadic%20return.%20P%20Natl%20Acad%20Sci%20USA.%202009%3B%20107%2C%20929%E2%80%93934." data-title="Influence%20of%20environmental%20statistics%20on%20inhibition%20of%20saccadic%20return" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Influence+of+environmental+statistics+on+inhibition+of+saccadic+return+Farrell+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref25"><span class="order">25.
            </span><a name="pone.0124190.ref025" id="pone.0124190.ref025" class="link-target"></a>Liu CL, Chiau HY, Tseng P, Hung DL, Tzeng OJL, Muggleton NG, et al. The antisaccade cost is modulated by contextual experience of location probability. J Neurophysiol. 2010; 103, 1438–1447.  pmid:20032240 <ul class="reflinks"><li><a href="#" data-author="Liu" data-cit="LiuCL%2C%20ChiauHY%2C%20TsengP%2C%20HungDL%2C%20TzengOJL%2C%20MuggletonNG%2C%20et%20al.%20The%20antisaccade%20cost%20is%20modulated%20by%20contextual%20experience%20of%20location%20probability.%20J%20Neurophysiol.%202010%3B%20103%2C%201438%E2%80%931447.%20doi%3A%2010.1152%2Fjn.00815.2009%2020032240" data-title="The%20antisaccade%20cost%20is%20modulated%20by%20contextual%20experience%20of%20location%20probability" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/20032240" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+antisaccade+cost+is+modulated+by+contextual+experience+of+location+probability+Liu+2010" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref26"><span class="order">26.
            </span><a name="pone.0124190.ref026" id="pone.0124190.ref026" class="link-target"></a>Folk CL, Annett S. Do locally defined feature discontinuities capture attention? Percept Psychophys. 1994; 56, 277–287. pmid:7971128 <ul class="reflinks"><li><a href="#" data-author="Folk" data-cit="FolkCL%2C%20AnnettS.%20Do%20locally%20defined%20feature%20discontinuities%20capture%20attention%3F%20Percept%20Psychophys.%201994%3B%2056%2C%20277%E2%80%93287.%207971128" data-title="Do%20locally%20defined%20feature%20discontinuities%20capture%20attention%3F" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/7971128" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Do+locally+defined+feature+discontinuities+capture+attention%3F+Folk+1994" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref27"><span class="order">27.
            </span><a name="pone.0124190.ref027" id="pone.0124190.ref027" class="link-target"></a>Bacon WF, Egeth HE. Overriding stimulus-driven attentional capture. Percept Psychophys. 1994; 55, 485–496. pmid:8008550 <ul class="reflinks"><li><a href="#" data-author="Bacon" data-cit="BaconWF%2C%20EgethHE.%20Overriding%20stimulus-driven%20attentional%20capture.%20Percept%20Psychophys.%201994%3B%2055%2C%20485%E2%80%93496.%208008550" data-title="Overriding%20stimulus-driven%20attentional%20capture" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/8008550" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Overriding+stimulus-driven+attentional+capture+Bacon+1994" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref28"><span class="order">28.
            </span><a name="pone.0124190.ref028" id="pone.0124190.ref028" class="link-target"></a>Gibson BS, Kelsey EM. Stimulus-driven attentional capture is contingent on attentional set for displaywide visual features. J Exp Psychol Human. 1998; 24, 699–706. pmid:9627409 <ul class="reflinks"><li><a href="#" data-author="Gibson" data-cit="GibsonBS%2C%20KelseyEM.%20Stimulus-driven%20attentional%20capture%20is%20contingent%20on%20attentional%20set%20for%20displaywide%20visual%20features.%20J%20Exp%20Psychol%20Human.%201998%3B%2024%2C%20699%E2%80%93706.%209627409" data-title="Stimulus-driven%20attentional%20capture%20is%20contingent%20on%20attentional%20set%20for%20displaywide%20visual%20features" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/9627409" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Stimulus-driven+attentional+capture+is+contingent+on+attentional+set+for+displaywide+visual+features+Gibson+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref29"><span class="order">29.
            </span><a name="pone.0124190.ref029" id="pone.0124190.ref029" class="link-target"></a>Jingling L, Yeh SL. New objects do not capture attention without a setting: Evidence from inattentional blindness. Vis Cogn. 2007; 15, 661–684. <ul class="reflinks"><li><a href="#" data-author="Jingling" data-cit="JinglingL%2C%20YehSL.%20New%20objects%20do%20not%20capture%20attention%20without%20a%20setting%3A%20Evidence%20from%20inattentional%20blindness.%20Vis%20Cogn.%202007%3B%2015%2C%20661%E2%80%93684." data-title="New%20objects%20do%20not%20capture%20attention%20without%20a%20setting%3A%20Evidence%20from%20inattentional%20blindness" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=New+objects+do+not+capture+attention+without+a+setting%3A+Evidence+from+inattentional+blindness+Jingling+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref30"><span class="order">30.
            </span><a name="pone.0124190.ref030" id="pone.0124190.ref030" class="link-target"></a>Liu L, Kuyk T, Fuhr P. Visual search training in subjects with severe to profound low vision. Vision Res.2007; 47, 2627–2636. pmid:17707452 <ul class="reflinks"><li><a href="#" data-author="Liu" data-cit="LiuL%2C%20KuykT%2C%20FuhrP.%20Visual%20search%20training%20in%20subjects%20with%20severe%20to%20profound%20low%20vision.%20Vision%20Res.2007%3B%2047%2C%202627%E2%80%932636.%2017707452" data-title="Visual%20search%20training%20in%20subjects%20with%20severe%20to%20profound%20low%20vision" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17707452" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Visual+search+training+in+subjects+with+severe+to+profound+low+vision+Liu+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref31"><span class="order">31.
            </span><a name="pone.0124190.ref031" id="pone.0124190.ref031" class="link-target"></a>Sireteanu R, Rettenbach R. Perceptual learning in visual search: Fast, enduring, but non-specific. Vision Res.1995; 35, 2037–2043. pmid:7660607 <ul class="reflinks"><li><a href="#" data-author="Sireteanu" data-cit="SireteanuR%2C%20RettenbachR.%20Perceptual%20learning%20in%20visual%20search%3A%20Fast%2C%20enduring%2C%20but%20non-specific.%20Vision%20Res.1995%3B%2035%2C%202037%E2%80%932043.%207660607" data-title="Perceptual%20learning%20in%20visual%20search%3A%20Fast%2C%20enduring%2C%20but%20non-specific" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/7660607" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Perceptual+learning+in+visual+search%3A+Fast%2C+enduring%2C+but+non-specific+Sireteanu+1995" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref32"><span class="order">32.
            </span><a name="pone.0124190.ref032" id="pone.0124190.ref032" class="link-target"></a>Sireteanu R, Rettenbach R. Perceptual learning in visual search generalizes over tasks, locations, and eyes Vision Res. 2000; 40, 2925–2949. pmid:11000393 <ul class="reflinks"><li><a href="#" data-author="Sireteanu" data-cit="SireteanuR%2C%20RettenbachR.%20Perceptual%20learning%20in%20visual%20search%20generalizes%20over%20tasks%2C%20locations%2C%20and%20eyes%20Vision%20Res.%202000%3B%2040%2C%202925%E2%80%932949.%2011000393" data-title="Perceptual%20learning%20in%20visual%20search%20generalizes%20over%20tasks%2C%20locations%2C%20and%20eyes" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/11000393" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Perceptual+learning+in+visual+search+generalizes+over+tasks%2C+locations%2C+and+eyes+Sireteanu+2000" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref33"><span class="order">33.
            </span><a name="pone.0124190.ref033" id="pone.0124190.ref033" class="link-target"></a>Hess RF, Dakin SC, Field DJ. The role of “contrast enhancement” in the detection and appearance of visual contours. Vision Res. 1998; 38, 783–787. pmid:9624429 <ul class="reflinks"><li><a href="#" data-author="Hess" data-cit="HessRF%2C%20DakinSC%2C%20FieldDJ.%20The%20role%20of%20%E2%80%9Ccontrast%20enhancement%E2%80%9D%20in%20the%20detection%20and%20appearance%20of%20visual%20contours.%20Vision%20Res.%201998%3B%2038%2C%20783%E2%80%93787.%209624429" data-title="The%20role%20of%20%E2%80%9Ccontrast%20enhancement%E2%80%9D%20in%20the%20detection%20and%20appearance%20of%20visual%20contours" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/9624429" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+role+of+%E2%80%9Ccontrast+enhancement%E2%80%9D+in+the+detection+and+appearance+of+visual+contours+Hess+1998" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref34"><span class="order">34.
            </span><a name="pone.0124190.ref034" id="pone.0124190.ref034" class="link-target"></a>Brainard DH. The psychophysics toolbox. Spatial Vision. 1997; 10, 433–436. pmid:9176952 <ul class="reflinks"><li><a href="#" data-author="Brainard" data-cit="BrainardDH.%20The%20psychophysics%20toolbox.%20Spatial%20Vision.%201997%3B%2010%2C%20433%E2%80%93436.%209176952" data-title="The%20psychophysics%20toolbox" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/9176952" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+psychophysics+toolbox+Brainard+1997" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref35"><span class="order">35.
            </span><a name="pone.0124190.ref035" id="pone.0124190.ref035" class="link-target"></a>Pelli DG. The VideoToolbox software for visual psychophysics: transforming numbers into movies. Spatial Vision. 1997; 10, 437–442. pmid:9176953 <ul class="reflinks"><li><a href="#" data-author="Pelli" data-cit="PelliDG.%20The%20VideoToolbox%20software%20for%20visual%20psychophysics%3A%20transforming%20numbers%20into%20movies.%20Spatial%20Vision.%201997%3B%2010%2C%20437%E2%80%93442.%209176953" data-title="The%20VideoToolbox%20software%20for%20visual%20psychophysics%3A%20transforming%20numbers%20into%20movies" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/9176953" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=The+VideoToolbox+software+for+visual+psychophysics%3A+transforming+numbers+into+movies+Pelli+1997" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref36"><span class="order">36.
            </span><a name="pone.0124190.ref036" id="pone.0124190.ref036" class="link-target"></a>Chun MM, Jiang Y. Implicit, long-term spatial contextual memory. J Exp Psychol Learn. 2003; 29, 224–234. pmid:12696811 <ul class="reflinks"><li><a href="#" data-author="Chun" data-cit="ChunMM%2C%20JiangY.%20Implicit%2C%20long-term%20spatial%20contextual%20memory.%20J%20Exp%20Psychol%20Learn.%202003%3B%2029%2C%20224%E2%80%93234.%2012696811" data-title="Implicit%2C%20long-term%20spatial%20contextual%20memory" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/12696811" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Implicit%2C+long-term+spatial+contextual+memory+Chun+2003" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref37"><span class="order">37.
            </span><a name="pone.0124190.ref037" id="pone.0124190.ref037" class="link-target"></a>Chow HM, Jingling L, Tseng CH. Collinear integration affects visual search at V1. J Vision. 2013; 13, 1–20. <ul class="reflinks"><li><a href="#" data-author="Chow" data-cit="ChowHM%2C%20JinglingL%2C%20TsengCH.%20Collinear%20integration%20affects%20visual%20search%20at%20V1.%20J%20Vision.%202013%3B%2013%2C%201%E2%80%9320." data-title="Collinear%20integration%20affects%20visual%20search%20at%20V1" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Collinear+integration+affects+visual+search+at+V1+Chow+2013" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref38"><span class="order">38.
            </span><a name="pone.0124190.ref038" id="pone.0124190.ref038" class="link-target"></a>Driver J, Davis G, Russell C, Turatto M, Freeman E. Segmentation, attention and phenomenal visual objects. Cogn. 2001; 80, 61–95. <ul class="reflinks"><li><a href="#" data-author="Driver" data-cit="DriverJ%2C%20DavisG%2C%20RussellC%2C%20TurattoM%2C%20FreemanE.%20Segmentation%2C%20attention%20and%20phenomenal%20visual%20objects.%20Cogn.%202001%3B%2080%2C%2061%E2%80%9395." data-title="Segmentation%2C%20attention%20and%20phenomenal%20visual%20objects" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://scholar.google.com/scholar?q=Segmentation%2C+attention+and+phenomenal+visual+objects+Driver+2001" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref39"><span class="order">39.
            </span><a name="pone.0124190.ref039" id="pone.0124190.ref039" class="link-target"></a>Kimchi R, Yeshurun Y, Cohen-Savransky A. Automatic, stimulus-driven attentional capture by objecthood. Psychon B Rev. 2007; 14, 166–172. pmid:17546748 <ul class="reflinks"><li><a href="#" data-author="Kimchi" data-cit="KimchiR%2C%20YeshurunY%2C%20Cohen-SavranskyA.%20Automatic%2C%20stimulus-driven%20attentional%20capture%20by%20objecthood.%20Psychon%20B%20Rev.%202007%3B%2014%2C%20166%E2%80%93172.%2017546748" data-title="Automatic%2C%20stimulus-driven%20attentional%20capture%20by%20objecthood" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/17546748" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Automatic%2C+stimulus-driven+attentional+capture+by+objecthood+Kimchi+2007" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref40"><span class="order">40.
            </span><a name="pone.0124190.ref040" id="pone.0124190.ref040" class="link-target"></a>Yeshurun Y, Kimchi R, Sha’shoua G, Carmel T. Perceptual objects capture attention. Vision Res. 2009; 49, 1329–1335.  pmid:18299141 <ul class="reflinks"><li><a href="#" data-author="Yeshurun" data-cit="YeshurunY%2C%20KimchiR%2C%20Sha%E2%80%99shouaG%2C%20CarmelT.%20Perceptual%20objects%20capture%20attention.%20Vision%20Res.%202009%3B%2049%2C%201329%E2%80%931335.%20doi%3A%2010.1016%2Fj.visres.2008.01.014%2018299141" data-title="Perceptual%20objects%20capture%20attention" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/18299141" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Perceptual+objects+capture+attention+Yeshurun+2009" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref41"><span class="order">41.
            </span><a name="pone.0124190.ref041" id="pone.0124190.ref041" class="link-target"></a>Li Z. A saliency map in primary visual cortex. Trends Cogn Sci. 2002; 6, 9–16. pmid:11849610 <ul class="reflinks"><li><a href="#" data-author="Li" data-cit="LiZ.%20A%20saliency%20map%20in%20primary%20visual%20cortex.%20Trends%20Cogn%20Sci.%202002%3B%206%2C%209%E2%80%9316.%2011849610" data-title="A%20saliency%20map%20in%20primary%20visual%20cortex" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/11849610" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=A+saliency+map+in+primary+visual+cortex+Li+2002" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref42"><span class="order">42.
            </span><a name="pone.0124190.ref042" id="pone.0124190.ref042" class="link-target"></a>Zhaoping L. The primary visual cortex creates a bottom-up saliency map. In: Itti L, Rees G, Tsotsos JK, editors. Neurobiology of attention. Elsevier: Amsterdam; 2005. pp. 570–575. <ul class="find-nolinks"></ul></li><li id="ref43"><span class="order">43.
            </span><a name="pone.0124190.ref043" id="pone.0124190.ref043" class="link-target"></a>Jingling L, Zhaoping L. Change detection is easier at texture border bars when they are parallel to the border: Evidence for V1 mechanisms of bottom-up salience. Perception. 2008; 37(2), 197–206. pmid:18456924 <ul class="reflinks"><li><a href="#" data-author="Jingling" data-cit="JinglingL%2C%20ZhaopingL.%20Change%20detection%20is%20easier%20at%20texture%20border%20bars%20when%20they%20are%20parallel%20to%20the%20border%3A%20Evidence%20for%20V1%20mechanisms%20of%20bottom-up%20salience.%20Perception.%202008%3B%2037%282%29%2C%20197%E2%80%93206.%2018456924" data-title="Change%20detection%20is%20easier%20at%20texture%20border%20bars%20when%20they%20are%20parallel%20to%20the%20border%3A%20Evidence%20for%20V1%20mechanisms%20of%20bottom-up%20salience" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/18456924" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Change+detection+is+easier+at+texture+border+bars+when+they+are+parallel+to+the+border%3A+Evidence+for+V1+mechanisms+of+bottom-up+salience+Jingling+2008" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref44"><span class="order">44.
            </span><a name="pone.0124190.ref044" id="pone.0124190.ref044" class="link-target"></a>Hubel DH, Livingstone MS. Segregation of form, color, and stereopsis in primate area 18. J Neurosci. 1987; 7, 3378–3415. pmid:2824714 <ul class="reflinks"><li><a href="#" data-author="Hubel" data-cit="HubelDH%2C%20LivingstoneMS.%20Segregation%20of%20form%2C%20color%2C%20and%20stereopsis%20in%20primate%20area%2018.%20J%20Neurosci.%201987%3B%207%2C%203378%E2%80%933415.%202824714" data-title="Segregation%20of%20form%2C%20color%2C%20and%20stereopsis%20in%20primate%20area%2018" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/2824714" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Segregation+of+form%2C+color%2C+and+stereopsis+in+primate+area+18+Hubel+1987" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref45"><span class="order">45.
            </span><a name="pone.0124190.ref045" id="pone.0124190.ref045" class="link-target"></a>Hubel DH, Wiesel TN. Receptive fields and functional architecture of monkey striate cortex. J Physiol. 1968; 195, 215–243. pmid:4966457 <ul class="reflinks"><li><a href="#" data-author="Hubel" data-cit="HubelDH%2C%20WieselTN.%20Receptive%20fields%20and%20functional%20architecture%20of%20monkey%20striate%20cortex.%20J%20Physiol.%201968%3B%20195%2C%20215%E2%80%93243.%204966457" data-title="Receptive%20fields%20and%20functional%20architecture%20of%20monkey%20striate%20cortex" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/4966457" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Receptive+fields+and+functional+architecture+of+monkey+striate+cortex+Hubel+1968" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li><li id="ref46"><span class="order">46.
            </span><a name="pone.0124190.ref046" id="pone.0124190.ref046" class="link-target"></a>Roelfsema RPR, Houtkamp R. Incremental grouping of image elements in vision. Attention Perception Psychophys. 2011; 73(8), 2542–2572.  pmid:21901573 <ul class="reflinks"><li><a href="#" data-author="Roelfsema" data-cit="RoelfsemaRPR%2C%20HoutkampR.%20Incremental%20grouping%20of%20image%20elements%20in%20vision.%20Attention%20Perception%20Psychophys.%202011%3B%2073%288%29%2C%202542%E2%80%932572.%20doi%3A%2010.3758%2Fs13414-011-0200-0%2021901573" data-title="Incremental%20grouping%20of%20image%20elements%20in%20vision" target="_new" title="Go to article in CrossRef">
                      View Article
                    </a></li><li><a href="http://www.ncbi.nlm.nih.gov/pubmed/21901573" target="_new" title="Go to article in PubMed">
                        PubMed/NCBI
                      </a></li><li><a href="http://scholar.google.com/scholar?q=Incremental+grouping+of+image+elements+in+vision+Roelfsema+2011" target="_new" title="Go to article in Google Scholar">
                      Google Scholar
                    </a></li></ul></li></ol></div>

