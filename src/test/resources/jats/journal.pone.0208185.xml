<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0208185</article-id>
<article-id pub-id-type="publisher-id">PONE-D-18-11276</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Social networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Social networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Communications</subject><subj-group><subject>Social communication</subject><subj-group><subject>Social media</subject><subj-group><subject>Facebook</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Social networks</subject><subj-group><subject>Social media</subject><subj-group><subject>Facebook</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Social networks</subject><subj-group><subject>Social media</subject><subj-group><subject>Facebook</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Metabolic networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Mathematical models</subject><subj-group><subject>Random walk</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A unified framework for link prediction based on non-negative matrix factorization with coupling multivariate information</article-title>
<alt-title alt-title-type="running-head">Link prediction based on NMF with multivariate attributes</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Wang</surname>
<given-names>Wenjun</given-names>
</name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2236-959X</contrib-id>
<name name-style="western">
<surname>Tang</surname>
<given-names>Minghu</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Jiao</surname>
<given-names>Pengfei</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>School of Computer Science and Technology, Qinghai Nationalities University, Qinghai, China</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Olier</surname>
<given-names>Ivan</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Liverpool John Moores University, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">mhtang@tju.edu.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>29</day>
<month>11</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<year>2018</year>
</pub-date>
<volume>13</volume>
<issue>11</issue>
<elocation-id>e0208185</elocation-id>
<history>
<date date-type="received">
<day>13</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>13</day>
<month>11</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Wang et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0208185"/>
<abstract>
<p>Many link prediction methods have been developed to infer unobserved links or predict missing links based on the observed network structure that is always incomplete and subject to interfering noise. Thus, the performance of existing methods is usually limited in that their computation depends only on input graph structures, and they do not consider external information. The effects of social influence and homophily suggest that both network structure and node attribute information should help to resolve the task of link prediction. This work proposes SASNMF, a link prediction unified framework based on non-negative matrix factorization that considers not only graph structure but also the internal and external auxiliary information, which refers to both the node attributes and the structural latent feature information extracted from the network. Furthermore, three different combinations of internal and external information are proposed and input into the framework to solve the link prediction problem. Extensive experimental results on thirteen real networks, five node attribute networks and eight non-attribute networks show that the proposed framework has competitive performance compared with benchmark methods and state-of-the-art methods, indicating the superiority of the presented algorithm.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>this work was supported by the applied basic research project of QingHai Province</institution>
</funding-source>
<award-id>2018-ZJ-707</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2236-959X</contrib-id>
<name name-style="western">
<surname>Tang</surname>
<given-names>Minghu</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by the Major Project of National Social Science Fundation of China (14ZDB153),the major research plan of the National Natural Science Foundation of China (91746205,91746107,91224009,51438009), the research project of applied basic of Qinghai Province(2018-ZJ-707). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="6"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Data Availability Statement: All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>As a very important research direction in complex networks, link prediction is attracting a large number of researchers from different disciplines, including computer science, biology, physics and sociology, because of its wide application. It aims to infer the likelihood of the existence of a link between two nodes unconnected by means of the known structure information in the network [<xref ref-type="bibr" rid="pone.0208185.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0208185.ref003">3</xref>]. Link prediction can be used to explore the evolution mechanism of the network [<xref ref-type="bibr" rid="pone.0208185.ref004">4</xref>,<xref ref-type="bibr" rid="pone.0208185.ref005">5</xref>], recommend trusted partners in business trade [<xref ref-type="bibr" rid="pone.0208185.ref006">6</xref>], recommend travel hotspots [<xref ref-type="bibr" rid="pone.0208185.ref007">7</xref>,<xref ref-type="bibr" rid="pone.0208185.ref008">8</xref>], mine suspects in counterterrorism networks [<xref ref-type="bibr" rid="pone.0208185.ref009">9</xref>–<xref ref-type="bibr" rid="pone.0208185.ref011">11</xref>], analyse criminal networks [<xref ref-type="bibr" rid="pone.0208185.ref012">12</xref>,<xref ref-type="bibr" rid="pone.0208185.ref013">13</xref>] and so on.</p>
<p>In recent years, with the development of complex network research, people have proposed many ways to predict the links for specific networks in different fields from various perspectives [<xref ref-type="bibr" rid="pone.0208185.ref014">14</xref>–<xref ref-type="bibr" rid="pone.0208185.ref016">16</xref>]. In simple terms, the existing methods for link prediction can be divided into three categories: unsupervised, supervised and other mixed methods. i) The first computes similarity scores between two nodes based on the known topological structure of the network. It is one of the most widely used methods in recent years and methods such as Common neighbour(CN), Adamic-Adar index(AA), and Resource Allocation index(RA), became the baseline for judging new methods [<xref ref-type="bibr" rid="pone.0208185.ref001">1</xref>]. This kind of method only depends on the information of known topology structure in network. Therefore, its prediction results are easily affected by network data sparsity (The number of edges known to be present is often significantly less than the number of edges known to be absent.). In fact, this is still the biggest challenge in the current research of link prediction. ii) The supervised approaches, on the other hand, attempt to be directly predictive of link behaviour. They generally need to find the characteristics of the node interaction and learn latent features from the topological structure of network [<xref ref-type="bibr" rid="pone.0208185.ref017">17</xref>–<xref ref-type="bibr" rid="pone.0208185.ref019">19</xref>]. Our work is to use this method to achieve multiple attribute fusion techniques to improve prediction performance. iii) The mixed methods include many methods, such as those mainly based on the probability model, perturbation-based frameworks, and matrix completion, etc. The probability model is inherently high cost in computational complexity since its application is limited [<xref ref-type="bibr" rid="pone.0208185.ref020">20</xref>,<xref ref-type="bibr" rid="pone.0208185.ref021">21</xref>]. In addition, structural perturbation-based and matrix completion methods are the most recently proposed the state-of-the-art approaches. Lü LY et al. [<xref ref-type="bibr" rid="pone.0208185.ref022">22</xref>] assumed that the regularity of a network is reflected in the consistency of structural features before and after a random removal of a small set of links. Based on the perturbation of the adjacency matrix, they proposed a universal structural consistency index that is free of prior knowledge of the network organisation. Furthermore, Xu XY [<xref ref-type="bibr" rid="pone.0208185.ref023">23</xref>] and Wang WJ et al. [<xref ref-type="bibr" rid="pone.0208185.ref024">24</xref>] proposed a perturbation framework based on matrix decomposition for link prediction. On the other hand, Pech Ratha et al. [<xref ref-type="bibr" rid="pone.0208185.ref025">25</xref>] proposed a method for link prediction based on matrix completion.</p>
<p>Although these methods can achieve prediction tasks, there is still a shortcomings of insufficient useful information to some extent. Moreover, they are always challenged by high computational costs and data sparsity and network noise. In addition, with the increase of data scale, how the proposed method can be scalable, transplantable and robust in large-scale networks becomes the evaluation basis of the algorithm. Therefore, how to mine the network features, solve the above challenges and improve the performance of link prediction become the main concerns in this paper.</p>
<p>In fact, a complex network is an abstraction of real world, where the nodes represent entities that have very rich attribute information in the real environment. For example, individuals in online social networks have sociological characteristics such as gender, age, religious belief, educational background, and hobbies. The principle of social influence and homophily show that users with similar attributes, or in some cases antithetical attributes, are likely to link to one another [<xref ref-type="bibr" rid="pone.0208185.ref026">26</xref>–<xref ref-type="bibr" rid="pone.0208185.ref028">28</xref>], motivating the use of attribute information for link prediction. Additionally, some previous studies have also empirically demonstrated that non-topological information such as node attributes has a certain impact on the formation and evolution of social networks [<xref ref-type="bibr" rid="pone.0208185.ref029">29</xref>–<xref ref-type="bibr" rid="pone.0208185.ref032">32</xref>]. Therefore, network structure and node attribute information can be considered when predicting links.</p>
<p>In recent years, with the development of other fields related to complex networks, some methods of link prediction have been proposed based on the attribute information of nodes [<xref ref-type="bibr" rid="pone.0208185.ref033">33</xref>,<xref ref-type="bibr" rid="pone.0208185.ref034">34</xref>]. These methods, such as relational learning[<xref ref-type="bibr" rid="pone.0208185.ref035">35</xref>–<xref ref-type="bibr" rid="pone.0208185.ref037">37</xref>], semantic mining[<xref ref-type="bibr" rid="pone.0208185.ref016">16</xref>,<xref ref-type="bibr" rid="pone.0208185.ref033">33</xref>,<xref ref-type="bibr" rid="pone.0208185.ref038">38</xref>]. random walk[<xref ref-type="bibr" rid="pone.0208185.ref039">39</xref>,<xref ref-type="bibr" rid="pone.0208185.ref040">40</xref>], matrix factorization[<xref ref-type="bibr" rid="pone.0208185.ref041">41</xref>], have been proposed to leverage attribute information for link prediction. However, due to the diversity and heterogeneity of information and the difference of fusion methods, the overall effect of these algorithms is insufficient. Therefore, the algorithmic question of how to simultaneously incorporate these two sources of information remains largely unanswered. More recently, Gong N Z et al.[<xref ref-type="bibr" rid="pone.0208185.ref039">39</xref>] proposed an approach based on random walk algorithm to predict links as well as to infer node attributes, it suffers from scalability issues. Backstrom and Leskovec [<xref ref-type="bibr" rid="pone.0208185.ref042">42</xref>] presented a supervised random walk algorithm for link prediction, but this approach only incorporates node information for neighboring nodes. Taking these influence into account, we would like to consider: Can this external information about the nodes contribute to infer an interaction relationship between the nodes? What is the role of this external auxiliary information in predicting the interaction of nodes? How much dependency exists between external information and internal interaction? What methods of fusion are the most effective?</p>
<p>Because non-negative matrix factorization (NMF) [<xref ref-type="bibr" rid="pone.0208185.ref043">43</xref>, <xref ref-type="bibr" rid="pone.0208185.ref044">44</xref>] has the advantages of non-negative, extensibility and interpretability of physical phenomena, it has been widely used in the study of complex networks [<xref ref-type="bibr" rid="pone.0208185.ref045">45</xref>–<xref ref-type="bibr" rid="pone.0208185.ref047">47</xref>]. For example, Yang et al. [<xref ref-type="bibr" rid="pone.0208185.ref048">48</xref>] designed a probabilistic latent variable model which combined the NMF and block structure of matrices for link prediction, but they did not use the node attribute information. Chen BL et al. [<xref ref-type="bibr" rid="pone.0208185.ref041">41</xref>] proposed a non-negative matrix factorization for link prediction that combines network structure and node-attribute information, but this approach does not fully explore the combination form of structure and attribute information in depth, and the complexity is high. As previous studies have shown that node sociological information can assist prediction, and NMF based on matrix decomposition not only has non-negative and interpretable advantages, but also can easily integrate heterogeneous information, make multiple information work together. Inspired by the advantages of non-negative matrix factorization, in this work, we use it to fuse heterogeneous multi-source information for link prediction problem.</p>
<p>In this paper, we propose a unified framework, SASNMF, for link prediction of coupled multivariate information based on NMF. The framework combines local information of a node attribute with global information of the topological structure to solve the link prediction problem from a new perspective of the macro/micro-level. Furthermore, the effects of different combinations of multivariate information on the prediction results are verified under the same framework. Experimental results on 13 real-world network datasets display that the proposed framework has competitive performance compared with baseline and several state-of-the-art algorithms, indicating the superiority of our algorithm. Specifically, this paper makes the following contributions.</p>
<p>First, we develop a prediction framework based on NMF, and auxiliary information from two different levels of macroscopic and microscopic information is coupled to realize the purpose of node relationship prediction.</p>
<p>Second, two kinds of auxiliary information are mined and used to alleviate the problem that the structural information cannot be fully utilized due to data sparsity and reduce the effect of the noise in the forecast.</p>
<p>Third, several different combination modes of auxiliary information are proposed, and the performance is compared and analysed separately under the same framework for the datasets with and without attributes.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec003">
<title>Preliminaries</title>
<p>In this section, we first describe the problem of link prediction. In addition, we review the conventional NMF method.</p>
<sec id="sec004">
<title>Problem description</title>
<p>For a social network can be represented as an undirected graph G = (V,E), where V = {<italic>v</italic><sub>1</sub>,<italic>v</italic><sub>2</sub>,⋯<italic>v</italic><sub><italic>n</italic></sub>} is the set of users (nodes) and E ⊆ V × V is the set of existing relations (edges) between users. The interaction relation between nodes is formally marked as an adjacency matrix <italic>A</italic><sub><italic>n</italic>×<italic>n</italic></sub> in network with n vertices. The element of the i<sup>th</sup> row and the j<sup>th</sup> column in the matrix correspond to the link between node i and j in the network, where <italic>A</italic><sub><italic>ij</italic></sub> = 1 if there is a link from i to j and <italic>A</italic><sub><italic>ij</italic></sub> = 0 otherwise. Generally, the adjacency matrix A represents the macro-relations of the network topology. The problem of link prediction is inferring the probability of an existent link between nodes x and y based on known information in the network, and the probability is expressed as score <italic>P</italic><sub><italic>xy</italic></sub>. The score can be viewed as the similarity of nodes x and y. The higher <italic>P</italic><sub><italic>xy</italic></sub> is, the more similar x is to y. According to the score, all nonexistent links in the network can be sorted in descending order. The links at the top are the most likely to exist. In this paper, we compute the score <italic>P</italic><sub><italic>xy</italic></sub> based on NMF.</p>
<p>To test the algorithm’s accuracy, the observed links, E, are randomly divided into two parts: the training set, E<sup>train</sup> is treated as known information, while the probe set, E<sup>test</sup> has no known information and is used for testing in the prediction experiment. The proportion of links in these two parts ranges from 90% to 20%. Thus, when the training set consists of 90% of links, the remaining 10% of links constitute the test set. Furthermore, in the experiment, we conducted the simulations of SASNMF 100 times for each network and only report the average values in this paper.</p>
</sec>
<sec id="sec005">
<title>NMF review</title>
<p>Given a matrix <inline-formula id="pone.0208185.e001"><alternatives><graphic id="pone.0208185.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mi mathvariant="normal">V</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, the NMF aims to find two nonnegative factor matrices <inline-formula id="pone.0208185.e002"><alternatives><graphic id="pone.0208185.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mi mathvariant="normal">W</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0208185.e003"><alternatives><graphic id="pone.0208185.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi mathvariant="normal">H</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> that make V ≈ V′ = WH. In general, the k, (m + n)k ≪ mn, is the number of latent features or the inner rank of V. The matrix W is called the basis matrix, and H is the coefficient matrix. The column vector of the original matrix V is the weighted sum of all column vectors of matrix W, while the weighted coefficient is just the elements of the corresponding column vector of matrix H.</p>
<p>The optimization problem of NMF is a convex optimization problem[<xref ref-type="bibr" rid="pone.0208185.ref049">49</xref>]. Due to its NP-hardness and lack of appropriate convex formulations, the nonconvex formulations with relatively easy solvability are generally adopted, and only local minima are achievable in a reasonable computational time. Hence, the classic and also more practical approach is to perform alternating minimization of a suitable cost function as the similarity measures between V and the product WH[<xref ref-type="bibr" rid="pone.0208185.ref044">44</xref>].In this paper, our goal is to find V′ as an approximation of V to implement the task of link prediction. Then, the problem of link prediction in networks can be cast as the following NMF problem:
<disp-formula id="pone.0208185.e004">
<alternatives>
<graphic id="pone.0208185.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">H</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.25em"/></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="script">l</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <inline-formula id="pone.0208185.e005"><alternatives><graphic id="pone.0208185.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mi mathvariant="script">l</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>,</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula> is a general loss function. Generally speaking, the form of Euclidean distances are commonly used as this function. Assuming that there are two matrices X and Y, according to the definition of Euclidean distance, this loss function can be written as following form:
<disp-formula id="pone.0208185.e006">
<alternatives>
<graphic id="pone.0208185.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mi mathvariant="script">l</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mo>−</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
In this work, we will also make use of such Euclidean loss. Then, our problem of link prediction is to solve the following optimization problem:
<disp-formula id="pone.0208185.e007">
<alternatives>
<graphic id="pone.0208185.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mspace width="0.50em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">W</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">H</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
where ‖∙‖<sub><italic>F</italic></sub> indicates the Frobenius norm, constrain W ≥ 0,H ≥ 0 requires that all the elements in matrices W and H are non-negative. The Frobenius norm of the matrix X is denoted by <inline-formula id="pone.0208185.e008"><alternatives><graphic id="pone.0208185.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msub><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt><mml:mo>=</mml:mo><mml:msqrt><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:msqrt></mml:math></alternatives></inline-formula>.</p>
<p>Although there have been some notable results on NMF, they are far to be perfect with lots of open questions remained to be solved. More details can be found in Ref. 44.</p>
</sec>
</sec>
</sec>
<sec id="sec006" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec007">
<title>Prediction framework: SASNMF</title>
<p>Because of the influence of the data sparsity, and that the observed links are only a small proportion of all possible links, the methods that rely solely on network structural information have the problem of low prediction accuracy. According to the introduction above, the influence of data sparsity can be alleviated, and the link prediction accuracy can be improved by using the auxiliary information of the network. Therefore, in this paper, we attempt to fully integrate the auxiliary information to make up for the incomplete topology information so that the prediction performance is improved. According to the NMF algorithm, we use the adjacent matrix <italic>A</italic><sub><italic>n</italic>×<italic>n</italic></sub>, which represents the macroscopic information of the network topology structure, and the auxiliary attribute similarity matrix <italic>S</italic><sub><italic>n</italic>×<italic>n</italic></sub>, which represents the microcosmic information, to create the NMF framework. Here, we need to find two nonnegative factors matrices W and H to satisfy the form of V ≈ WH. Thus, the matrix A is decomposed into <inline-formula id="pone.0208185.e009"><alternatives><graphic id="pone.0208185.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mi mathvariant="normal">A</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, where k ≪ n. In the same way, the similarity matrix S is decomposed into <inline-formula id="pone.0208185.e010"><alternatives><graphic id="pone.0208185.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mi mathvariant="normal">S</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, where m ≪ n. Then, we map these two pieces of information into two low-rank approximation spaces, in which <italic>W</italic><sub>1</sub> and <italic>W</italic><sub>2</sub> represent the bases in their latent spaces. According to formula (<xref ref-type="disp-formula" rid="pone.0208185.e007">3</xref>), we have
<disp-formula id="pone.0208185.e011">
<alternatives>
<graphic id="pone.0208185.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mspace width="1.5em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
<disp-formula id="pone.0208185.e012">
<alternatives>
<graphic id="pone.0208185.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mspace width="1.50em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn>
</mml:math>
</alternatives>
<label>(5)</label>
</disp-formula></p>
<p>However, our goal is to develop an indicator that can couple multivariate information to help improve the accuracy of link prediction. Therefore, formula (<xref ref-type="disp-formula" rid="pone.0208185.e011">4</xref>) and (<xref ref-type="disp-formula" rid="pone.0208185.e012">5</xref>) are combined into the following new form
<disp-formula id="pone.0208185.e013">
<alternatives>
<graphic id="pone.0208185.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:mi mathvariant="normal">Q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(6)</label>
</disp-formula></p>
<p>The information shown in the above formula (<xref ref-type="disp-formula" rid="pone.0208185.e013">6</xref>) are only a simple combination of both the topological structure and auxiliary attribute, and they are not fully integrated into the same feature space. Therefore, we need to find a common factor matrix W to combine this information and then to make it a guider within the processing of the link prediction problem. That is, we develop a framework for link prediction that can employ a low-rank latent feature space representation to realize network structure prediction and add the lack of information within the network. Furthermore, let W = <italic>W</italic><sub>1</sub> = <italic>W</italic><sub>2</sub> to indicate that the two pieces of information in the network are mapped to the same feature space. At the same time, to avoid overfitting and to leverage the effects extent between the topology information and auxiliary attribute information in the link prediction results, we need to constrain and mediate the framework through setting up parameters. Finally, the objective function is created as follows:
<disp-formula id="pone.0208185.e014">
<alternatives>
<graphic id="pone.0208185.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mi mathvariant="normal">Q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(7)</label>
</disp-formula>
<disp-formula id="pone.0208185.e015">
<alternatives>
<graphic id="pone.0208185.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:mi mathvariant="normal">s</mml:mi><mml:mo>.</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">W</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn>
</mml:math>
</alternatives>
</disp-formula>
where <inline-formula id="pone.0208185.e016"><alternatives><graphic id="pone.0208185.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mi>W</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, α is an equilibrium parameter for mediating the effect of the structure and attribute, and β is a regularization parameter to avoid overfitting.</p>
<p>Although it is difficult to obtain the global optimal solution of Q, the local can be implemented by a multiplicative iteration method.</p>
<p>To (7) decompose, by introducing the Lagrangian multiplier ψ,φ,ϕ for the nonnegativity of W, <italic>H</italic><sub>1</sub> and <italic>H</italic><sub>2</sub>; we obtain the loss function without constraints:
<disp-formula id="pone.0208185.e017">
<alternatives>
<graphic id="pone.0208185.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e017" xlink:type="simple"/>
<mml:math display="block" id="M17">
<mml:mi mathvariant="normal">L</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">φ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(8)</label>
</disp-formula></p>
<p>Then, taking partial derivatives of L with respect to W, <italic>H</italic><sub>1</sub> and <italic>H</italic><sub>2</sub>, we have
<disp-formula id="pone.0208185.e018">
<alternatives>
<graphic id="pone.0208185.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">ψ</mml:mi>
</mml:math>
</alternatives>
<label>(9)</label>
</disp-formula>
<disp-formula id="pone.0208185.e019">
<alternatives>
<graphic id="pone.0208185.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e019" xlink:type="simple"/>
<mml:math display="block" id="M19">
<mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">A</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">φ</mml:mi>
</mml:math>
</alternatives>
<label>(10)</label>
</disp-formula>
<disp-formula id="pone.0208185.e020">
<alternatives>
<graphic id="pone.0208185.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e020" xlink:type="simple"/>
<mml:math display="block" id="M20">
<mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">α</mml:mi><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">S</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">α</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">ϕ</mml:mi><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
<label>(11)</label>
</disp-formula></p>
<p>In terms of the Karush-Kuhn-Tucker (KKT) complementary slackness condition ψW = 0, φH<sub>1</sub> = 0 and ϕH<sub>2</sub> = 0, and Let <inline-formula id="pone.0208185.e021"><alternatives><graphic id="pone.0208185.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0208185.e022"><alternatives><graphic id="pone.0208185.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0208185.e023"><alternatives><graphic id="pone.0208185.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>, we can derive the following updating rules with respect to W, <italic>H</italic><sub>1</sub> and <italic>H</italic><sub>2</sub>:
<disp-formula id="pone.0208185.e024">
<alternatives>
<graphic id="pone.0208185.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e024" xlink:type="simple"/>
<mml:math display="block" id="M24">
<mml:mi>W</mml:mi><mml:mo>⟵</mml:mo><mml:mi>W</mml:mi><mml:mi>.</mml:mi><mml:mo>*</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>S</mml:mi><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo><mml:mi>.</mml:mi><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(12)</label>
</disp-formula>
<disp-formula id="pone.0208185.e025">
<alternatives>
<graphic id="pone.0208185.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e025" xlink:type="simple"/>
<mml:math display="block" id="M25">
<mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⟵</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>.</mml:mi><mml:mo>*</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:mi>.</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(13)</label>
</disp-formula>
<disp-formula id="pone.0208185.e026">
<alternatives>
<graphic id="pone.0208185.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e026" xlink:type="simple"/>
<mml:math display="block" id="M26">
<mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>⟵</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mi>.</mml:mi><mml:mo>*</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>S</mml:mi><mml:mo>)</mml:mo><mml:mi>.</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>W</mml:mi><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="normal">β</mml:mi><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(14)</label>
</disp-formula>
where .* and ./ represent the elementwise multiplication and division, respectively. The score between nodes can be obtained by W and H<sub>1</sub>. Then, we can predict the edges.</p>
<p>To sum up, pseudo code of the proposed Link prediction algorithm based on NMF with coupling multivariate information is described as follows:</p>
<boxed-text id="pone.0208185.box001" position="float">
<sec id="sec008">
<title>Algorithm Name: SASNMF</title>
<p specific-use="line"><bold>Input</bold>: A: the adjacency matrix of the given network, S: the</p>
<p specific-use="line">auxiliary information matrix, k: number of features, α and β: parameters.</p>
<p specific-use="line"><bold>Output</bold>: the approximate matrix of the network A</p>
<p specific-use="line">1: divide A into <italic>A</italic><sup><italic>train</italic></sup>,<italic>A</italic><sup><italic>test</italic></sup></p>
<p specific-use="line">2: get the number of latent features k by Colibri</p>
<p specific-use="line">3: Initialize W, H<sub>1</sub> and H<sub>2</sub>.</p>
<p specific-use="line">4: do while</p>
<p specific-use="line">5: update W, H<sub>1</sub> and H<sub>2</sub> by means of formulas (<xref ref-type="disp-formula" rid="pone.0208185.e024">12</xref>),(<xref ref-type="disp-formula" rid="pone.0208185.e025">13</xref>) and (<xref ref-type="disp-formula" rid="pone.0208185.e026">14</xref>).</p>
<p specific-use="line">6: get W and H<sub>1</sub> after until object function convergence</p>
<p specific-use="line">7: end while</p>
<p specific-use="line">8: output W × <italic>H</italic><sub>1</sub></p>
</sec>
</boxed-text>
</sec>
<sec id="sec009">
<title>Computational complexity analysis</title>
<p>The computational complexity of SASNMF algorithm mainly comes from two parts. One is to extract auxiliary information, including external auxiliary information from node sociological attributes and internal auxiliary information extracted from topology structure. The second is iterative update matrices W, H<sub>1</sub> and H<sub>2</sub> at the same time.</p>
<p>Given an attributed network with n nodes, m attributes, then the matrix of attributes similarity, <italic>S</italic><sub><italic>n</italic>×<italic>n</italic></sub>, is obtained by using cosine similarity algorithm based on node’s attribute vectors. So the time complexity is O(<italic>n</italic><sup>2</sup>). Similarly, the time complexity of the internal auxiliary information extracted based on topology structure is also O(<italic>n</italic><sup>2</sup>).</p>
<p>When updating W, H<sub>1</sub> and H<sub>2</sub>, to reduce the time overhead, we utilizes the objective relative error as the stopping criterion and set to less than 10<sup>−6</sup> in experiment. In addition, the decomposed dimension is a k-dimensional vector, their time complexities are O(<italic>n</italic><sup>2</sup>k) time. So the total time cost of the algorithm is O(<italic>n</italic><sup>2</sup> + <italic>n</italic><sup>2</sup> + <italic>n</italic><sup>2</sup>k). Since k can be treated as constants, complexity of the step is O(<italic>n</italic><sup>2</sup>). To sum up, the computational cost of our approach is nearly to O(<italic>n</italic><sup>2</sup>).</p>
<p>Of course, we can also improve our algorithm according to the relevant literature to achieve parallel computing[<xref ref-type="bibr" rid="pone.0208185.ref050">50</xref>], so as to obtain performance optimization. This is what we want to do in the future.</p>
</sec>
<sec id="sec010">
<title>Auxiliary information preprocessing</title>
<p>Here, we propose that the auxiliary information can be derived not only from external data but also from internal network structure information. SASNMF allows us to directly model such information into the framework to enhance the prediction performance. To distinguish sources of multivariate auxiliary information, we call those extracted from the network structure as <bold><italic>internal</italic></bold> auxiliary information and attributes of nodes as <bold><italic>external</italic></bold> auxiliary information.</p>
<p>It is an essential of our work that this external auxiliary information, node properties, is preprocessed. Considering the privacy of users, these information has been treated anonymously. When pretreated these attribute values, such as age, using directly actual measure values. Others, such as religious belief, are assigned a determined value in term of an appointed numerical range required. In addition, the numerical 0 or 1 is employed also to express two kinds of different status value. For these information, we use the vector Z<sub><italic>m</italic></sub> to denote that the node has m attributes. All of the node’s attribute information in network G is represented as matrix Z<sub><italic>n</italic>×<italic>m</italic></sub>. The matrix element Z<sub><italic>ij</italic></sub> represents the j<sup><italic>th</italic></sup> attribute value of the <italic>i</italic><sup><italic>th</italic></sup> node. However, owing to the heterogeneity of node attribute, it is impossible that exert the better indicative effect of attributes on the prediction results through using a linear combination. Therefore, all of the attributes are normalized by the column of attribute matrix, that is, formula <inline-formula id="pone.0208185.e027"><alternatives><graphic id="pone.0208185.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>. Although it has been processed, the effectiveness of this attribute matrix in prediction is still very poor. Therefore, it is necessary to calculate the similarity between the attribute vectors Z<sub><italic>m</italic></sub> of each node and to form the attribute similarity matrix before it can be applied to the prediction framework. To compute the similarity between attributes, the Euclidean distance, cosine similarity or Pearson method can be used to calculate. Here, the three common similarity measures were tested and analyzed respectively. Finally, we use the measure of similarity based on cosine, <inline-formula id="pone.0208185.e028"><alternatives><graphic id="pone.0208185.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>.</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, to realize the evaluation of attribute similarity.</p>
<p>This internal auxiliary information is actually the latent feature of node, which the local structure information for the nodes themselves need be extracted from the input network by unsupervised structure similarity methods. In this work, for analysing the influence of node latent feature on the prediction performance, we employ seven similarity indices to compute the score, Sim, of the structure similarity between any two nodes as the internal auxiliary information. Furthermore, the prediction performance are analysed by comparing the node attribute with the structure information.</p>
</sec>
<sec id="sec011">
<title>Multivariate information combination mode</title>
<p>To test the effectiveness and analyse the influence to predict under different coupling modes of auxiliary information, we propose the following combination methods.</p>
<list list-type="roman-lower">
<list-item><p>A+S mode: the adjacent matrix A and external auxiliary information S are combined to input into the proposed framework. This method is directly marked as SASNMF.</p></list-item>
<list-item><p>A+Sim mode: the adjacent matrix A and internal auxiliary information Sim are combined to input into the proposed framework. The Sim is regarded as matrix S in the proposed framework. Thus, this method is marked as *+SASNMF, where * represented any similarity methods.</p></list-item>
<list-item><p>Sim+S mode: the adjacent matrix A is replaced as the internal auxiliary information Sim. This method is marked as A (= *)+SASNMF, where * represented any similarity methods.</p></list-item>
</list>
<p>For two types of network datasets: the second combination method, ii), is only used for the network without node attributes, while all of the methods are used for a network with real-world node attributes. Our experiments show that both types of auxiliary information can increase the performance of link prediction.</p>
</sec>
</sec>
<sec id="sec012" sec-type="results">
<title>Results</title>
<sec id="sec013">
<title>Datasets description</title>
<p>We consider the following 13 real-world networks drawn from disparate fields. Among them, one contains external attributes, and we generate internal attributes for all of them.</p>
<p>The five networks with external attribute information: i) Lazega-lawyers [<xref ref-type="bibr" rid="pone.0208185.ref051">51</xref>]: The network is a social network between 71 partners and associates in some New England law firms. In addition, each entity in the network is described by features such as gender, office-location, age, and years employed. We did some preprocessing of the features (binarized the features such as the age and years employed) and then constructed a kernel matrix of pairwise similarities. In this article, we choose seven attributes to calculate. ii) Facebook [<xref ref-type="bibr" rid="pone.0208185.ref052">52</xref>]: The network is extracted from the Facebook online social network. A user can provide profile information (e.g., age, gender, education and information). By selecting some informative attributes in this profile information, we create a feature vector for each user. iii) WebKB [<xref ref-type="bibr" rid="pone.0208185.ref053">53</xref>]: The network consists of 4 subnetworks (Cornell, Texas, Washington and Wisconsin) gathered from 4 universities. The node represents a webpage that is annotated by 1703-dimensional binary valued word attributes. The first three of them are used for our experiments.</p>
<p>The eight networks without external attributes information: i) Karate [<xref ref-type="bibr" rid="pone.0208185.ref054">54</xref>]—social network of friendships between 34 members of a karate club at a US university in the 1970s; ii) Jazz [<xref ref-type="bibr" rid="pone.0208185.ref055">55</xref>]—jazz musician network, the link denotes the relationship between two persons if they played together in the same band; iii) USAir [<xref ref-type="bibr" rid="pone.0208185.ref056">56</xref>]—the air transportation network of US Airlines; iv) Political blogs (PolitB) [<xref ref-type="bibr" rid="pone.0208185.ref057">57</xref>]—the network of hyperlinks between weblogs on US politics; v) C. <italic>elegans</italic> [<xref ref-type="bibr" rid="pone.0208185.ref058">58</xref>]—the neural network of <italic>C</italic>. <italic>elegans</italic> worms; vi) Adjnoun [<xref ref-type="bibr" rid="pone.0208185.ref059">59</xref>]—The adjnoun network is the network of common adjectives and noun adjacencies for the novel “David Copperfield” by Charles Dickens; vii) Netsci [<xref ref-type="bibr" rid="pone.0208185.ref059">59</xref>]—Netsci is a collaboration network of researchers who publish papers on network science; and viii) Metabolic [<xref ref-type="bibr" rid="pone.0208185.ref058">58</xref>]—the metabolic network of the nematode worm <italic>C</italic>. <italic>elegans</italic>. These networks are often used as benchmark networks to test the predictive performance of new methods.</p>
<p>The basic topology features of these networks are summarized in <xref ref-type="table" rid="pone.0208185.t001">Table 1</xref>. The symbol N and E are the total number of nodes and links, respectively. &lt;K&gt; is the average degree. &lt;d&gt; is the mean shortest distance. C is the clustering coefficient, and #attributes is the number of node attributes.</p>
<table-wrap id="pone.0208185.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.t001</object-id>
<label>Table 1</label> <caption><title>The basic topology features of real networks.</title></caption>
<alternatives>
<graphic id="pone.0208185.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Network</th>
<th align="center">N</th>
<th align="center">E</th>
<th align="center">&lt;K&gt;</th>
<th align="center">&lt;d&gt;</th>
<th align="center">C</th>
<th align="center">#attributes</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><bold>Lazega-lawyers</bold></td>
<td align="center">71</td>
<td align="center">378</td>
<td align="center">10.8</td>
<td align="center">2.104</td>
<td align="center">0.391</td>
<td align="center">7</td>
</tr>
<tr>
<td align="center"><bold>Facebook</bold></td>
<td align="center">228</td>
<td align="center">3419</td>
<td align="center">29.991</td>
<td align="center">1.868</td>
<td align="center">0.616</td>
<td align="center">56</td>
</tr>
<tr>
<td align="center"><bold>Cornell</bold></td>
<td align="center">195</td>
<td align="center">286</td>
<td align="center">2.903</td>
<td align="center">3.2</td>
<td align="center">0.157</td>
<td align="center">1703</td>
</tr>
<tr>
<td align="center"><bold>Texas</bold></td>
<td align="center">187</td>
<td align="center">298</td>
<td align="center">3.027</td>
<td align="center">3.036</td>
<td align="center">0.196</td>
<td align="center">1703</td>
</tr>
<tr>
<td align="center"><bold>Washington</bold></td>
<td align="center">230</td>
<td align="center">366</td>
<td align="center">3.373</td>
<td align="center">2.995</td>
<td align="center">0.209</td>
<td align="center">1703</td>
</tr>
<tr>
<td align="center"><bold>Krate</bold></td>
<td align="center">34</td>
<td align="center">78</td>
<td align="center">4.588</td>
<td align="center">2.408</td>
<td align="center">0.571</td>
<td align="center">/</td>
</tr>
<tr>
<td align="center"><bold>Jazz</bold></td>
<td align="center">198</td>
<td align="center">2742</td>
<td align="center">27.70</td>
<td align="center">2.235</td>
<td align="center">0.618</td>
<td align="center">/</td>
</tr>
<tr>
<td align="center"><bold>USAir</bold></td>
<td align="center">332</td>
<td align="center">2126</td>
<td align="center">12.81</td>
<td align="center">2.74</td>
<td align="center">0.749</td>
<td align="center">/</td>
</tr>
<tr>
<td align="center"><bold>PolitB</bold></td>
<td align="center">1222</td>
<td align="center">16714</td>
<td align="center">27.36</td>
<td align="center">2.74</td>
<td align="center">0.36</td>
<td align="center">/</td>
</tr>
<tr>
<td align="center"><bold>C. <italic>elegans</italic></bold></td>
<td align="center">297</td>
<td align="center">2148</td>
<td align="center">14.47</td>
<td align="center">2.46</td>
<td align="center">0.308</td>
<td align="center">/</td>
</tr>
<tr>
<td align="center"><bold>Netsci</bold></td>
<td align="center">379</td>
<td align="center">914</td>
<td align="center">4.82</td>
<td align="center">6.04</td>
<td align="center">0.798</td>
<td align="center">/</td>
</tr>
<tr>
<td align="center"><bold>Metabolic</bold></td>
<td align="center">453</td>
<td align="center">2025</td>
<td align="center">8.940</td>
<td align="center">2.664</td>
<td align="center">0.647</td>
<td align="center">/</td>
</tr>
<tr>
<td align="center"><bold>Adjnoun</bold></td>
<td align="center">112</td>
<td align="center">425</td>
<td align="center">7.589</td>
<td align="center">2.536</td>
<td align="center">0.173</td>
<td align="center">/</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec014">
<title>Evaluation metrics</title>
<p>Like many existing prediction studies [<xref ref-type="bibr" rid="pone.0208185.ref001">1</xref>], in our work adopts also the most frequently-used metrics AUC (area under the ROC curve) to measure the performance of link prediction [<xref ref-type="bibr" rid="pone.0208185.ref060">60</xref>]. This metric is viewed as a robust measure in the presence of data imbalance [<xref ref-type="bibr" rid="pone.0208185.ref019">19</xref>].</p>
<p>The AUC can be interpreted as the probability that a randomly chosen missing link (a link in <italic>E</italic><sup><italic>test</italic></sup>) is given a higher score than a randomly chosen nonexistent link (a link in U\E, where U denotes the universal set). In the implementation, among n independent comparisons, if there are <italic>n</italic>′ occurrences of the missing link having a higher score and <italic>n</italic>″ occurrences of the missing link and nonexistent link having the same score, we define the accuracy as:
<disp-formula id="pone.0208185.e029">
<alternatives>
<graphic id="pone.0208185.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e029" xlink:type="simple"/>
<mml:math display="block" id="M29">
<mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>″</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula></p>
<p>If all the scores are generated from an independent and identical distribution, the accuracy should be approximately 0.5. Therefore, the degree to which the accuracy exceeds 0.5 indicates how much better the algorithm performs than pure chance.</p>
<p>In addition, we have adopted the Precision metric, which is also one of the most popular index of evaluation link prediction [<xref ref-type="bibr" rid="pone.0208185.ref061">61</xref>]. Given the ranking of the non-observed links in decreasing order according to their scores. The precision is defined as the ratio of relevant items selected to the number of items selected. That is to say, if we take the top-L links as the predicted ones, among which <inline-formula id="pone.0208185.e030"><alternatives><graphic id="pone.0208185.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mi mathvariant="script">l</mml:mi></mml:math></alternatives></inline-formula> links are right, then,
<disp-formula id="pone.0208185.e031">
<alternatives>
<graphic id="pone.0208185.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e031" xlink:type="simple"/>
<mml:math display="block" id="M31">
<mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
<label>(16)</label>
</disp-formula>
Clearly, a higher value of precision means a higher prediction accuracy.</p>
<p>Although the computing result is not unique through taking different <italic>L</italic> values for a single algorithm, in order to ensure the fairness for all comparison algorithms, the same value can be taken for <italic>L</italic>. This value does not affect the final comparison. Therefore, in our work, for the convenience of comparison, all the algorithms are unified to take the value of <italic>L</italic> = 100.</p>
</sec>
<sec id="sec015">
<title>Comparison methods</title>
<p>In this section, we mainly evaluate the performance of our algorithm. According to the way in multivariate information coupling mode, our methods are represented as SASNMF and *+SASNMF. More specifically, there are three types of coupling mode for auxiliary information using our framework, namely, i) Global network structure information coupling external auxiliary information from node attributes (A+S). ii) Global network structure information coupling internal auxiliary information from local structure latent feature (A+Sim). iii) Internal auxiliary information from local structure latent feature and external auxiliary information from node attributes are fused (Sim+S).</p>
<p>To analyse performance of algorithm proposed, we adopt two kinds of comparison methods. One is baseline algorithms, such as CN, AA, etc., which are often used for existing methods as benchmark to evaluate these approaches. We used seven here. In this work, they are also used to extract local structural latent features of nodes to act as internal auxiliary information.</p>
<p>The second is several state-of-the-art methods. These are divided into two categories: both structural information and node attribute information are adopted and only structural information is utilized.</p>
</sec>
<sec id="sec016">
<title>Baseline methods</title>
<p>We list four types of link prediction methods as the baseline methods, including five local algorithms based on the number of common neighbours between pairs of nodes (CN,AA,RA,Salton and Jaccard), a global random walk method(ACT) and a local path method(Katz) and NMF method based on matrix factorization with the Frobenius norm. The mathematical expressions of these methods are shown in <xref ref-type="table" rid="pone.0208185.t002">Table 2</xref>. Their detailed definitions can be found in ref. 1–3 and 43.</p>
<table-wrap id="pone.0208185.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.t002</object-id>
<label>Table 2</label> <caption><title>Mathematical expressions of baseline methods.</title></caption>
<alternatives>
<graphic id="pone.0208185.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Methods</th>
<th align="center">Formula</th>
<th align="center">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Common neighbour(CN)</td>
<td align="center"><italic>S</italic><sub><italic>xy</italic></sub> = |Γ(<italic>x</italic>) ∩ Γ(<italic>y</italic>)|</td>
<td align="center" rowspan="5">Where Γ(<italic>x</italic>) denotes the set of neighbours of node x, |*| is the cardinality of the set *, and <italic>k</italic>(<italic>x</italic>) is the degree of node <italic>x</italic>.</td>
</tr>
<tr>
<td align="center">Salton</td>
<td align="center"><inline-formula id="pone.0208185.e032"><alternatives><graphic id="pone.0208185.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∩</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:msqrt></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="center">Jaccard</td>
<td align="center"><inline-formula id="pone.0208185.e033"><alternatives><graphic id="pone.0208185.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∩</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∪</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="center">Resource Allocation Index(RA)</td>
<td align="center"><inline-formula id="pone.0208185.e034"><alternatives><graphic id="pone.0208185.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∩</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi>Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="center">Adamic-Adar index(AA)</td>
<td align="center"><inline-formula id="pone.0208185.e035"><alternatives><graphic id="pone.0208185.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∩</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi>Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></td>
</tr>
<tr>
<td align="center">Average Commute Time (ACT)</td>
<td align="center"><inline-formula id="pone.0208185.e036"><alternatives><graphic id="pone.0208185.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula></td>
<td align="center">Where <inline-formula id="pone.0208185.e037"><alternatives><graphic id="pone.0208185.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0208185.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:msubsup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> represents the elements of matrix <italic>L</italic><sup>+</sup>, the pseudo-inverse of the Laplacian matrix.</td>
</tr>
<tr>
<td align="center">Katz</td>
<td align="center"><italic>S</italic><sub><italic>xy</italic></sub> = ((<italic>I</italic> − <italic>θ</italic> ∙ <italic>A</italic>)<sup>−1</sup> − <italic>I</italic>)<sub><italic>xy</italic></sub></td>
<td align="center">Where <italic>θ</italic> is a parameter, takes the default value 0.1, and <italic>I</italic> is the diagonal matrix.</td>
</tr>
<tr>
<td align="center">NMF</td>
<td align="center">Non-negative matrix factorization</td>
<td align="center">MF-based method</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec017">
<title>State-of-the-art methods</title>
<p>In addition, apart from the baseline methods, we also further compare the performance of the proposed SASNMF method with the other three state-of-art competitive algorithms.</p>
<p>The structure perturbation method (SPM) based on nonnegative matrix factorization [<xref ref-type="bibr" rid="pone.0208185.ref024">24</xref>], which is based on the perturbation of the adjacency matrix, assumes that the regularity of a network is reflected in the consistency of structural features before and after a random removal of a small set of links. In particular it outperforms state-of-the-art link prediction methods both in accuracy and robustness[<xref ref-type="bibr" rid="pone.0208185.ref022">22</xref>,<xref ref-type="bibr" rid="pone.0208185.ref023">23</xref>]. In the SPM method, we use the method of NMF-D1 with random deletion perturbation. And the perturbation ratio is 0.04, the default value of perturbation times is 20.</p>
<p>Matrix completion (MC) [<xref ref-type="bibr" rid="pone.0208185.ref025">25</xref>] is a global information-based prediction algorithm based upon the low-rank and sparse property of the adjacency matrix. It employ the robust principal component analysis method through minimizing the nuclear norm of the matrix which fits the training data to reconstruct a network that is close to the original network and accordingly identify the missing links. In the MC method, in addition to the partial values of the parameter λ provided in the literature, we also perform an optimal analysis of the parameter and finally select the best one. The parameter values of this method are referred to in the <bold><xref ref-type="supplementary-material" rid="pone.0208185.s001">S1 File</xref>.</bold></p>
<p>In addition, Chen BL et al. [<xref ref-type="bibr" rid="pone.0208185.ref041">41</xref>] proposed a link prediction method based on NMF(NMF-LP), which adopted node attributes. Therefore, we compare this method with our framework.</p>
</sec>
<sec id="sec018">
<title>Experiments results</title>
<p>Parameters setting: In order to achieve good prediction results, before the whole experiment, we analyzed the sensitivity of the model parameters α and β. We set the proportion of training set as 0.9, and the range of the two parameters are set from 1 to 100, respectively. And then take the widely used evaluation index AUC and Precision for link predication as evidence. The values of AUC and precision are calculated on 13 networks, and compared with each other. Finally, the optimal range of parameters is gradually obtained. Furthermore, we select five networks including Lazega, Facebook, Cornell, Texas, four networks with node attributes and Kate, one non-attributes from the all networks, and analyze the experimental sensitivity of α and β in the performance of link predication in a smaller range. As represented in Fig<xref ref-type="fig" rid="pone.0208185.g001">1</xref>, it is obvious that the performances on Lazega, Facebook, Cornell, Texas and Kate are gradual stable. Although the different settings of α and β have significant influence on the predict results, we also know that our framework has equally better performance than other baseline methods. Without losing generality, we set α = 4, β = 32 in subsequent experiments.</p>
<fig id="pone.0208185.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Model parameter sensitivity analysis.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.g001" xlink:type="simple"/>
</fig>
<p>Using optimized parameter results, in this section, we show the AUC and precision results of our proposed methods based on NMF with coupling multivariate information and other comparison methods on the 13 real network data in Tables <xref ref-type="table" rid="pone.0208185.t003">3</xref>–<xref ref-type="table" rid="pone.0208185.t006">6</xref>.</p>
<table-wrap id="pone.0208185.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.t003</object-id>
<label>Table 3</label> <caption><title>The average predicting precision obtained by 100 independent runs on 5 networks with external attributes.</title> <p>The training set contains 90% of the total connections.</p></caption>
<alternatives>
<graphic id="pone.0208185.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.t003" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Precision</th>
<th align="center">Lazega</th>
<th align="center">Facebook</th>
<th align="center">Cornell</th>
<th align="center">Texas</th>
<th align="center">Washington</th>
<th align="center">Mean</th>
<th align="center">Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">SASNMF</td>
<td align="center">0.1661<sup>(2)</sup></td>
<td align="center">0.3923<sup>(1)</sup></td>
<td align="center">0.0655<sup>(12)</sup></td>
<td align="center">0.0154<sup>(15)</sup></td>
<td align="center">0.0451<sup>(7)</sup></td>
<td align="center">7.4</td>
<td align="center">A+S</td>
</tr>
<tr>
<td align="center">AA+SASNMF</td>
<td align="center">0.1579<sup>(3)</sup></td>
<td align="center">0.2952<sup>(10)</sup></td>
<td align="center">0.0917<sup>(6)</sup></td>
<td align="center">0.0182<sup>(11)</sup></td>
<td align="center">0.0092<sup>(14)</sup></td>
<td align="center">8.8</td>
<td align="center" rowspan="4">A+Sim</td>
</tr>
<tr>
<td align="center">CN+SASNMF</td>
<td align="center">0.1479<sup>(6)</sup></td>
<td align="center">0.2913<sup>(13)</sup></td>
<td align="center">0.0934<sup>(5)</sup></td>
<td align="center">0.0168<sup>(14)</sup></td>
<td align="center">0.0114<sup>(12)</sup></td>
<td align="center">10</td>
</tr>
<tr>
<td align="center">RA+SASNMF</td>
<td align="center">0.1516<sup>(4)</sup></td>
<td align="center">0.2931<sup>(12)</sup></td>
<td align="center">0.0866<sup>(9)</sup></td>
<td align="center">0.0193<sup>(10)</sup></td>
<td align="center">0.0108<sup>(13)</sup></td>
<td align="center">9.6</td>
</tr>
<tr>
<td align="center">Salton+SASNMF</td>
<td align="center">0.1484<sup>(5)</sup></td>
<td align="center">0.2963<sup>(9)</sup></td>
<td align="center">0.0876<sup>(7)</sup></td>
<td align="center">0.0171<sup>(13)</sup></td>
<td align="center">0.0146<sup>(11)</sup></td>
<td align="center">9</td>
</tr>
<tr>
<td align="center">A (= AA)+SASNMF</td>
<td align="center">0.1316<sup>(12)</sup></td>
<td align="center">0.2836<sup>(16)</sup></td>
<td align="center">0.1069<sup>(3)</sup></td>
<td align="center">0.1071<sup>(1)</sup></td>
<td align="center">0.1135<sup>(1)</sup></td>
<td align="center">6.6</td>
<td align="center" rowspan="4">Sim+S</td>
</tr>
<tr>
<td align="center">A (= CN)+ SASNMF</td>
<td align="center">0.1474<sup>(7)</sup></td>
<td align="center">0.2842<sup>(15)</sup></td>
<td align="center">0.0828<sup>(10)</sup></td>
<td align="center">0.0536<sup>(5)</sup></td>
<td align="center">0.0919<sup>(3)</sup></td>
<td align="center">8</td>
</tr>
<tr>
<td align="center">A (= RA)+ SASNMF</td>
<td align="center">0.1316<sup>(12)</sup></td>
<td align="center">0.2944<sup>(11)</sup></td>
<td align="center">0.1103<sup>(2)</sup></td>
<td align="center">0.0857<sup>(2)</sup></td>
<td align="center">0.1000<sup>(2)</sup></td>
<td align="center">5.8</td>
</tr>
<tr>
<td align="center">A (= Salton)+ SASNMF</td>
<td align="center">0.0842<sup>(18)</sup></td>
<td align="center">0.1646<sup>(19)</sup></td>
<td align="center">0.0000<sup>(18)</sup></td>
<td align="center">0.0000<sup>(18)</sup></td>
<td align="center">0.0000<sup>(15)</sup></td>
<td align="center">17.6</td>
</tr>
<tr>
<td align="center">AA</td>
<td align="center">0.1321<sup>(11)</sup></td>
<td align="center">0.3247<sup>(4)</sup></td>
<td align="center">0.0869<sup>(8)</sup></td>
<td align="center">0.0739<sup>(3)</sup></td>
<td align="center">0.0873<sup>(5)</sup></td>
<td align="center">6.2</td>
<td align="center" rowspan="8">Baseline methods</td>
</tr>
<tr>
<td align="center">CN</td>
<td align="center">0.1371<sup>(10)</sup></td>
<td align="center">0.3136<sup>(7)</sup></td>
<td align="center">0.0741<sup>(11)</sup></td>
<td align="center">0.0432<sup>(6)</sup></td>
<td align="center">0.0892<sup>(4)</sup></td>
<td align="center">7.6</td>
</tr>
<tr>
<td align="center">RA</td>
<td align="center">0.1271<sup>(14)</sup></td>
<td align="center">0.3808<sup>(2)</sup></td>
<td align="center">0.0866<sup>(9)</sup></td>
<td align="center">0.0700<sup>(4)</sup></td>
<td align="center">0.0792<sup>(6)</sup></td>
<td align="center">7</td>
</tr>
<tr>
<td align="center">Salton</td>
<td align="center">0.0953<sup>(16)</sup></td>
<td align="center">0.3002<sup>(8)</sup></td>
<td align="center">0.0000<sup>(18)</sup></td>
<td align="center">0.0004<sup>(17)</sup></td>
<td align="center">0.0000<sup>(15)</sup></td>
<td align="center">14.8</td>
</tr>
<tr>
<td align="center">Jaccard</td>
<td align="center">0.0921<sup>(17)</sup></td>
<td align="center">0.3162<sup>(6)</sup></td>
<td align="center">0.0010<sup>(17)</sup></td>
<td align="center">0.0004<sup>(17)</sup></td>
<td align="center">0.0000<sup>(15)</sup></td>
<td align="center">14.4</td>
</tr>
<tr>
<td align="center">Katz</td>
<td align="center">0.1303<sup>(13)</sup></td>
<td align="center">0.0163<sup>(20)</sup></td>
<td align="center">0.0359<sup>(15)</sup></td>
<td align="center">0.0104<sup>(16)</sup></td>
<td align="center">0.0222<sup>(8)</sup></td>
<td align="center">14.4</td>
</tr>
<tr>
<td align="center">ACT</td>
<td align="center">0.0311<sup>(19)</sup></td>
<td align="center">0.2575<sup>(17)</sup></td>
<td align="center">0.0255<sup>(16)</sup></td>
<td align="center">0.0179<sup>(12)</sup></td>
<td align="center">0.0000<sup>(15)</sup></td>
<td align="center">15.8</td>
</tr>
<tr>
<td align="center">NMF</td>
<td align="center">0.1471<sup>(8)</sup></td>
<td align="center">0.2907<sup>(14)</sup></td>
<td align="center">0.0969<sup>(4)</sup></td>
<td align="center">0.0154<sup>(15)</sup></td>
<td align="center">0.0108<sup>(13)</sup></td>
<td align="center">10.8</td>
</tr>
<tr>
<td align="center">SPM</td>
<td align="center">0.1742<sup>(1)</sup></td>
<td align="center">0.3546<sup>(3)</sup></td>
<td align="center">0.1276<sup>(1)</sup></td>
<td align="center">0.0314<sup>(8)</sup></td>
<td align="center">0.0200<sup>(10)</sup></td>
<td align="center">4.6</td>
<td align="left" rowspan="3">State-of-the-art methods</td>
</tr>
<tr>
<td align="center">MC</td>
<td align="center">0.1084<sup>(15)</sup></td>
<td align="center">0.3184<sup>(5)</sup></td>
<td align="center">0.0455<sup>(14)</sup></td>
<td align="center">0.0400<sup>(7)</sup></td>
<td align="center">0.0200<sup>(9)</sup></td>
<td align="center">10</td>
</tr>
<tr>
<td align="center">NMF-LP</td>
<td align="center">0.1461<sup>(9)</sup></td>
<td align="center">0.1715<sup>(18)</sup></td>
<td align="center">0.0621<sup>(13)</sup></td>
<td align="center">0.0243<sup>(9)</sup></td>
<td align="center">0.0146<sup>(11)</sup></td>
<td align="center">12</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pone.0208185.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.t004</object-id>
<label>Table 4</label> <caption><title>The average predicting AUC obtained by 100 independent runs on 5 real networks with external attributes.</title> <p>The training set contains 90% of the total connections.</p></caption>
<alternatives>
<graphic id="pone.0208185.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.t004" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">AUC</th>
<th align="center">Lazega</th>
<th align="center">Facebook</th>
<th align="center">Cornell</th>
<th align="center">Texas</th>
<th align="center">Washington</th>
<th align="center">Mean</th>
<th align="center">Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">SASNMF</td>
<td align="center">0.8003<sup>(4)</sup></td>
<td align="center">0.9354<sup>(3)</sup></td>
<td align="center">0.7000<sup>(9)</sup></td>
<td align="center">0.6398<sup>(15)</sup></td>
<td align="center">0.6886<sup>(10)</sup></td>
<td align="center">8.2</td>
<td align="center">A+S</td>
</tr>
<tr>
<td align="center">AA+SASNMF</td>
<td align="center">0.7717<sup>(9)</sup></td>
<td align="center">0.9075<sup>(11)</sup></td>
<td align="center">0.7830<sup>(4)</sup></td>
<td align="center">0.6734<sup>(8)</sup></td>
<td align="center">0.7368<sup>(5)</sup></td>
<td align="center">7.4</td>
<td align="center" rowspan="4">A+Sim</td>
</tr>
<tr>
<td align="center">CN+SASNMF</td>
<td align="center">0.7668<sup>(13)</sup></td>
<td align="center">0.9088<sup>(9)</sup></td>
<td align="center">0.7875<sup>(3)</sup></td>
<td align="center">0.6686<sup>(10)</sup></td>
<td align="center">0.7358<sup>(6)</sup></td>
<td align="center">8.2</td>
</tr>
<tr>
<td align="center">RA+SASNMF</td>
<td align="center">0.7704<sup>(11)</sup></td>
<td align="center">0.9137<sup>(8)</sup></td>
<td align="center">0.7876<sup>(2)</sup></td>
<td align="center">0.6730<sup>(9)</sup></td>
<td align="center">0.7410<sup>(3)</sup></td>
<td align="center">6.6</td>
</tr>
<tr>
<td align="center">Salton+SASNMF</td>
<td align="center">0.7707<sup>(10)</sup></td>
<td align="center">0.9138<sup>(7)</sup></td>
<td align="center">0.7817<sup>(5)</sup></td>
<td align="center">0.6746<sup>(7)</sup></td>
<td align="center">0.7378<sup>(4)</sup></td>
<td align="center">6.6</td>
</tr>
<tr>
<td align="center">A (= AA)+SASNMF</td>
<td align="center">0.7960<sup>(5)</sup></td>
<td align="center">0.8810<sup>(14)</sup></td>
<td align="center">0.7000<sup>(9)</sup></td>
<td align="center">0.7060<sup>(3)</sup></td>
<td align="center">0.7330<sup>(7)</sup></td>
<td align="center">7.6</td>
<td align="center" rowspan="4">Sim+S</td>
</tr>
<tr>
<td align="center">A (= CN)+ SASNMF</td>
<td align="center">0.8030<sup>(2)</sup></td>
<td align="center">0.8580<sup>(15)</sup></td>
<td align="center">0.6600<sup>(15)</sup></td>
<td align="center">0.6490<sup>(12)</sup></td>
<td align="center">0.6650<sup>(13)</sup></td>
<td align="center">11.4</td>
</tr>
<tr>
<td align="center">A (= RA)+ SASNMF</td>
<td align="center">0.8120<sup>(1)</sup></td>
<td align="center">0.8950<sup>(13)</sup></td>
<td align="center">0.7270<sup>(8)</sup></td>
<td align="center">0.7170<sup>(2)</sup></td>
<td align="center">0.7700<sup>(1)</sup></td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">A (= Salton)+ SASNMF</td>
<td align="center">0.7350<sup>(17)</sup></td>
<td align="center">0.8210<sup>(18)</sup></td>
<td align="center">0.6500<sup>(16)</sup></td>
<td align="center">0.5590<sup>(18)</sup></td>
<td align="center">0.6310<sup>(16)</sup></td>
<td align="center">17</td>
</tr>
<tr>
<td align="center">AA</td>
<td align="center">0.7864<sup>(7)</sup></td>
<td align="center">0.9355<sup>(2)</sup></td>
<td align="center">0.6973<sup>(11)</sup></td>
<td align="center">0.6807<sup>(5)</sup></td>
<td align="center">0.6919<sup>(9)</sup></td>
<td align="center">6.8</td>
<td align="center" rowspan="8">Baseline methods</td>
</tr>
<tr>
<td align="center">CN</td>
<td align="center">0.7768<sup>(8)</sup></td>
<td align="center">0.9243<sup>(6)</sup></td>
<td align="center">0.6673<sup>(14)</sup></td>
<td align="center">0.6489<sup>(13)</sup></td>
<td align="center">0.6609<sup>(14)</sup></td>
<td align="center">11</td>
</tr>
<tr>
<td align="center">RA</td>
<td align="center">0.7896<sup>(6)</sup></td>
<td align="center">0.9514<sup>(1)</sup></td>
<td align="center">0.6956<sup>(12)</sup></td>
<td align="center">0.6748<sup>(6)</sup></td>
<td align="center">0.6925<sup>(8)</sup></td>
<td align="center">6.6</td>
</tr>
<tr>
<td align="center">Salton</td>
<td align="center">0.7587<sup>(14)</sup></td>
<td align="center">0.9260<sup>(5)</sup></td>
<td align="center">0.6179<sup>(18)</sup></td>
<td align="center">0.5765<sup>(17)</sup></td>
<td align="center">0.6081<sup>(17)</sup></td>
<td align="center">14.2</td>
</tr>
<tr>
<td align="center">Jaccard</td>
<td align="center">0.7559<sup>(15)</sup></td>
<td align="center">0.9067<sup>(12)</sup></td>
<td align="center">0.6188<sup>(17)</sup></td>
<td align="center">0.5794<sup>(16)</sup></td>
<td align="center">0.6063<sup>(18)</sup></td>
<td align="center">15.6</td>
</tr>
<tr>
<td align="center">Katz</td>
<td align="center">0.5876<sup>(20)</sup></td>
<td align="center">0.3394<sup>(20)</sup></td>
<td align="center">0.6792<sup>(13)</sup></td>
<td align="center">0.3392<sup>(20)</sup></td>
<td align="center">0.3898<sup>(20)</sup></td>
<td align="center">18.6</td>
</tr>
<tr>
<td align="center">ACT</td>
<td align="center">0.6485<sup>(18)</sup></td>
<td align="center">0.8468<sup>(16)</sup></td>
<td align="center">0.7341<sup>(7)</sup></td>
<td align="center">0.7002<sup>(4)</sup></td>
<td align="center">0.6513<sup>(15)</sup></td>
<td align="center">12</td>
</tr>
<tr>
<td align="center">NMF</td>
<td align="center">0.7673<sup>(12)</sup></td>
<td align="center">0.9086<sup>(10)</sup></td>
<td align="center">0.7639<sup>(6)</sup></td>
<td align="center">0.6650<sup>(11)</sup></td>
<td align="center">0.6868<sup>(11)</sup></td>
<td align="center">10</td>
</tr>
<tr>
<td align="center">SPM</td>
<td align="center">0.8014<sup>(3)</sup></td>
<td align="center">0.9294<sup>(4)</sup></td>
<td align="center">0.8063<sup>(1)</sup></td>
<td align="center">0.7274<sup>(1)</sup></td>
<td align="center">0.7615<sup>(2)</sup></td>
<td align="center">2.2</td>
<td align="center" rowspan="3">State-of-the-art methods</td>
</tr>
<tr>
<td align="center">MC</td>
<td align="center">0.6072<sup>(19)</sup></td>
<td align="center">0.8326<sup>(17)</sup></td>
<td align="center">0.5068<sup>(19)</sup></td>
<td align="center">0.4354<sup>(19)</sup></td>
<td align="center">0.4770<sup>(19)</sup></td>
<td align="center">18.6</td>
</tr>
<tr>
<td align="center">NMF-LP</td>
<td align="center">0.7551<sup>(16)</sup></td>
<td align="center">0.7795<sup>(19)</sup></td>
<td align="center">0.6975<sup>(10)</sup></td>
<td align="center">0.6401<sup>(14)</sup></td>
<td align="center">0.6705<sup>(12)</sup></td>
<td align="center">14.2</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pone.0208185.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.t005</object-id>
<label>Table 5</label> <caption><title>The average predicting precision obtained by 100 independent runs on 8 real networks with only internal attributes.</title> <p>The training set contains 90% of the total connections.</p></caption>
<alternatives>
<graphic id="pone.0208185.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.t005" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Precision</th>
<th align="center">Karate</th>
<th align="center">Jazz</th>
<th align="center">USAir</th>
<th align="center">PolitB</th>
<th align="center">C.<italic>elegans</italic></th>
<th align="center">NetSci</th>
<th align="center">Metabolic</th>
<th align="center">Adjnoun</th>
<th align="center">Mean</th>
<th align="center">Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AA+SASNMF</td>
<td align="center">0.1575<sup>(4)</sup></td>
<td align="center">0.5519<sup>(7)</sup></td>
<td align="center">0.3387<sup>(6)</sup></td>
<td align="center">0.1829<sup>(2)</sup></td>
<td align="center">0.1432<sup>(5)</sup></td>
<td align="center">0.3595<sup>(7)</sup></td>
<td align="center">0.2630<sup>(3)</sup></td>
<td align="center">0.0684<sup>(4)</sup></td>
<td align="center">4.75</td>
<td align="center" rowspan="4">A+Sim</td>
</tr>
<tr>
<td align="center">CN+SASNMF</td>
<td align="center">0.1600<sup>(3)</sup></td>
<td align="center">0.5563<sup>(5)</sup></td>
<td align="center">0.2087<sup>(10)</sup></td>
<td align="center">0.1142<sup>(10)</sup></td>
<td align="center">0.1417<sup>(6)</sup></td>
<td align="center">0.3247<sup>(10)</sup></td>
<td align="center">0.1758<sup>(9)</sup></td>
<td align="center">0.0279<sup>(8)</sup></td>
<td align="center">7.625</td>
</tr>
<tr>
<td align="center">RA+SASNMF</td>
<td align="center">0.1525<sup>(5)</sup></td>
<td align="center">0.5570<sup>(4)</sup></td>
<td align="center">0.2051<sup>(11)</sup></td>
<td align="center">0.1185<sup>(9)</sup></td>
<td align="center">0.1459<sup>(4)</sup></td>
<td align="center">0.3555<sup>(8)</sup></td>
<td align="center">0.1797<sup>(7)</sup></td>
<td align="center">0.0272<sup>(9)</sup></td>
<td align="center">7.125</td>
</tr>
<tr>
<td align="center">Salton+SASNMF</td>
<td align="center">0.1725<sup>(2)</sup></td>
<td align="center">0.5588<sup>(3)</sup></td>
<td align="center">0.3096<sup>(8)</sup></td>
<td align="center">0.1455<sup>(7)</sup></td>
<td align="center">0.1466<sup>(3)</sup></td>
<td align="center">0.3306<sup>(9)</sup></td>
<td align="center">0.2308<sup>(4)</sup></td>
<td align="center">0.0329<sup>(7)</sup></td>
<td align="center">5.375</td>
</tr>
<tr>
<td align="center">AA</td>
<td align="center">0.1267<sup>(9)</sup></td>
<td align="center">0.5234<sup>(10)</sup></td>
<td align="center">0.3991<sup>(3)</sup></td>
<td align="center">0.1735<sup>(4)</sup></td>
<td align="center">0.1057<sup>(8)</sup></td>
<td align="center">0.7192<sup>(2)</sup></td>
<td align="center">0.1969<sup>(6)</sup></td>
<td align="center">0.0767<sup>(2)</sup></td>
<td align="center">5.5</td>
<td align="center" rowspan="8">Baseline methods</td>
</tr>
<tr>
<td align="center">CN</td>
<td align="center">0.1150<sup>(11)</sup></td>
<td align="center">0.5031<sup>(12)</sup></td>
<td align="center">0.3786<sup>(4)</sup></td>
<td align="center">0.1748<sup>(3)</sup></td>
<td align="center">0.0913<sup>(10)</sup></td>
<td align="center">0.5062<sup>(5)</sup></td>
<td align="center">0.1410<sup>(10)</sup></td>
<td align="center">0.0726<sup>(3)</sup></td>
<td align="center">7.25</td>
</tr>
<tr>
<td align="center">RA</td>
<td align="center">0.1371<sup>(7)</sup></td>
<td align="center">0.5413<sup>(8)</sup></td>
<td align="center">0.4683<sup>(1)</sup></td>
<td align="center">0.1504<sup>(6)</sup></td>
<td align="center">0.1029<sup>(9)</sup></td>
<td align="center">0.7312<sup>(1)</sup></td>
<td align="center">0.2726<sup>(2)</sup></td>
<td align="center">0.0649<sup>(5)</sup></td>
<td align="center">4.875</td>
</tr>
<tr>
<td align="center">Salton</td>
<td align="center">0.0008<sup>(14)</sup></td>
<td align="center">0.5314<sup>(9)</sup></td>
<td align="center">0.0521<sup>(14)</sup></td>
<td align="center">0.0102<sup>(14)</sup></td>
<td align="center">0.0182<sup>(14)</sup></td>
<td align="center">0.5496<sup>(3)</sup></td>
<td align="center">0.0510<sup>(12)</sup></td>
<td align="center">0.0014<sup>(12)</sup></td>
<td align="center">11.5</td>
</tr>
<tr>
<td align="center">Jaccard</td>
<td align="center">0.0013<sup>(13)</sup></td>
<td align="center">0.5176<sup>(11)</sup></td>
<td align="center">0.0677<sup>(12)</sup></td>
<td align="center">0.0167<sup>(13)</sup></td>
<td align="center">0.0207<sup>(13)</sup></td>
<td align="center">0.5489<sup>(4)</sup></td>
<td align="center">0.0495<sup>(13)</sup></td>
<td align="center">0.0016<sup>(11)</sup></td>
<td align="center">11.25</td>
</tr>
<tr>
<td align="center">Katz</td>
<td align="center">0.1358<sup>(8)</sup></td>
<td align="center">0.0202<sup>(14)</sup></td>
<td align="center">0.0527<sup>(13)</sup></td>
<td align="center">0.0265<sup>(12)</sup></td>
<td align="center">0.0222<sup>(12)</sup></td>
<td align="center">0.0995<sup>(13)</sup></td>
<td align="center">0.0192<sup>(14)</sup></td>
<td align="center">0.0009<sup>(13)</sup></td>
<td align="center">12.375</td>
</tr>
<tr>
<td align="center">ACT</td>
<td align="center">0.1088<sup>(12)</sup></td>
<td align="center">0.1679<sup>(13)</sup></td>
<td align="center">0.3304<sup>(7)</sup></td>
<td align="center">0.0740<sup>(11)</sup></td>
<td align="center">0.0533<sup>(11)</sup></td>
<td align="center">0.0000<sup>(14)</sup></td>
<td align="center">0.0934<sup>(11)</sup></td>
<td align="center">0.0967<sup>(1)</sup></td>
<td align="center">10</td>
</tr>
<tr>
<td align="center">NMF</td>
<td align="center">0.1488<sup>(6)</sup></td>
<td align="center">0.5548<sup>(6)</sup></td>
<td align="center">0.2111<sup>(9)</sup></td>
<td align="center">0.1213<sup>(8)</sup></td>
<td align="center">0.1493<sup>(2)</sup></td>
<td align="center">0.3189<sup>(11)</sup></td>
<td align="center">0.1796<sup>(8)</sup></td>
<td align="center">0.0235<sup>(10)</sup></td>
<td align="center">7.5</td>
</tr>
<tr>
<td align="center">SPM</td>
<td align="center">0.2250<sup>(1)</sup></td>
<td align="center">0.6092<sup>(2)</sup></td>
<td align="center">0.3677<sup>(5)</sup></td>
<td align="center">0.1711<sup>(5)</sup></td>
<td align="center">0.1702<sup>(1)</sup></td>
<td align="center">0.4801<sup>(6)</sup></td>
<td align="center">0.2888<sup>(1)</sup></td>
<td align="center">0.0386<sup>(6)</sup></td>
<td align="center">3.375</td>
<td align="center" rowspan="2">State-of-the-art methods</td>
</tr>
<tr>
<td align="center">MC</td>
<td align="center">0.1163<sup>(10)</sup></td>
<td align="center">0.6143<sup>(1)</sup></td>
<td align="center">0.4205<sup>(2)</sup></td>
<td align="center">0.1872<sup>(1)</sup></td>
<td align="center">0.1256<sup>(7)</sup></td>
<td align="center">0.3068<sup>(12)</sup></td>
<td align="center">0.2179<sup>(5)</sup></td>
<td align="center">0.0279<sup>(8)</sup></td>
<td align="center">5.75</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pone.0208185.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.t006</object-id>
<label>Table 6</label> <caption><title>The average predicting AUC obtained by 100 independent runs on 8 real networks with only internal attributes.</title> <p>The training set contains 90% of the total connections.</p></caption>
<alternatives>
<graphic id="pone.0208185.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.t006" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">AUC</th>
<th align="center">Karate</th>
<th align="center">Jazz</th>
<th align="center">USAir</th>
<th align="center">PolitB</th>
<th align="center">C.<italic>elegans</italic></th>
<th align="center">NetSci</th>
<th align="center">Metabolic</th>
<th align="center">Adjnoun</th>
<th align="center">Mean</th>
<th align="center">Mode</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AA+SASNMF</td>
<td align="center">0.7721<sup>(2)</sup></td>
<td align="center">0.9598<sup>(6)</sup></td>
<td align="center">0.9502<sup>(5)</sup></td>
<td align="center">0.9420<sup>(1)</sup></td>
<td align="center">0.8723<sup>(2)</sup></td>
<td align="center">0.9350<sup>(8)</sup></td>
<td align="center">0.8652<sup>(4)</sup></td>
<td align="center">0.7143<sup>(2)</sup></td>
<td align="center">3.75</td>
<td align="center" rowspan="4">A+Sim</td>
</tr>
<tr>
<td align="center">CN+SASNMF</td>
<td align="center">0.7361<sup>(6)</sup></td>
<td align="center">0.9534<sup>(11)</sup></td>
<td align="center">0.8987<sup>(10)</sup></td>
<td align="center">0.7980<sup>(12)</sup></td>
<td align="center">0.8332<sup>(7)</sup></td>
<td align="center">0.9401<sup>(6)</sup></td>
<td align="center">0.7979<sup>(9)</sup></td>
<td align="center">0.6213<sup>(10)</sup></td>
<td align="center">8.875</td>
</tr>
<tr>
<td align="center">RA+SASNMF</td>
<td align="center">0.7217<sup>(9)</sup></td>
<td align="center">0.9570<sup>(8)</sup></td>
<td align="center">0.8941<sup>(11)</sup></td>
<td align="center">0.8253<sup>(11)</sup></td>
<td align="center">0.8256<sup>(8)</sup></td>
<td align="center">0.9338<sup>(9)</sup></td>
<td align="center">0.7923<sup>(10)</sup></td>
<td align="center">0.6171<sup>(12)</sup></td>
<td align="center">9.75</td>
</tr>
<tr>
<td align="center">Salton+SASNMF</td>
<td align="center">0.7688<sup>(3)</sup></td>
<td align="center">0.9538<sup>(10)</sup></td>
<td align="center">0.9472<sup>(6)</sup></td>
<td align="center">0.8940<sup>(7)</sup></td>
<td align="center">0.8588<sup>(5)</sup></td>
<td align="center">0.9359<sup>(7)</sup></td>
<td align="center">0.8329<sup>(6)</sup></td>
<td align="center">0.6800<sup>(7)</sup></td>
<td align="center">6.375</td>
</tr>
<tr>
<td align="center">AA</td>
<td align="center">0.7282<sup>(8)</sup></td>
<td align="center">0.9664<sup>(3)</sup></td>
<td align="center">0.9684<sup>(2)</sup></td>
<td align="center">0.9270<sup>(2)</sup></td>
<td align="center">0.8654<sup>(4)</sup></td>
<td align="center">0.9916<sup>(2)</sup></td>
<td align="center">0.9561<sup>(2)</sup></td>
<td align="center">0.6866<sup>(5)</sup></td>
<td align="center">3.5</td>
<td align="center" rowspan="8">Baseline methods</td>
</tr>
<tr>
<td align="center">CN</td>
<td align="center">0.6984<sup>(10)</sup></td>
<td align="center">0.9591<sup>(7)</sup></td>
<td align="center">0.9550<sup>(3)</sup></td>
<td align="center">0.9213<sup>(4)</sup></td>
<td align="center">0.8423<sup>(6)</sup></td>
<td align="center">0.9904<sup>(5)</sup></td>
<td align="center">0.9236<sup>(3)</sup></td>
<td align="center">0.6898<sup>(4)</sup></td>
<td align="center">5.25</td>
</tr>
<tr>
<td align="center">RA</td>
<td align="center">0.7338<sup>(7)</sup></td>
<td align="center">0.9721<sup>(1)</sup></td>
<td align="center">0.9734<sup>(1)</sup></td>
<td align="center">0.9265<sup>(3)</sup></td>
<td align="center">0.8695<sup>(3)</sup></td>
<td align="center">0.9908<sup>(4)</sup></td>
<td align="center">0.9607<sup>(1)</sup></td>
<td align="center">0.6819<sup>(6)</sup></td>
<td align="center">3.25</td>
</tr>
<tr>
<td align="center">Salton</td>
<td align="center">0.6321<sup>(12)</sup></td>
<td align="center">0.9667<sup>(2)</sup></td>
<td align="center">0.9254<sup>(7)</sup></td>
<td align="center">0.8782<sup>(8)</sup></td>
<td align="center">0.7874<sup>(11)</sup></td>
<td align="center">0.9931<sup>(1)</sup></td>
<td align="center">0.8119<sup>(7)</sup></td>
<td align="center">0.6202<sup>(11)</sup></td>
<td align="center">7.375</td>
</tr>
<tr>
<td align="center">Jaccard</td>
<td align="center">0.6068<sup>(13)</sup></td>
<td align="center">0.9619<sup>(5)</sup></td>
<td align="center">0.9178<sup>(8)</sup></td>
<td align="center">0.8752<sup>(9)</sup></td>
<td align="center">0.7924<sup>(10)</sup></td>
<td align="center">0.9915<sup>(3)</sup></td>
<td align="center">0.7808<sup>(11)</sup></td>
<td align="center">0.6257<sup>(9)</sup></td>
<td align="center">8.5</td>
</tr>
<tr>
<td align="center">Katz</td>
<td align="center">0.7475<sup>(4)</sup></td>
<td align="center">0.4076<sup>(14)</sup></td>
<td align="center">0.3843<sup>(14)</sup></td>
<td align="center">0.4766<sup>(14)</sup></td>
<td align="center">0.4722<sup>(14)</sup></td>
<td align="center">0.9206<sup>(10)</sup></td>
<td align="center">0.4535<sup>(14)</sup></td>
<td align="center">0.2607<sup>(14)</sup></td>
<td align="center">12.25</td>
</tr>
<tr>
<td align="center">ACT</td>
<td align="center">0.6603<sup>(11)</sup></td>
<td align="center">0.7973<sup>(13)</sup></td>
<td align="center">0.8990<sup>(9)</sup></td>
<td align="center">0.9006<sup>(6)</sup></td>
<td align="center">0.7548<sup>(12)</sup></td>
<td align="center">0.5758<sup>(14)</sup></td>
<td align="center">0.7654<sup>(12)</sup></td>
<td align="center">0.7462<sup>(1)</sup></td>
<td align="center">9.75</td>
</tr>
<tr>
<td align="center">NMF</td>
<td align="center">0.7387<sup>(5)</sup></td>
<td align="center">0.9556<sup>(9)</sup></td>
<td align="center">0.8761<sup>(12)</sup></td>
<td align="center">0.8395<sup>(10)</sup></td>
<td align="center">0.8250<sup>(9)</sup></td>
<td align="center">0.9039<sup>(12)</sup></td>
<td align="center">0.8008<sup>(8)</sup></td>
<td align="center">0.6352<sup>(8)</sup></td>
<td align="center">9.125</td>
</tr>
<tr>
<td align="center">SPM</td>
<td align="center">0.7978<sup>(1)</sup></td>
<td align="center">0.9624<sup>(4)</sup></td>
<td align="center">0.9504<sup>(4)</sup></td>
<td align="center">0.9132<sup>(5)</sup></td>
<td align="center">0.8766<sup>(1)</sup></td>
<td align="center">0.9110<sup>(11)</sup></td>
<td align="center">0.8482<sup>(5)</sup></td>
<td align="center">0.7082<sup>(3)</sup></td>
<td align="center">4.25</td>
<td align="center" rowspan="2">State-of-the-art methods</td>
</tr>
<tr>
<td align="center">MC</td>
<td align="center">0.5704<sup>(14)</sup></td>
<td align="center">0.8709<sup>(12)</sup></td>
<td align="center">0.8142<sup>(13)</sup></td>
<td align="center">0.6767<sup>(13)</sup></td>
<td align="center">0.5874<sup>(13)</sup></td>
<td align="center">0.6721<sup>(13)</sup></td>
<td align="center">0.6026<sup>(13)</sup></td>
<td align="center">0.4670<sup>(13)</sup></td>
<td align="center">13</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Tables <xref ref-type="table" rid="pone.0208185.t003">3</xref> and <xref ref-type="table" rid="pone.0208185.t004">4</xref> show the results calculated on five networks with external auxiliary information (namely, node attributes), while Tables <xref ref-type="table" rid="pone.0208185.t005">5</xref> and <xref ref-type="table" rid="pone.0208185.t006">6</xref> show the eight networks with only internal information. To facilitate comparison, we add Mode column to the table, and classify it according to different combination mode and different comparison method to show the difference. In the four tables, the presented links for every dataset are partitioned into a training set (90%) and a probe set (10%). From these tables, we can see that the prediction results by means of various combination formulas under the SASNMF framework are significantly better than the other comparison methods. In addition, these methods using external auxiliary information are generally superior to the baseline methods that use only structure information.</p>
<p>These experimental results are classified according to whether the network has external auxiliary information, namely, node attributes, and both AUC and precision evaluation criteria were used for performance analysis. In the four tables, the upper right of the numbers represents the respective Precision-ranking (AUC-ranking) position of each method in each network. The smaller the number is, the better the prediction performance of the algorithm (see <bold><xref ref-type="supplementary-material" rid="pone.0208185.s001">S1 File</xref></bold>). To reflect the overall performance of all algorithms on different networks, the column labelled as Mean in the table is the mean ranking value of each method across all the networks. It is an indicator of average performance. To facilitate analysis, the column labelled as Mode represents different information combinations. Through the results shown in these four tables, we can see that although the methods proposed: A+S, A + Sim, Sim + S were not always the best, it can be found from the average of performance ranking levels on each network that the prediction performance of these three forms based on the SASNMF framework are in the leading position as a whole. This finding indicates that this auxiliary information, including the internal structure latent features and the external node attributes, is salutary to enhance the accuracy of link prediction.</p>
<p>To further test the overall prediction effect of the three combination methods proposed, we give only the results of precision and AUC based on four baseline methods, AA, CN, RA and Salton on real networks in <xref ref-type="fig" rid="pone.0208185.g002">Fig 2</xref>. Here, we use a baseline method and its two combinations, namely, A+Sim and Sim+S, to compare with SASNMF.</p>
<fig id="pone.0208185.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The AUC and precision score on 5 real networks with external attribute information.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.g002" xlink:type="simple"/>
</fig>
<p>Similarly, to compare the overall performance of the combined mode A+Sim with the baseline method and the state-of-the-art methods on 13 real networks, we consider four baseline methods (AA, CN, RA and Salton) and their combined modes. The AUC and precision results are shown in Figs <xref ref-type="fig" rid="pone.0208185.g003">3</xref> and <xref ref-type="fig" rid="pone.0208185.g004">4</xref>.</p>
<fig id="pone.0208185.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The AUC and precision results compared with baseline methods on 13 real networks.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.g003" xlink:type="simple"/>
</fig>
<fig id="pone.0208185.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The AUC results compared with the state-of-the-art methods on 13 real networks.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.g004" xlink:type="simple"/>
</fig>
<p>From <xref ref-type="fig" rid="pone.0208185.g004">Fig 4</xref>, we can see that the proposed combination method based on our framework is also better overall than the MC and NMF methods besides the SPM. Of course, the SPM method is not as good as our method on some of the datasets in the experiment.</p>
<p>In addition, to test the performance of our methods, the relative precision and AUC results of our proposed methods and other baseline methods under different fractions of training sets in the different network are shown in <xref ref-type="fig" rid="pone.0208185.g005">Fig 5</xref>.</p>
<fig id="pone.0208185.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.g005</object-id>
<label>Fig 5</label>
<caption>
<title>The precision and AUC results in different proportion training sets.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.g005" xlink:type="simple"/>
</fig>
<p>For the NMF-LP method, because it is a link prediction method based on node attribute information, we only make a comparative analysis with it on these networks with node attributes. In the whole comparative experiment, we find that the time complexity of NMF-LP method is much higher than our algorithm, and from the final experimental results, the performance of our algorithm is more competitive than it.</p>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>In summary, real networks are sparse and contain noise. To overcome prediction difficulties by means of internal and external auxiliary information, we proposed a unified prediction framework based on non-negative matrix factorization with coupling multivariate information, which can model the internal latent feature information and external node attribute information of the network. Based on this framework, we also proposed three combination methods that are represented as A+S, A+Sim, and Sim+S. According to the proposed combination patterns, we design a large number of experiments for networks with node attributes and networks without node attributes under our framework. We compared the proposed methods with 8 benchmark methods and 3 state-of-the-art methods on 13 real network datasets.</p>
<p>In addition, the selection of the rank after the matrix decomposition was also important because of its effect on the prediction result and the number of latent features k in the SASNMF framework is different for each dataset. Here, to illustrate the problem, the results of different k for the Lazega-lawyer dataset are shown as follows in <xref ref-type="fig" rid="pone.0208185.g006">Fig 6</xref>.</p>
<fig id="pone.0208185.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0208185.g006</object-id>
<label>Fig 6</label>
<caption>
<title>The accuracy of different k values is calculated and compared by two metrics.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.g006" xlink:type="simple"/>
</fig>
<p>In the figure, the training sets are from 90% to 20% and only a network dataset—Lazega-lawyer.</p>
<p>As seen in Figs <xref ref-type="fig" rid="pone.0208185.g002">2</xref> and <xref ref-type="fig" rid="pone.0208185.g003">3</xref>, the methods in which the mode is A+S, A+Sim and Sim+S are better than the corresponding benchmark methods. Especially, through our framework, the prediction effect of using node attributes as auxiliary information is competitive compared to those baseline methods.</p>
<p>To better test the extensibility and robustness, <xref ref-type="fig" rid="pone.0208185.g005">Fig 5</xref> shows the results of precision and AUC under different proportions of training sets E<sup><italic>train</italic></sup> and test sets E<sup><italic>test</italic></sup>. <xref ref-type="fig" rid="pone.0208185.g005">Fig 5</xref> shows a prediction trend for five attribute networks, where the partition ratio, E<sup><italic>train</italic></sup> and E<sup><italic>test</italic></sup>, is from 0.9 to 0.2. We find that the performance of all methods declines obviously as the E<sup><italic>train</italic></sup> ratio decreases in <xref ref-type="fig" rid="pone.0208185.g005">Fig 5</xref>. However, there is a gentle trend decline under the SASNMF method. Moreover, from the whole process of dataset partitioning to analyse the results synthetically, its prediction effect is obviously superior to other baseline methods. This finding indicates that these methods that rely only on structural information can make the prediction worse as the number of connected sets in the training set decreases. Our framework can alleviate the problem of data sparsity by coupling multivariate auxiliary information. Especially, on the Lazega-lawyer and Facebook datasets, the impact of using SASNMF on the results is obviously better than that of other comparison methods. Although the precision test of the Cornell, Texas and Washington datasets is inferior to that of AA and RA, our model is far better than that of these two methods under the corresponding AUC evaluation. It can be said that the overall effect of our method is good under the AUC index.</p>
<p>Therefore, why does our method not work well on these three datasets? Through in-depth analysis, we think that the main reason for this phenomenon lies in the attribute information. In fact, the attribute values used in these three datasets are simply quantized whether the words in the article appear or not, compared with the first two data sets. However, the attribute values of the first two datasets are true social attributes. Therefore, the attribute of these three networks cannot be said to better reflect the true similarity between nodes.</p>
<p>In addition, the number of latent features k in the SASNMF framework is different for each dataset. Moreover, the determination of the latent features k is a very important and difficult problem in matrix factorization. <xref ref-type="fig" rid="pone.0208185.g006">Fig 6</xref> shows only the results under different k for the Lazega-lawyer dataset. In this paper, because it is not our primary focus, we take an easy and effective method for automatic determination of k, by Colibri [<xref ref-type="bibr" rid="pone.0208185.ref062">62</xref>], which seeks a nonorthogonal basis by sampling the columns of the input matrix. However, to observe the influence of different k in the process of matrix factorization for the prediction effect, we take some of k’s value by means of the limitative form of k(m + n) ≪ mn provisionally. Due to the adjacent matrix A being symmetrical here, the k is far less than n/2. <xref ref-type="fig" rid="pone.0208185.g006">Fig 6</xref> shows that the influence of the selection of k on the prediction results is obvious.</p>
</sec>
<sec id="sec020" sec-type="conclusions">
<title>Conclusion</title>
<p>In recent years, link prediction based on network topology has been one of the research hotspots in the field of data mining. However, in many instances, algorithms that use only network structure do not provide the precision needed for link prediction. At present, with the development of mobile Internet, the more descriptive information owned by the entities in the network is becoming an asset to be used. Inspired by this, based on the advantages of NMF such as interpretability, nonnegativity and information fusion, a unified framework of link prediction is proposed in this paper. By this framework, the adjacency matrix A, which represents the macroscopic information of a network topology, and the auxiliary information matrix S, which represents the microscopic information of the network, are mapped to the same low-rank latent feature space to realize the multivariate information coupling. Then, the link prediction task can be realized by merging into a prediction matrix that can infer the missing relationship of the network. At the same time, to further analyse the usability of the network auxiliary information, we not only use the external attributes of the nodes but also explore the latent features of the nodes that are extracted as internal auxiliary information by some traditional structural similarity indices from local and global perspectives. On the basis of multivariate information, we further propose three different combinations. We used three class combination forms as the simulation cases of the proposed framework and experiments to show the feasibility, effectiveness, and competitiveness of the framework. Moreover, a large number of experiments on five networks with node sociological attributes and eight networks without node attributes show that the prediction performance under this unified framework is competitive compared with seven baseline methods and three state-of-art methods on the whole according to the different combination patterns proposed by us. This finding demonstrates that the proposed framework has advantages in combining the structure and attribute information for link prediction. Furthermore, the framework is easy to extend to directed and weighted networks by letting the matrix V be directed and weighted because it is based on NMF.</p>
<p>In the future, there are some limitations and improved studies for our proposed framework. One of which is how to set parameters α and β to be adaptive on different networks. Furthermore, we will extend our methods to more generalized situations such as extending the model to edge attributes and combination attributes of edges and nodes and dynamic network link prediction. Designing efficient methods to solve these issues will be interesting.</p>
</sec>
<sec id="sec021">
<title>Supporting information</title>
<supplementary-material id="pone.0208185.s001" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pone.0208185.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>This is the data source for Figs <xref ref-type="fig" rid="pone.0208185.g004">4</xref> and <xref ref-type="fig" rid="pone.0208185.g005">5</xref>.</title>
<p>(XLSX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pone.0208185.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lü</surname> <given-names>LY</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>T</given-names></name>. <article-title>Link prediction in complex networks: A survey</article-title>. <source>Physica A Statistical Mechanics &amp; Its Applications</source>, <year>2011</year>, <volume>390</volume>(<issue>6</issue>):<fpage>1150</fpage>–<lpage>1170</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.physa.2010.11.027" xlink:type="simple">https://doi.org/10.1016/j.physa.2010.11.027</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>YR</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>XY</given-names></name>. <article-title>Link prediction in social networks: the state-of-the-art</article-title>. <source>Science China Information Sciences</source>, <year>2015</year>, <volume>58</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>38</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11432-014-5237-y" xlink:type="simple">https://doi.org/10.1007/s11432-014-5237-y</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martínez</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Berzal</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Cubero</surname> <given-names>J C</given-names></name>. <article-title>A Survey of Link Prediction in Complex Networks</article-title>. <source>Acm Computing Surveys</source>, <year>2017</year>, <volume>49</volume>(<issue>4</issue>):<fpage>69</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/3012704" xlink:type="simple">https://doi.org/10.1145/3012704</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref004"><label>4</label><mixed-citation publication-type="other" xlink:type="simple">kumar R, Novak J, Tomkins A. Structure and evolution of online social networks. KDD’06, August 20–23, 2006, Philadelphia, Pennsylvania, USA.</mixed-citation></ref>
<ref id="pone.0208185.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Zhang Q</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lü</surname> <given-names>LY</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>T</given-names></name>. <article-title>Link prediction in complex networks: a local naïve Bayes model</article-title>. <source>Europhysics Letters</source>, <year>2011</year>, <volume>96</volume>(<issue>4</issue>): <fpage>48007</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1209/0295-5075/96/48007" xlink:type="simple">https://doi.org/10.1209/0295-5075/96/48007</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guan</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>An</surname> <given-names>HZ</given-names></name>, <name name-style="western"><surname>Gao</surname> <given-names>XY</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>HJ</given-names></name>. <article-title>Estimating potential trade links in the international crude oil trade: A link prediction approach</article-title>. <source>Energy</source>, <year>2016</year>, <volume>102</volume>:<fpage>406</fpage>–<lpage>415</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.energy.2016.02.099" xlink:type="simple">https://doi.org/10.1016/j.energy.2016.02.099</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref007"><label>7</label><mixed-citation publication-type="other" xlink:type="simple">Cheng ZY, Caverlee J, Lee K, Sui DZ. Exploring Millions of Footprints in Location Sharing Services. Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media, 2011: 81–88.</mixed-citation></ref>
<ref id="pone.0208185.ref008"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Feng SS, Li XT, Zeng YF, C G, Chee YM, Y Q. Personalized ranking metric embedding for next new POI recommendation. International Conference on Artificial Intelligence. AAAI Press, 2015:2069–2075.</mixed-citation></ref>
<ref id="pone.0208185.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bohannon</surname> <given-names>John</given-names></name>. <article-title>Counterterrorism's new tool: 'metanetwork' analysis</article-title>. <source>Science</source>, <year>2009</year>, <volume>325</volume>(<issue>5939</issue>):<fpage>409</fpage>–<lpage>411</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.325_409" xlink:type="simple">https://doi.org/10.1126/science.325_409</ext-link> <object-id pub-id-type="pmid">19628852</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benigni</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Joseph</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Carley</surname> <given-names>KM</given-names></name>. <article-title>Online extremism and the communities that sustain it: Detecting the ISIS supporting community on Twitter</article-title>. <source>Plos One</source>, <year>2017</year>, <volume>12</volume>(<issue>12</issue>):<fpage>e0181405</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0181405" xlink:type="simple">https://doi.org/10.1371/journal.pone.0181405</ext-link> <object-id pub-id-type="pmid">29194446</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref011"><label>11</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Tayebi M</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Glässer</surname> <given-names>U</given-names></name>. <source>Social Network Analysis in Predictive Policing</source>. <publisher-name>Springer press</publisher-name>, <year>2016</year>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-3-319-41492-8_2" xlink:type="simple">https://doi.org/10.1007/978-3-319-41492-8_2</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Budur</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kong</surname> <given-names>VS</given-names></name>. <article-title>Structural Analysis of Criminal Network and Predicting Hidden Links using Machine Learning</article-title>. <source>Computer Science</source>, <year>2015</year>:<fpage>641</fpage>–<lpage>650</lpage>.</mixed-citation></ref>
<ref id="pone.0208185.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berlusconi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Calderoni</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Parolini</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Verani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Piccardi</surname> <given-names>C</given-names></name>. <article-title>Link Prediction in Criminal Networks: A Tool for Criminal Intelligence Analysis</article-title>. <source>Plos One</source>, <year>2016</year>, <volume>11</volume>(<issue>4</issue>):<fpage>e0154244</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0154244" xlink:type="simple">https://doi.org/10.1371/journal.pone.0154244</ext-link> <object-id pub-id-type="pmid">27104948</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liben-Nowell</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kleinberg</surname> <given-names>J</given-names></name>. <article-title>The Link Prediction Problem for Social Networks</article-title>. <source>Journal of the American Society for Information Science and Technology</source>, <year>2007</year>, <volume>58</volume>(<issue>7</issue>):<fpage>1019</fpage>–<lpage>1031</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/asi.v58:7" xlink:type="simple">https://doi.org/10.1002/asi.v58:7</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jordan</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Alves</surname> <given-names>OCP</given-names></name>, <name name-style="western"><surname>Wilde</surname> <given-names>PD</given-names></name>, <name name-style="western"><surname>Lima-Neto</surname> <given-names>FBD</given-names></name>. <article-title>Link-prediction to tackle the boundary specification problem in social network surveys</article-title>. <source>Plos One</source>, <year>2017</year>, <volume>12</volume>(<issue>4</issue>):<fpage>e0176094</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0176094" xlink:type="simple">https://doi.org/10.1371/journal.pone.0176094</ext-link> <object-id pub-id-type="pmid">28426826</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsugawa</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kito</surname> <given-names>K</given-names></name>. <article-title>Retweets as a Predictor of Relationships among Users on Social Media</article-title>. <source>Plos One</source>, <year>2017</year>, <volume>12</volume>(<issue>1</issue>):<fpage>e0170279</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0170279" xlink:type="simple">https://doi.org/10.1371/journal.pone.0170279</ext-link> <object-id pub-id-type="pmid">28107489</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasan M</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Chaoji</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Salem</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Zaki</surname> <given-names>M</given-names></name>. <article-title>Link prediction using supervised learning</article-title>. <source>Proc of Sdm Workshop on Link Analysis Counterterrorism &amp; Security</source>, <year>2006</year>, <volume>30</volume>(<issue>9</issue>):<fpage>798</fpage>–<lpage>805</lpage>.</mixed-citation></ref>
<ref id="pone.0208185.ref018"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Menon A K, Elkan C. A Log-Linear Model with Latent Features for Dyadic Prediction. IEEE, International Conference on Data Mining. IEEE, 2011:364–373. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ICDM.2010.148" xlink:type="simple">https://doi.org/10.1109/ICDM.2010.148</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref019"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Menon AK, Elkan C. Link prediction via matrix factorization. European Conference on Machine Learning and Knowledge Discovery in Databases. Springer-Verlag, 2011:437–452. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-3-642-23783-6_28" xlink:type="simple">https://doi.org/10.1007/978-3-642-23783-6_28</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clauset</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Newman M</surname> <given-names>E</given-names></name>. <article-title>Hierarchical structure and the prediction of missing links in networks</article-title>. <source>Nature</source>, <year>2008</year>, <volume>453</volume>(<issue>7191</issue>):<fpage>98</fpage>–<lpage>101</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature06830" xlink:type="simple">https://doi.org/10.1038/nature06830</ext-link> <object-id pub-id-type="pmid">18451861</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pan</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Lü</surname> <given-names>LY</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>CK</given-names></name>. <article-title>Predicting missing links and identifying spurious links via likelihood analysis</article-title>. <source>Scientific Reports</source>, <year>2016</year>, <volume>6</volume>:<fpage>22955</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep22955" xlink:type="simple">https://doi.org/10.1038/srep22955</ext-link> <object-id pub-id-type="pmid">26961965</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lü</surname> <given-names>LY</given-names></name>, <name name-style="western"><surname>Pan</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>YC</given-names></name>, <name name-style="western"><surname>Stanley H</surname> <given-names>E</given-names></name>. <article-title>Toward link predictability of complex networks</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <year>2015</year>, <volume>112</volume>(<issue>8</issue>):<fpage>2325</fpage>–<lpage>30</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1424644112" xlink:type="simple">https://doi.org/10.1073/pnas.1424644112</ext-link> <object-id pub-id-type="pmid">25659742</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu</surname> <given-names>XY</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Jiao</surname> <given-names>LC</given-names></name>. <article-title>Link prediction in complex networks via matrix perturbation and decomposition</article-title>. <source>Scientific Reports</source>, <year>2017</year>, <volume>7</volume>(<issue>1</issue>). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-017-14847-2" xlink:type="simple">https://doi.org/10.1038/s41598-017-14847-2</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Cai</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Jiao</surname> <given-names>PF</given-names></name>, <name name-style="western"><surname>P</surname> <given-names>L</given-names></name>. <article-title>A perturbation-based framework for link prediction via non-negative matrix factorization</article-title>. <source>Scientific Reports</source>, <year>2016</year>, <volume>6</volume>:<fpage>38938</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/srep38938" xlink:type="simple">https://doi.org/10.1038/srep38938</ext-link> <object-id pub-id-type="pmid">27976672</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ratha</surname> <given-names>Pech</given-names></name>, <name name-style="western"><surname>Hao</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Pan</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Cheng</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>T</given-names></name>. <article-title>Link Prediction via Matrix Completion</article-title>. <source>Europhysics Letters</source>,<year>2017</year>,<volume>117</volume>(<issue>3</issue>). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1209/0295-5075/117/38002" xlink:type="simple">https://doi.org/10.1209/0295-5075/117/38002</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref026"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Fond T L, Neville J. Randomization tests for distinguishing social influence and homophily effects. In Proceedings of the World Wide Web Conference (WWW). ACM, New York, 2011, 601–610. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1772690.1772752" xlink:type="simple">https://doi.org/10.1145/1772690.1772752</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kumar</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Novak</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Raghavan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tomkins</surname> <given-names>A</given-names></name>. <article-title>Structure and evolution of blogspace</article-title>. <source>Communications of the ACM</source>, <year>2004</year>, <volume>47</volume> (<issue>12</issue>): <fpage>35</fpage>–<lpage>39</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1035134.1035162" xlink:type="simple">https://doi.org/10.1145/1035134.1035162</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref028"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">Kim M, Leskovec J. Modeling social networks with node attributes using the multiplicative attribute graph model. In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence(UAI),2011. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/15427951.2012.625257" xlink:type="simple">https://doi.org/10.1080/15427951.2012.625257</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kossinets</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Watts</surname> <given-names>DJ</given-names></name>. <article-title>Empirical analysis of an evolving social network</article-title>. <source>Science</source>, <year>2006</year>,<volume>311</volume>(<issue>5757</issue>): <fpage>88</fpage>–<lpage>90</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.1116869" xlink:type="simple">https://doi.org/10.1126/science.1116869</ext-link> <object-id pub-id-type="pmid">16400149</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref030"><label>30</label><mixed-citation publication-type="other" xlink:type="simple">Yin ZJ, Gupta M, Weninger T, Han JW. LINKREC: a unified framework for link recommendation with user attributes and graph structure. International Conference on World Wide Web, WWW 2010:1211–1212. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1772690.1772879" xlink:type="simple">https://doi.org/10.1145/1772690.1772879</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname> <given-names>ZC</given-names></name>, <name name-style="western"><surname>Ye</surname> <given-names>YM</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>XT</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>HJ</given-names></name>. <article-title>Joint Weighted Nonnegative Matrix Factorization for Mining Attributed Graphs</article-title>. <source>Advances in Knowledge Discovery and Data Mining</source>. <year>2017</year>:<fpage>368</fpage>–<lpage>380</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-3-319-57454-7_29" xlink:type="simple">https://doi.org/10.1007/978-3-319-57454-7_29</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref032"><label>32</label><mixed-citation publication-type="other" xlink:type="simple">Hsu CC, Lai YA, Chen WH, Feng MH, Lin SD. Unsupervised Ranking using Graph Structures and Node Attributes. Tenth ACM International Conference on Web Search and Data Mining. ACM, 2017:771–779. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/3018661.3018668" xlink:type="simple">https://doi.org/10.1145/3018661.3018668</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref033"><label>33</label><mixed-citation publication-type="other" xlink:type="simple">Shi SL, Li YP, Wen YM, Xie W. Adding the sentiment attribute of nodes to improve link prediction in social network. International Conference on Fuzzy Systems and Knowledge Discovery. IEEE, 2015:1263–1269. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/FSKD.2015.7382124" xlink:type="simple">https://doi.org/10.1109/FSKD.2015.7382124</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref034"><label>34</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Mallek</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Boukhris</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Elouedi</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Lefevre</surname> <given-names>E</given-names></name>. <source>Evidential Link Prediction in Uncertain Social Networks Based on Node Attributes</source>. <publisher-name>Springer press</publisher-name>, <year>2017</year>: <fpage>595</fpage>–<lpage>601</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-3-319-60042-0_65" xlink:type="simple">https://doi.org/10.1007/978-3-319-60042-0_65</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref035"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Miller KT., Griffiths TL, Jordan MI. Nonparametric latent feature models for link prediction. In Proceedings of the Neural Information Processing Systems Conference (NIPS), 2009. <ext-link ext-link-type="uri" xlink:href="http://173.236.226.255/tom/papers/linkpred.pdf" xlink:type="simple">http://173.236.226.255/tom/papers/linkpred.pdf</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref036"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">A. P. Singh and G. J. Gordon. 2008. Relational learning via collective matrix factorization. In Proceedings of the KDD. Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24–27, 2008. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1401890.1401969" xlink:type="simple">https://doi.org/10.1145/1401890.1401969</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fan</surname> <given-names>XH</given-names></name>, <name name-style="western"><surname>Richard Xu</surname> <given-names>YD</given-names></name>, <name name-style="western"><surname>Cao</surname> <given-names>LB</given-names></name>, <name name-style="western"><surname>S</surname> <given-names>Y</given-names></name>. <article-title>Learning Nonparametric Relational Models by Conjugately Incorporating Node Information in a Network</article-title>. <source>IEEE Transactions on Cybernetics</source>, <year>2017</year>, <volume>47</volume>(<issue>3</issue>):<fpage>589</fpage>–<lpage>599</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TCYB.2016.2521376" xlink:type="simple">https://doi.org/10.1109/TCYB.2016.2521376</ext-link> <object-id pub-id-type="pmid">26887024</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref038"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Yuan GC, Murukannaiah PK, Zhang Z, Singh MP. Exploiting sentiment homophily for link prediction. 8th ACM Conference on Recommender Systems, 2014:17–24. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/2645710.2645734" xlink:type="simple">https://doi.org/10.1145/2645710.2645734</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gong</surname> <given-names>NZ</given-names></name>, <name name-style="western"><surname>Talwalkar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Mackey</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Richard Shin E</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Stefanov</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Joint Link Prediction and Attribute Inference Using a Social-Attribute Network</article-title>. <source>Acm Transactions on Intelligent Systems &amp; Technology</source>, <year>2014</year>, <volume>5</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>20</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/2594455" xlink:type="simple">https://doi.org/10.1145/2594455</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Z</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Gao</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Y</surname> <given-names>G</given-names></name>. <article-title>A New Method for Link Prediction Using Various Features in Social Networks</article-title>. <source>Web Information System and Application Conference</source>. IEEE, <volume>2015</volume>:<fpage>144</fpage>–<lpage>147</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/WISA.2014.34" xlink:type="simple">https://doi.org/10.1109/WISA.2014.34</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>FF</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>SB</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>L</given-names></name>. <article-title>Link prediction based on non-negative matrix factorization[J]</article-title>. <source>Plos One</source>, <year>2017</year>, <volume>12</volume>(<issue>8</issue>):<fpage>e0182968</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0182968" xlink:type="simple">https://doi.org/10.1371/journal.pone.0182968</ext-link> <object-id pub-id-type="pmid">28854195</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Backstrom</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Leskovec</surname> <given-names>J</given-names></name>. <article-title>Supervised random walks: predicting and recommending links in social networks</article-title>. <source>ACM International Conference on Web Search and Data Mining</source>. ACM, <year>2011</year>:<fpage>635</fpage>–<lpage>644</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1935826.1935914" xlink:type="simple">https://doi.org/10.1145/1935826.1935914</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>. <article-title>Learning the parts of objects by non-negative matrix factorization</article-title>. <source>Nature</source>.<year>1999</year>, <volume>401</volume>(<issue>6755</issue>): <fpage>788</fpage>–<lpage>791</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/44565" xlink:type="simple">https://doi.org/10.1038/44565</ext-link> <object-id pub-id-type="pmid">10548103</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>YX</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>YJ</given-names></name>. <article-title>Nonnegative Matrix Factorization: A Comprehensive Review</article-title>. <source>IEEE Transactions on Knowledge &amp; Data Engineering</source>, <year>2013</year>, <volume>25</volume>(<issue>6</issue>):<fpage>1336</fpage>–<lpage>1353</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TKDE.2012.51" xlink:type="simple">https://doi.org/10.1109/TKDE.2012.51</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gemulla</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Nijkamp</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Haas</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Sismanis</surname> <given-names>Y</given-names></name>. <article-title>Large-scale matrix factorization with distributed stochastic gradient descent</article-title>. <source>KDD’11</source>, <volume>2011</volume>: <fpage>69</fpage>–<lpage>77</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/2020408.2020426" xlink:type="simple">https://doi.org/10.1145/2020408.2020426</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref046"><label>46</label><mixed-citation publication-type="other" xlink:type="simple">Bao Y, Fang H, Zhang J. TopicMF: simultaneously exploiting ratings and reviews for recommendation. Twenty-Eighth AAAI Conference on Artificial Intelligence. 2014:2–8.</mixed-citation></ref>
<ref id="pone.0208185.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>XC</given-names></name>, <name name-style="western"><surname>Zong</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>XY</given-names></name>. <article-title>Constrained Clustering With Nonnegative Matrix Factorization</article-title>. <source>IEEE Transactions on Neural Networks &amp; Learning Systems</source>, <year>2016</year>, <volume>27</volume>(<issue>7</issue>):<fpage>1514</fpage>–<lpage>1526</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TNNLS.2015.2448653" xlink:type="simple">https://doi.org/10.1109/TNNLS.2015.2448653</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref048"><label>48</label><mixed-citation publication-type="other" xlink:type="simple">Yang Q, Dong EM, Xie Z. Link prediction via nonnegative matrix factorization enhanced by blocks information. In: 2014 10th International Conference on Natural Computation (ICNC), IEEE, 2014:823–827. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ICNC.2014.6975944" xlink:type="simple">https://doi.org/10.1109/ICNC.2014.6975944</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vasiloglou</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Gray</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>DV</given-names></name>. <article-title>Non-Negative Matrix Factorization, Convexity and Isometry</article-title>. <source>Proc. SIAM Data Mining Conf.</source>, <year>2009</year>: <fpage>673</fpage>–<lpage>684</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1137/1.9781611972795.58" xlink:type="simple">https://doi.org/10.1137/1.9781611972795.58</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>FD</given-names></name>, <name name-style="western"><surname>Shan</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>YH</given-names></name>. <article-title>Parallel Nonnegative Matrix Factorization with Manifold Regularization</article-title>. <source>Journal of Electrical and Computer Engineering</source>, <volume>2018</volume>:<fpage>1</fpage>–<lpage>10</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1155/2018/6270816" xlink:type="simple">https://doi.org/10.1155/2018/6270816</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lazega</surname> <given-names>E</given-names></name>. <article-title>The Collegial Phenomenon: The Social Mechanisms of Cooperation Among Peers in a Corporate Law Partnership</article-title>. <source>Sociologie du Travail</source>, <year>2006</year>,<volume>48</volume>(<issue>1</issue>):<fpage>88</fpage>–<lpage>109</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.soctra.2006.01.001" xlink:type="simple">https://doi.org/10.1016/j.soctra.2006.01.001</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McAuley</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Leskovec</surname> <given-names>J</given-names></name>. <article-title>Learning to discover social circles in ego networks</article-title>. <source>NIPS</source>, <year>2012</year>: <fpage>539</fpage>–<lpage>547</lpage>.</mixed-citation></ref>
<ref id="pone.0208185.ref053"><label>53</label><mixed-citation publication-type="other" xlink:type="simple">Lu Q, Getoor L. Link-based Text Classification. In Proceedings of the IJCAI Workshop on Text Mining and Link Analysis. 2003.</mixed-citation></ref>
<ref id="pone.0208185.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zachary</surname> <given-names>W. W.</given-names></name> <article-title>An information flow model for conflict and fission in small groups</article-title>. <source>Journal of Anthropological Research</source>, <year>1977</year>, <volume>33</volume>(<issue>4</issue>), <fpage>452</fpage>–<lpage>473</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1086/jar.33.4.3629752" xlink:type="simple">https://doi.org/10.1086/jar.33.4.3629752</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pablo</surname> <given-names>M</given-names></name>. <name name-style="western"><surname>Gleiser</surname></name>, <name name-style="western"><surname>Leon</surname> <given-names>Danon</given-names></name>. <article-title>Community structure in jazz</article-title>. <source>Advances in Complex Systems</source>, <year>2003</year>, <volume>6</volume>(<issue>4</issue>): <fpage>565</fpage>–<lpage>573</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1142/S0219525903001067" xlink:type="simple">https://doi.org/10.1142/S0219525903001067</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref056"><label>56</label><mixed-citation publication-type="other" xlink:type="simple">Batagelj,V. &amp; Mrvar, A. Pajek datasets, available at <ext-link ext-link-type="uri" xlink:href="http://vlado.fmf.uni-lj.si/pub/networks/data/default.htm" xlink:type="simple">http://vlado.fmf.uni-lj.si/pub/networks/data/default.htm</ext-link>.</mixed-citation></ref>
<ref id="pone.0208185.ref057"><label>57</label><mixed-citation publication-type="other" xlink:type="simple">Lada A. Adamic, Natalie Glance. The political blogosphere and the 2004 U.S. election: divided they blog. Proceedings of the 3rd International Workshop on Link Discovery, ACM, 2005, 62(1):36–43. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1134271.1134277" xlink:type="simple">https://doi.org/10.1145/1134271.1134277</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watts</surname> <given-names>D.J.</given-names></name>, <name name-style="western"><surname>Strogatz</surname> <given-names>S.H.</given-names></name> <article-title>Collective Dynamics of “Small-World” Networks</article-title>. <source>Nature,</source> <year>1998</year>, <volume>393</volume>: <fpage>440</fpage>–<lpage>442</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/30918" xlink:type="simple">https://doi.org/10.1038/30918</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Newman</surname> <given-names>M.E.J.</given-names></name> <article-title>Finding community structure in networks using the eigenvectors of matrices</article-title>. <source>Physical Review E</source>, <year>2006</year>, <volume>74</volume>(<issue>3</issue>): <fpage>036104</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.74.036104" xlink:type="simple">https://doi.org/10.1103/PhysRevE.74.036104</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hanely</surname> <given-names>J.A.</given-names></name>, <name name-style="western"><surname>McNeil</surname> <given-names>B.J.</given-names></name> <article-title>The meaning and use of the area under a receiver operating characteristic (ROC) curve</article-title>. <source>Radiology</source>, <year>1982</year>, <volume>143</volume>(<issue>1</issue>):<fpage>29</fpage>–<lpage>36</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1148/radiology.143.1.7063747" xlink:type="simple">https://doi.org/10.1148/radiology.143.1.7063747</ext-link> <object-id pub-id-type="pmid">7063747</object-id></mixed-citation></ref>
<ref id="pone.0208185.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herlocker</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Konstann</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Terveen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Riedl</surname> <given-names>JT</given-names></name>. <article-title>Evaluating collaborative filtering recommender systems</article-title>. <source>Acm Trans Information Systems</source>, <year>2004</year>, <volume>22</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>53</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/963770.963772" xlink:type="simple">https://doi.org/10.1145/963770.963772</ext-link></mixed-citation></ref>
<ref id="pone.0208185.ref062"><label>62</label><mixed-citation publication-type="other" xlink:type="simple">Tong HH, Papadimitriou S, Sun JM, Yu PS, Faloutsos C. Colibri: fast mining of large static and dynamic graphs. the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM Press, 2008:686–694. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1401890.1401973" xlink:type="simple">https://doi.org/10.1145/1401890.1401973</ext-link></mixed-citation></ref>
</ref-list>
</back>
</article>